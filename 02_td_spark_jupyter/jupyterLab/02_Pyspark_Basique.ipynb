{"cells":[{"cell_type":"markdown","source":["# PySpark Basique Operations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"068d2d21-10ef-416a-93f4-6a0b784197a7"}}},{"cell_type":"markdown","source":["## 1 Table\n\nNous allons voir dans un premier temps la manière de selectionner une table, ce qui nous sera utile afin de pouvoir récuperer nos données par le futur.\n\nPour cela nous allons utiliser la fonction **table()**\n\nLe format est le suivant :\n\nmon_dataframe =  spark.table(**le nom de ma table**)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fde0085a-97a9-4499-ab66-69a397e8280d"}}},{"cell_type":"code","source":["# En pyspark nous auron donc \n# ma_variable = spark.table('')\n\n# Exécutons maintenant ce code \nmon_dataframe = spark.table('formation.arrivees')\n\n# Affichons notre dataframe afin de nous assurer de la récuperation de notre table\n# Pour rappel nous avons deux manières d'afficher nos résultat :\n\n# soit par la fonction display() qui va nous permettre de pouvoir mettre en forme nos résultats grâce aux options présentes en dessous \ndisplay(mon_dataframe)\n\n# soit par la fonction show() qui nous permet également d'afficher notre dataframe. On peut lui donner des paramètres lui indiquant la manière d'afficher le dataframe\nmon_dataframe.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8451692-dbf0-456d-b3f3-4646c8b3d8cc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":[">Nous pouvons constater que nous récupérons bien notre dataframe !  \n>Nous pouvons également limiter le nombre de résultat renvoyé grâce à la fonction limit()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed8c73b6-a627-414e-96cb-4bc5c3f0f64b"}}},{"cell_type":"code","source":["# Exemple avec la fonction display\nmon_dataframe = mon_dataframe.limit(5)\ndisplay(mon_dataframe)\n\n# Exemple avec la fonction show()\nmon_dataframe.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40ab9903-b603-4921-b037-60dfcafe52a6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 2 Select\n\nMaintenant que nous avons récupéré notre table, nous allons selectionner les colonnes qui nous intéressent grâce au **select()**\n\nLe format est le suivant :\n\nmon_dataframe =  spark.table(**le nom de ma table**).select(**les noms de mes colonnes**)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8209a461-4504-4257-a8fe-b18de48cf865"}}},{"cell_type":"code","source":["# nous rajoutons aprés avoir récupéré notre table la selection de nos colonnes \nmon_dataframe =  spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\")\ndisplay(mon_dataframe)\n\n# nous pouvons constater que le retour nous affiche bel et bien que deux colonnes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96269e0d-d67e-419a-8681-f40592acac3b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\n-- Si nous devions le traduire en SQL simple cela serait l'équivalent de :\nSELECT numero_caf,date_jour from formation.arrivees;\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a4aa154-9463-474a-b170-dac3d67163fa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2.1 Selection de colonnes\n\nNous avons vu la selection de colonnes grâce au **select()**  \nIl est également possible de saisir les colonnes grâce à **col()**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56683c94-518b-4e74-8535-515b35fe7042"}}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\n# nous rajoutons aprés avoir récupéré notre table la selection de nos colonnes \nmon_dataframe =  spark.table('formation.arrivees').select(F.col('numero_caf'),F.col('date_jour'))\n\ndisplay(mon_dataframe)\n\n# Exemple de la documentation officiel de Pyspark\n# def col(col: str) -> Column:\n#    \"\"\"\n#    Returns a :class:`~pyspark.sql.Column` based on the given column name.'\n#    Examples\n#    --------\n#    >>> col('x')\n#    Column<'x'>\n#    >>> column('x')\n#    Column<'x'>\n#    \"\"\"\n#    return _invoke_function(\"col\", col)\n\n# On peut également se servir de col() afin de n'utiliser que cette colonne \nmy_col = F.col(\"numero_caf\")\n\n# On affiche que notre colonne récupéré ci-dessus à partir de notre dataframe en limitant notre retour à 5 résultat\nmon_dataframe.select(my_col).show(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f27dab74-f684-4e31-838d-e5b846db8d44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2.2 Ajout de colonnes\n\nNous avons la possibilité également d'ajouter des colonnes grâce à la fonction **withColumn()**\n\nMais celle-ci ne permet pas que cela, elle peut aussi permettre :\n- Le changement de type d'une colonne\n- L'actualisation des valeurs contenues dans une colonne\n- La création d'une colonne à partir d'une existante\n- Le renommage d'une colonne"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21fcdf34-297d-42ce-a276-895e626477d8"}}},{"cell_type":"markdown","source":["#### Ajout de colonne Exemple\n\nLe format est le suivant pour l'ajout d'une colonne :\n\n`mon_dataframe = spark.table('ma_table').select(mes_colonnes)`\n\n`mon_dataframe = mon_dataframe.withColumn(\"nom_de_ma_nouvelle_colonne\",condition)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff04df87-1e26-4672-a0db-0b44f703934f"}}},{"cell_type":"markdown","source":["##### Exercice\n\nAjouter une colonne nous permettant de savoir si **date_jour** est supérieur ou non à 2021"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51230fee-723d-4d77-8fe6-0ed2c463a9de"}}},{"cell_type":"code","source":["# Nous récupérons notre dataframe\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\")\n\n#Exemple avec l'ajout d'une colonne nous indiquant si la date est supérieur à 2021\nmon_dataframe.withColumn(\"date_2021\", mon_dataframe[\"date_jour\"]>\"2021\").show(5)\n\n# Autre exemple de réponse \ndf = spark.table('formation.arrivees').select(\"numero_caf\" , \"date_jour\",\"service\").withColumn(\"nouvelle colonne\" , F.when( F.col(\"date_jour\") > \"2021-01-01\", ).otherwise(0) ).show()\n\n# Autre exemple de réponse\ndf = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\").withColumn('2021', F.col(\"date_jour\").between('2021-01-01', '2021-12-31')).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"354bf7a7-f8cc-4c24-90cc-62597f45f778"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Exemple Concret \n\nCette fois-ci nous allons prendre un exemple concret avec l'ajout de colonnes afin d'établir des chiffres sur les pièces fournits."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76d82a15-f61f-4e75-a16a-820829e8f644"}}},{"cell_type":"markdown","source":["##### Exercice\n\nSur la table \"formation.arrivees\" ajouter des colonnes nous permettant de savoir si la colonne \"nombre_piece_masse\" est \n  - inferieur a \"nombre_piece_total\"\n  - inferieur ou égal a \"nombre_piece_total\"\n  - supérieur a \"nombre_piece_total\"\n  - supérieur ou égal a \"nombre_piece_total\"\n  - égal a \"nombre_piece_total\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25453150-a87d-48cd-b240-b1abe0a94230"}}},{"cell_type":"code","source":["mon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"nombre_courrier\",\"nombre_piece_masse\",\"nombre_piece_total\").limit(10)\ndf = mon_dataframe.withColumn(\"lt\",mon_dataframe[\"nombre_piece_masse\"]<mon_dataframe[\"nombre_piece_total\"])\\\n       .withColumn(\"gt\",mon_dataframe[\"nombre_piece_masse\"]>mon_dataframe[\"nombre_piece_total\"])\\\n       .withColumn(\"lte\",mon_dataframe[\"nombre_piece_masse\"]<=mon_dataframe[\"nombre_piece_total\"])\\\n       .withColumn(\"gte\",mon_dataframe[\"nombre_piece_masse\"]>=mon_dataframe[\"nombre_piece_total\"])\\\n       .withColumn(\"eq\",mon_dataframe[\"nombre_piece_masse\"]==mon_dataframe[\"nombre_piece_total\"])\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b3a3c09-27ef-4bbd-ad94-60973447cede"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["> Nous pouvons transformer le retour de réponse (en replaçant les True/False par 0 ou 1) afin de faciliter la récupération et le compte de celle-ci grâce au **when** que nous allons voir juste aprés"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"063c660f-ca33-4734-ad51-5d16bf3a6d4c"}}},{"cell_type":"code","source":["df = df.withColumn(\"lt\" , F.when(df.nombre_piece_masse < df.nombre_piece_total, 1).otherwise(0) )\\\n        .withColumn(\"lte\" , F.when(df.nombre_piece_masse <= df.nombre_piece_total, 1).otherwise(0) )\\\n        .withColumn(\"gt\" , F.when(df.nombre_piece_masse > df.nombre_piece_total, 1).otherwise(0) )\\\n        .withColumn(\"gte\" , F.when(df.nombre_piece_masse >= df.nombre_piece_total, 1).otherwise(0) )\\\n        .withColumn(\"eq\" , F.when(df.nombre_piece_masse >= df.nombre_piece_total, 1).otherwise(0) )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9855c0a5-d522-46a5-ab9a-6f64752f21e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Ajout de colonne avec lit\n\nNous avons la possibilité également lors de l'ajout de colonne de définir une valeur constante ou une valeur littérale grâce à la fonction **lit()** de Pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f524e8e9-c86f-4be4-bb10-382f6029a459"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit\n# Nous récupérons notre dataframe\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\")\n\nmon_dataframe.show(5)\n\n#Exemple avec l'ajout d'une colonne avec la fonction lit()\nmon_dataframe.withColumn(\"colonne_constante\", lit(\"FR\") ).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"711acedf-5066-4405-9ed5-ad025be89acf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Ajout de colonne avec conditions multiples et lit\n\nOn peut ajouter une colonne avec des conditions multiples, dans le premier exemple nous venons insérer une colonne qui affichera si \"date_jour\" est supérieur à 2021  \nNous pouvons enchainer les conditions multiples grâce au **when()**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cc8419f-5333-4138-aa16-a0b1a3b4ac67"}}},{"cell_type":"code","source":["from pyspark.sql.functions import when, lit\n# Nous récupérons notre dataframe\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\")\n\nmon_dataframe.show(5)\n\n#Exemple avec l'ajout d'une colonne avec la fonction lit()\ndf = mon_dataframe.withColumn(\"colonne_conditions\", \\\n                         when((mon_dataframe.date_jour < \"2019\"), lit(\"past 2019\"))\n                        .when((mon_dataframe.date_jour >= \"2019\") & (mon_dataframe.date_jour < \"2020\"),lit(\"past 2020\"))\n                        .otherwise(lit(\"PRESENT\")))\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac09b276-5b48-4482-9647-666216420d70"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2.3 Création de colonne avec Pivot\n\nLa fonction **pivot()** de Pyspark permet la création de colonnes de manière différente.  \nEn effet, ces colonnes vont être le fruit de la division ou la transposition d'une autre colonne.  \n\nOn pourrait simplifier cela en disant que l'on va tranposer les donnée d'une colonne en plusieurs.  \n\nLa notion importante de la fonction d'agrégation **pivot()** est que les données une fois transposées auront subis le **distinct**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54ff898a-1c82-4323-97cf-b71578ff9298"}}},{"cell_type":"markdown","source":["#### Représentation concrête\n\n```\n+-------------------+--------+\n|COLONNE A PIVOT    | VALEUR |\n+-------------------+--------+\n|future colonne1    | val1   |\n|future colonne2    | val2   |\n|future colonne3    | val3   |\n|future colonne4    | val4   |\n+-------------------+--------+\n```\n\nMa table aprés mon opération de **pivot()** deviendra cela dans sa forme la plus simple :\n\n```\n+----------------+------------------+-----------------+-----------------+\n|future colonne1 | future colonne2  | future colonne3 | future colonne4 |\n+----------------+------------------+-----------------+-----------------+\n|      val1      |      val2        |      val3       |      val4       |\n+----------------+------------------+-----------------+-----------------+\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c7fbd0b-809b-444c-a94d-3410de050496"}}},{"cell_type":"markdown","source":["#### Exemple avec une table fictive simple"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"662dd128-1696-4193-aa58-8b3a5891b999"}}},{"cell_type":"code","source":["# Création de données fictives\ndata = [(\"Banane\",50,\"USA\"),(\"Orange\",40,\"ESP\"),(\"Avocat\",200,\"USA\"),(\"Melon\",15,\"ESP\"),(\"Citron\",110,\"ITA\"),(\"Citron vert\",75,\"ITA\"),]\n\n# Création de nos appelations de colonnes\ncolumns= [\"Fruit\",\"Quantite\",\"Provenance\"]\n\n# Création de notre dataframe\nmon_dataframe = spark.createDataFrame(data = data, schema = columns)\n\n# On peut afficher son schéma si on veut\n# mon_dataframe.printSchema()\n\n# Affichage de notre dataframe\ndisplay(mon_dataframe)\n\n\n\n# Mise en pratique de notre fonction d'agrégation pivot()\n\n\n# On va donc ici grouper nos rows par Fruit\n# Transposer notre colonne \"Provenance\" en plusieurs\n# Et faire la somme de \"Quantite\" par \"Provenance\" pour chaque \"Fruit\"\nmon_dataframe = mon_dataframe.groupBy(\"Fruit\").pivot(\"Provenance\").sum(\"Quantite\").show(10)\n\n# On va donc ici grouper nos rows par Provenance\n# Transposer notre colonne \"Fruit\" en plusieurs\n# Et faire la somme de \"Quantite\" pour chaque \"Fruit\" pour chaque \"Provenance\"\nmon_dataframe = mon_dataframe.groupBy(\"Provenance\").pivot(\"Fruit\").sum(\"Quantite\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b49f8e3-a1fc-401c-921e-66305d696471"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Exemple concret avec notre table formation.arrivees"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ea3c4e8-3f39-4723-a8c9-917ac352b7cb"}}},{"cell_type":"code","source":["# Nous récupérons notre dataframe\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\",\"nombre_courrier\",\"nombre_piece_total\")\n\n# Nous allons pour exemple prendre un pivot fait avec la colonne \"service\"\n# La traduction de notre requete serait :\n    # Groupe mes rows par numéro de caf\n    # transpose ma colonne service en colonnes séparées (un identifiant de service devient une colonne)\n    # fait le total de courrier par service pour chaque numero_caf\nmon_dataframe = mon_dataframe.groupBy(\"numero_caf\").pivot(\"service\").sum(\"nombre_courrier\")\n\ndisplay(mon_dataframe)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4112bcb-ad16-4a28-b662-fe8e760e7422"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Ressources avec Pivot\n\n> l'implémentation de **pivot()** à été amélioré au fur et à mesure des versions de Pyspark, mais cela reste une fonction qui consomme des ressources."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1c2b9f6-3eae-4e93-b3a6-890c589b9b03"}}},{"cell_type":"markdown","source":["## 3 Filtre\n\nLa méthode **filter()** va nous permettre une fois notre table,nos colonnes selectionnées d'appliquer un filtre sur les résultats retournés.  \n\n> Plusieurs filtres peuvent être appliqués en même temps  \n> La méthode **filter()** correspond à la clause **WHERE** en SQL\n\nLe format est le suivant :\n\nmon_dataframe =  spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\").filter(**ma conditon**)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7ee7afa-b2c1-4373-b090-be6b48d28618"}}},{"cell_type":"code","source":["# Nous appliquons le filtrage en deux fois afin dans un premier de récuperer notre dataframe puis de le filtrer\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\")\n\n# Nous filtrons en venant appliquer la condition à notre colonne\nmon_dataframe = mon_dataframe.filter(mon_dataframe.numero_caf != \"001\")\n\n# La méthode filter permet d'écrire les conditions sous plusieurs formes\n# Nous reprendrons ci-dessous le format de condition propre au SQL\nmon_dataframe = mon_dataframe.filter(\"numero_caf != 001\")\n\n# Cela sous-entend que nous pouvons reprendre tous les opérateurs de conditions SQL \n\n# Affichage de notre dataframe\ndisplay(mon_dataframe)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"722cac0f-7b21-4792-a065-010463082f8b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3.1 Exemple de filtrages différents"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79c7ee0a-1fcc-43c5-9961-875bddfb78f5"}}},{"cell_type":"markdown","source":["#### Filtre multiple \n\n> Nous allons pour ce faire utiliser les opérateurs ( \"&\", \"|\" ) qui correspondent au ( \"AND\", \"OR\" ) en SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59cf9eb2-c09d-48f4-9324-1def343f27fb"}}},{"cell_type":"code","source":["mon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\")\n\n# Exemple avec des conditions multiples et l'affichage avec la méthode show()\nmon_dataframe = mon_dataframe.filter((mon_dataframe.numero_caf != \"001\" ) & (mon_dataframe.service == \"AS\") ).show()\n\ndisplay(mon_dataframe)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f197d90c-0a3d-4db2-bff1-bdc17a902b20"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Filtre basé sur une liste \n\n> Nous allons pour ce faire utilisé le mot clé **isin()**  \n> Nous lui passerons notre liste en paramètre"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92293d65-b750-48d6-afe7-ec8011f1c230"}}},{"cell_type":"code","source":["# Nous pouvons également appliquer des filtres ayant une liste comme entrée \n# Imaginons que dans notre cas nous voulons faire un filtrage sur plusieurs services \n\n# Nous définissons notre liste\nliste_service=[\"AS\",\"PF\",\"AC\"]\n\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\")\n\n# Nous appliquons notre filtrage sur la colonne qui sera verifié en fonction de notre liste\nmon_dataframe = mon_dataframe.filter(mon_dataframe.service.isin(liste_service))\n\ndisplay(mon_dataframe)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cd74ee6-edee-44d0-bc06-73b8ec31e306"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Filtre basé sur le début et la fin d'un champ\n\n> Nous utiliserons pour ce faire les mots clés **startswith()** et **endswith()**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a766e629-0e18-4519-a04d-944d54ea87ea"}}},{"cell_type":"code","source":["# Nous appliquons notre filtrage en fonction de la première lettre de mon champ, ici un A\n# On affiche grâce à la méthode show() et on limite les nombres de résultats retournés à 5\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\")\nmon_dataframe = mon_dataframe.filter(mon_dataframe.service.startswith(\"A\")).show(5)\n\n# Nous appliquons notre filtrage en fonction de la dernière lettre de mon champ, ici un U\n# On affiche grâce à la méthode show() et on limite les nombres de résultats retournés à 5\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"service\")\nmon_dataframe = mon_dataframe.filter(mon_dataframe.service.endswith(\"U\")).show(5)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ee2d291-a0ab-4e97-9ba2-38295cddfa89"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Filtre et suite\n\nD'autre filtres existent également voici une liste non-exhaustive :  \n\n- filtrage pour vérifier la contenance grâce au contains()\n\n- filtrage reprenant le **like** et le **rlike** du SQL \n\n  - like permet de match les valeurs avec le pattern fournit (attention toutefois like est **case sensitive** à la différence de **rlike** (regex like))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e4e6bb4-c103-4e05-970e-0600fb2f4114"}}},{"cell_type":"markdown","source":["## 4 Verifier nos rows\n\nParfois malgré le filtrage possible, nous voulons quand même pouvoir vérifier la nullité d'un row.\n\nPour cela nous avons la fonction **isnull()**\n\nNous avons plusieurs manières d'intégrer **isnull()**\n\n- au travers d'un **select** afin de tester la condition s'il le row est null ou pas\n- au travers des **filtres** afin de retourner les rows null ou pas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a1ac040-6f4c-48af-8d5b-c229d37805d7"}}},{"cell_type":"code","source":["# Première méthode au travers d'un SELECT\n\n# Nous importons nos fonctions SQL\nimport pyspark.sql.functions as F\n\n# Nous changeons de table afin d'être sur d'avoir des champs \"null\"\n# Nous appliquons directement le isnull dans notre select afin de récuperer un row qui nous renverra la valeur booléenne de la fonction isnull()\n# Cela revient pour chaque row à tester si celui-ci est null ou pas \nmon_dataframe = spark.table('bgescaf.gssdparr').select(\"NUMCAF\",\"DTJOUR\",F.isnull(F.col(\"NPIEMA\")),F.isnull(F.col(\"PPIEMA\")))\n\ndisplay(mon_dataframe)\n\n# On peut également écrire cela de cette façon :\n# On récupère notre dataframe au travers d'un SELECT\nmon_dataframe2 = spark.table('bgescaf.gssdparr').select(\"NUMCAF\",\"DTJOUR\",\"NPIEMA\",\"PPIEMA\")\n\n# Puis on SELECT de nouveau mais cette fois-ci en appliquant notre méthode isNull()\nmon_dataframe2 = mon_dataframe2.select(\"NUMCAF\",\"DTJOUR\",mon_dataframe2.NPIEMA.isNull(),mon_dataframe2.PPIEMA.isNull())\n\ndisplay(mon_dataframe2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2aede027-83e6-4d51-99d7-27532983ace5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Deuxième méthode au travers d'un filtre\n\n# Nous importons nos fonctions SQL *\nimport pyspark.sql.functions as F\n\n# Nous changeons de table afin d'être sur d'avoir des champs \"null\"\nmon_dataframe = spark.table('bgescaf.gssdparr').select(\"NUMCAF\",\"DTJOUR\",\"NPIEMA\",\"PPIEMA\")\n\n# Nous filtrons sur la valeur null\nmon_dataframe = mon_dataframe.filter(mon_dataframe.NPIEMA.isNull())\n\n# Autre manière d'écrire le filtre :\n# mon_dataframe = mon_dataframe.filter(\"NPIEMA IS NOT NULL\")\n# mon_dataframe = mon_dataframe.filter(F.col(\"NPIEMA\").isNull())\n\ndisplay(mon_dataframe)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"561ba3ce-cc87-4d54-8a2d-062d45a09a20"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 5 Doublons de rows\n\nNos tables peuvent contenir des rows en double.  \n\nAfin de palier cela et d'afficher nos rows sans doublons nous allons avoir recours à deux fonctions plus ou moins similaires.\n\n- **distinct()**\n- **dropDuplicate()**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b466158f-959f-4ebe-a88a-8dc2ea8ee11c"}}},{"cell_type":"markdown","source":["### 5.1 Suppression des doublons avec distinct()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4ca8344-4e0e-4613-b1e3-01739290e02d"}}},{"cell_type":"code","source":["# Nous récupérons notre dataframe\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\")\nprint(\"Nombre de rows : \" + str(mon_dataframe.count()))\n\ndisplay(mon_dataframe)\n\n# Nous récupérons notre dataframe mais cette fois-ci sans les doublons grâce au distinct()\nmon_dataframe2 = spark.table('formation.arrivees').select(\"numero_caf\").distinct()\nprint(\"Nombre de rows sans les doublons : \" + str(mon_dataframe2.count()))\n\ndisplay(mon_dataframe2)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80b40fb5-4f14-4c47-89b6-ecba716421b1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5.2 Suppression des doublons avec dropDuplicates()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fa12fd8-559a-4ee8-ba8e-bc78a4b28ec6"}}},{"cell_type":"code","source":["# Nous récupérons notre dataframe\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"service\")\nprint(\"Nombre de rows : \" + str(mon_dataframe.count()))\n\ndisplay(mon_dataframe.orderBy(\"numero_caf\"))\n\n# Nous récupérons notre dataframe mais cette fois-ci sans les doublons grâce au distinct()\nmon_dataframe2 = spark.table('formation.arrivees').select(\"numero_caf\",\"service\")\nmon_dataframe2 = mon_dataframe2.dropDuplicates([\"numero_caf\",\"service\"])\nprint(\"Nombre de rows sans les doublons : \" + str(mon_dataframe2.count()))\n\ndisplay(mon_dataframe2.orderBy(\"numero_caf\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e2b1984-0eb9-403a-b998-ddcb7cccb70d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5.3 Différence entre les deux"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"073c7da0-312d-47fc-bab2-52f8f4c4612a"}}},{"cell_type":"markdown","source":["La grande différence entre ces deux méthodes, réside principalement dans l'utilisation que vous allez en faire.  \n\nEn effet :\n\n- **distinct()** permet de supprimer les doublons du dataframe en prenant en compte l'ensemble des colonnes de la table.\n\nA contrario \n\n- **dropDuplicates()** permet de supprimers les doublons du dataframe en prenant en paramètre les colonnes que vous choisirez"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7b8c745-8695-4ecb-95c9-d4e2ba3e286e"}}},{"cell_type":"markdown","source":["## 6 Ordonner nos rows\n\n\nAprés avoir vérifier nos rows, nous allons les ordonners.\n\nPour cela nous allons principalement utiliser deux fonctions :\n\n- sort()\n- orderBy()\n\nToutes deux similaires à leurs homonymes en SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10c6a708-e508-4632-adec-18edb6de74fd"}}},{"cell_type":"markdown","source":["#### 6.1 Sort\n\nLa fonction **sort()** permet d'ordonner les données en fonction de la ou les colonnes qui lui sont donnée en paramètres.  \nElle prend également en paramètre un booléen afin de savoir si elle ordonne de manière croissante ou pas.\n\n> Par défaut la manière croissante sera toujours choisie"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86dc5ae4-eee8-43d1-99a3-a74e397ab5e9"}}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\n# Nous récupérons notre dataframe (cette fois-ci avec des colonnes contenant des quantitées)\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"nombre_courrier\",\"nombre_piece_masse\",\"service\")\n\nmon_dataframe.show(5)\n\n# Plusieurs manières d'écrire le sort sont encore une fois disponible\n# Dans l'exemple ci-dessous nous appliquons notre \"sort\" sur la colonne \"nombre_courrier\" de manière décroissante\nmon_dataframe.sort([\"nombre_courrier\"],ascending=[False]).show(10)\n\n# A l'inverse nous aurions pu également trier par ordre croissant en passant notre booleén à \"True\"\nmon_dataframe.sort([\"numero_caf\"],ascending=[True]).show(10)\n\n\n\n# Nous pourrions également l'écrire de ces manière :\n\n# 1\nmon_dataframe.sort(mon_dataframe.numero_caf.asc()).show(5)\n# Ou a l'inverse \nmon_dataframe.sort(mon_dataframe.numero_caf.desc()).show(5)\n\n\n# 2 \nmon_dataframe.sort(F.col(\"numero_caf\").asc()).show(4)\n# Ou a l'inverse \nmon_dataframe.sort(F.col(\"numero_caf\").desc()).show(4)\n\n\n# Sort plusieurs colonnes\nmon_dataframe.sort([\"nombre_courrier\",\"nombre_piece_masse\"],ascending=[False]).show(20)\nmon_dataframe.sort(\"nombre_courrier\",\"nombre_piece_masse\",ascending=[False]).show(20)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"626484b1-b77e-4505-a26f-776a026894f4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 6.2 Order By\n\nLa fonction **orderBy()** permet d'ordonner les données en fonction de la ou les colonnes qui lui sont donnée en paramètres.  \nElle peut prendre également en paramètre un booléen afin de savoir si elle ordonne de manière croissante ou pas.\n\n> Par défaut la manière croissante sera toujours choisie"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e858382-96ec-4e58-b5d6-8f26e7e31702"}}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\n# Nous récupérons notre dataframe (cette fois-ci avec des colonnes contenant des quantitées)\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"nombre_courrier\",\"nombre_piece_masse\",\"service\")\n\nmon_dataframe.show(5)\n\n# Plusieurs manières d'écrire le sort sont encore une fois disponible\n# Dans l'exemple ci-dessous nous appliquons notre \"sort\" sur la colonne \"nombre_courrier\" de manière décroissante\nmon_dataframe.orderBy([\"nombre_courrier\"],ascending=[False]).show(10)\n\n# A l'inverse nous aurions pu également trier par ordre croissant en passant notre booleén à \"True\"\nmon_dataframe.orderBy([\"numero_caf\"],ascending=[True]).show(10)\n\n\n\n# Nous pourrions également l'écrire de ces manière :\n\n# 1\nmon_dataframe.orderBy(mon_dataframe.numero_caf.asc()).show(5)\n# Ou a l'inverse \nmon_dataframe.orderBy(mon_dataframe.numero_caf.desc()).show(5)\n\n\n# 2 \nmon_dataframe.orderBy(F.col(\"numero_caf\").asc()).show(4)\n# Ou a l'inverse \nmon_dataframe.orderBy(F.col(\"numero_caf\").desc()).show(4)\n\n\n# Sort plusieurs colonnes\nmon_dataframe.orderBy([\"nombre_courrier\",\"nombre_piece_masse\"],ascending=[False]).show(20)\nmon_dataframe.orderBy(\"nombre_courrier\",\"nombre_piece_masse\",ascending=[False]).show(20)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8585c8c5-09c4-4a20-a753-bead1c0f404c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 6.3 Similitudes et différences\n\nComme nous avons pu le constater ces deux fonctions :\n\n* ces deux fonctions s'écrivent de la manière\n* ces deux fonctions produisent des résultats similaires\n\n\nMalgré tout cela, il existe quand même des différences :\n\n\n* En réalité la fonction **orderBy()** est un alias de sort() en Python comme en témoigne ici le code source de Pyspark\n\n```python\ndef sort(self, *cols, **kwargs):\n        \"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\n        .. versionadded:: 1.3.0\n        Parameters\n        ----------\n        cols : str, list, or :class:`Column`, optional\n             list of :class:`Column` or column names to sort by.\n        Other Parameters\n        ----------------\n        ascending : bool or list, optional\n            boolean or list of boolean (default ``True``).\n            Sort ascending vs. descending. Specify list for multiple sort orders.\n            If a list is specified, length of the list must equal length of the `cols`.\n        Examples\n        --------\n        >>> df.sort(df.age.desc()).collect()\n        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n        >>> df.sort(\"age\", ascending=False).collect()\n        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n        >>> df.orderBy(df.age.desc()).collect()\n        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n        >>> from pyspark.sql.functions import *\n        >>> df.sort(asc(\"age\")).collect()\n        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n        >>> df.orderBy(desc(\"age\"), \"name\").collect()\n        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n        >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n        \"\"\"\n        jdf = self._jdf.sort(self._sort_cols(cols, kwargs))\n        return DataFrame(jdf, self.sql_ctx)\n\n    orderBy = sort\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40010442-79ec-4d2d-b1a7-d51561b38a05"}}},{"cell_type":"markdown","source":["En réalité la différence se joue au niveau de SPARKSQL comme en témoigne la documentation officielle :\n\n  * La fonction **sort()** va permettre de renvoyer les lignes de résultat triées dans chaque partition dans l'ordre spécifié par l'utilisateur. Lorsqu'il y a plusieurs partitions, SORT BY peut renvoyer un résultat partiellement ordonné.\n\nA la différence de la fonction orderBy() :\n\n  * la fonction **orderBy()** va permettre de renvoyer les lignes de résultat de manière triée dans l'ordre spécifié par l'utilisateur. Contrairement à la clause SORT BY, cette clause garantit un ordre total dans la sortie."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1ea70fb-c90a-4c83-8bac-8486a7d7f408"}}},{"cell_type":"markdown","source":["## 7 Grouper nos rows\n\nLa fonction **groupBy()** est utilisé pour collecter les données identiques et les rassemblers en groupe.  \nElle permet par la suite d'exécuter des fonctions d'agrégation sur les données groupées\n\nEn voici une liste non-exhaustive :\n- count() : cela renvoie le nombre de row pour chaque groupe\n- mean() : cela renvoie la valeur moyenne pour chaque groupe\n- min() : cela renvoie la valeur minimum pour chaque groupe\n- max() : cela renvoie la valeur maximum pour chaque groupe\n- sum() : cela additionne les différentes valeurs et retourne le résultat pour chaque groupe\n- avg() : cela renvoie la valeur moyenne pour chaque groupe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a030c67-79c3-442a-babf-b26499045dab"}}},{"cell_type":"code","source":["# Nous récupérons notre dataframe (cette fois-ci avec des colonnes contenant des quantitées)\nmon_dataframe = spark.table('formation.arrivees').select(\"numero_caf\",\"date_jour\",\"nombre_courrier\",\"nombre_piece_masse\",\"service\")\n\n# Exemple avec un groupBy() avec un count()\n# Nous comptons ici le nombre de row pour chaque numero_caf et nous ordonnons le tout de manière croissante sur le numéro de caf\nmon_dataframe.groupBy(\"numero_caf\").count().orderBy(\"numero_caf\").show(5)\n\n# Exemple avec un groupBy() avec un mean()\n# Nous regroupons pour chaque numero de caf le nombre moyen de courrier et nous ordonnons le tout de manière croissante sur le numéro de caf\nmon_dataframe.groupBy(\"numero_caf\").mean(\"nombre_courrier\").orderBy(\"numero_caf\").show(5)\n\n# Exemple avec un groupBy() avec un min()\n# Nous regroupons pour chaque numero de caf le nombre minimum de courrier et nous ordonnons le tout de manière croissante sur le numéro de caf\nmon_dataframe.groupBy(\"numero_caf\").min(\"nombre_courrier\").orderBy(\"numero_caf\").show(5)\n\n# Exemple avec un groupBy() avec un max()\n# Nous regroupons pour chaque numero de caf le nombre maximum de courrier et nous ordonnons le tout de manière croissante sur le numéro de caf\nmon_dataframe.groupBy(\"numero_caf\").max(\"nombre_courrier\").orderBy(\"numero_caf\").show(5)\n\n# Exemple avec un groupBy() avec un sum()\n# Nous regroupons pour chaque numero de caf le nombre total (ou la somme total) de courrier et nous ordonnons le tout de manière croissante sur le numéro de caf\nmon_dataframe.groupBy(\"numero_caf\").sum(\"nombre_courrier\").orderBy(\"numero_caf\").show(5)\n\n\n# Autre manière de procéder\n\n# reponse Guillaume\nmon_dataframe = mon_dataframe.groupBy(\"numero_caf\").agg(F.max(\"nombre_courrier\")).show()\n\n# reponse Remi\nplouf = spark.table(\"formation.arrivees\").groupBy(\"numero_caf\").avg(\"nombre_courrier\").display()\n\n# reponse de Eline\nsomme_nb_courrier=df.groupBy(\"numero_caf\",\"service\").agg(F.sum(\"nombre_courrier\").alias(\"total_nombre_courrier\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b6e1fec-2af3-44b4-93e7-289e6b565e97"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 8 Joindre nos rows\n\nDéfinition simple :  \nLa jointure permet d'associer les rows de deux tables\n\nRappel sur les différents types de jointures :\n\n![jointure](<https://miro.medium.com/max/1400/1*Lb3WTGX-N6HunApw-jT16g.png>)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4355074a-e696-421d-81e5-45852c625618"}}},{"cell_type":"markdown","source":["### 8.1 Jointure simple ou inner join\n\nEn SQL comme en Pyspark la jointure simple ou **inner join** peut se faire uniquement lorsque la condition est \"vrai\" dans les deux tables.\n\nC'est à dire de manière très simple, que l'égalité entre deux colonnes doit être toujours \"**vrai**\"\n\n> C'est cette condition que l'on va exprimer lors de notre jointure afin de lier nos tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ad25fc8-c4e5-4378-8e11-c32993fd5da5"}}},{"cell_type":"code","source":["# Récupérons notre premier table A\nmon_dataframe_A = spark.table(\"formation.arrivees\")\ndisplay(mon_dataframe_A.limit(1))\n\n# Récupérons notre deuxième table B\nmon_dataframe_B = spark.table(\"formation.stockees\")\ndisplay(mon_dataframe_B.limit(1))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe612f64-a327-45bd-8bc8-556c3683dac1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Lorsque l'on regarde nos deux table qu'elle sont les champs que l'on retrouve de part et d'autre ?  \nOn peut constater que nos deux tables sont pratiquement similaire :\n- seul le champ delai est rajouté dans la table \"formation.stockees\"\n\nNous avons donc de quoi réaliser notre condition"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05057536-0c1d-426b-af9a-acd8b9478968"}}},{"cell_type":"code","source":["# Récupérons notre premier table A\nmon_dataframe_A = spark.table(\"formation.arrivees\")\n#display(mon_dataframe_A.limit(1))\n\n# Récupérons notre deuxième table B\nmon_dataframe_B = spark.table(\"formation.stockees\")\n#display(mon_dataframe_B.limit(1))\n\n\n# Procédons à notre jointure simple \n# Nous avons joins notre table formation.stockees à notre table formation.arrivees, en lui indiquant notre condition\n# Ici nous allons basé notre jointure sur l'équivalence entre nos champs de la colonne numero_caf\nmon_dataframe_C = mon_dataframe_A.join(mon_dataframe_B,mon_dataframe_A.numero_caf == mon_dataframe_B.numero_caf,\"inner\").drop(mon_dataframe.numero_caf)\ndisplay(mon_dataframe_C)\n\n# Nous procédons également à une jointure simple mais cette fois-ci avec une double condition\n# L'égalité entre les champs numero_caf et date_jour doit être \"vrai\"\nmon_dataframe_D = mon_dataframe_A.join(mon_dataframe_B, on=((mon_dataframe_A[\"numero_caf\"]==mon_dataframe_B[\"numero_caf\"]) & (mon_dataframe_A[\"date_jour\"]==mon_dataframe_B[\"date_jour\"])), how=\"inner\")\n\n# Affichage jointure simple\ndisplay(mon_dataframe_C)\n\n# Affichage jointure conditions multiples\ndisplay(mon_dataframe_D)\n\n# Permettant de supprimer la colonne de jointure dans le résultat, manière plus lisible d'écrire\nmon_dataframe_E = mon_dataframe_A.join(mon_dataframe_B, on=\"numero_caf\",how=\"inner\")\n# On peut également le faire avec des colonnes multiples\nmon_dataframe_F = mon_dataframe_A.join(mon_dataframe_B, on=[\"numero_caf\",\"date_jour\"],how=\"inner\")\n\n## Pour supprimer le doublon de notre colonne \"numero_caf\" on peut utiliser .drop(mon_dataframe.numero_caf).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1382904d-8400-43e4-98e2-5906b2e347a5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Nous pouvons constater que notre jointure à recouper les résultats de nos deux tables.\n\n> Par défaut le \"**inner**\" présent dans notre requête n'est pas obligatoire, il est le type de jointure par défaut si nous n'en précisons pas.\n\n\n> **Important** : Tous les rows présents dans nos tables pour lesquels la conditon n'est pas vérifié sont \"**drop**\".\n> C'est à dire qu'il n'apparaitront pas dans les résultats de notre requête !"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"effd5624-9716-489c-b057-e9dbe8a19440"}}},{"cell_type":"markdown","source":["### 8.2 Jointure croisé ou Cross join \n\nLe jointure cross permet d'effectuer un produit cartesien entre les deux tables.\n\nRetourne les combinaisons de chaques paires des deux table.\n\nS'il y a 100 lignes dans les tables, il y aura alors 10 000 lignes en résultat.\n\nVous trouverez une représentation graphique juste en dessous "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79478dda-b5ab-4831-b11b-6b3dbb8bba2f"}}},{"cell_type":"markdown","source":["![cross](<https://storage.googleapis.com/hackersandslackers-cdn/2019/07/CROSSJOIN.jpg>)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac242950-0e86-448f-a770-f45a430213f3"}}},{"cell_type":"code","source":["# Nous récuperons nos dataframe\nmon_dataframe_A = spark.table(\"formation.arrivees\")\nmon_dataframe_B = spark.table(\"formation.arrivees\")\n\n# Affichage du nombre de rows -> 409317\nprint(mon_dataframe_A.count())\n\n# Nous appliquons la jointure \nmon_dataframe_C = mon_dataframe_A.join(mon_dataframe_B , how=\"cross\")\n\n# Affichage du nombre de rows apres cross jointure ->  167540406489 (cela correspond bien à 409317x409317)\nprint(mon_dataframe_C.count())\n\n# Nous pouvons également l'écrire de cette manière\nmon_dataframe_C = mon_dataframe_A.crossJoin(mon_dataframe_B)\n\ndisplay(mon_dataframe_C)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1bfd60e-ae74-497e-82c8-2443df72a0a5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 8.3 Jointure Full,Full Outer, Outer\n\nDe manière trés simple les jointures **full** nous permettent de joindre nos rows, que l'égalité soit vrai ou pas.\n\n> Il remplacera les rows ou l'égalité n'est pas vérifié par des rows \"**null**\"\n\nLe format est la suivante :\n* Full\n  - `table1.join(table2, on=table1[\"nombre1\"]==table2[\"nombre2\"],how=\"full\").display()`\n* Full Outer\n  - `table1.join(table2, on=table1[\"nombre1\"]==table2[\"nombre2\"],how=\"full_outer\").display()`\n* Outer\n  - `table1.join(table2, on=table1[\"nombre1\"]==table2[\"nombre2\"],how=\"outer\").display()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efeba937-c1e3-4a9a-a4f4-acc9d075645c"}}},{"cell_type":"markdown","source":["Exemple de jointure **FULL**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe3dbbdb-8b23-418e-bf41-36c6ecabd6e5"}}},{"cell_type":"code","source":["# Nous récuperons nos dataframe\nmon_dataframe_A = spark.table(\"formation.arrivees\")\nmon_dataframe_B = spark.table(\"formation.arrivees\")\n\n# Affichage du nombre de rows -> 409317\nprint(mon_dataframe_A.count())\n\n# Nous appliquons la jointure \nmon_dataframe_C = mon_dataframe_A.join(mon_dataframe_B ,on=((mon_dataframe_A[\"numero_caf\"]==mon_dataframe_B[\"numero_caf\"]) & (mon_dataframe_A[\"date_jour\"]==mon_dataframe_B[\"date_jour\"])), how=\"full\")\n\n# Affichage du nombre de rows -> 1730575\nprint(mon_dataframe_C.count())\n\n# Affichage de notre table\ndisplay(mon_dataframe_C)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d265ef25-5abb-477b-b300-f43e3243665c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Exemple de jointure **FULL OUTER**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5073eefe-6119-45c1-a1fe-ec70ceeb1166"}}},{"cell_type":"code","source":["# Nous récuperons nos dataframe\nmon_dataframe_A = spark.table(\"formation.arrivees\")\nmon_dataframe_B = spark.table(\"formation.arrivees\")\n\n# Affichage du nombre de rows -> 409317\nprint(mon_dataframe_A.count())\n\n# Nous appliquons la jointure \nmon_dataframe_C = mon_dataframe_A.join(mon_dataframe_B ,on=((mon_dataframe_A[\"numero_caf\"]==mon_dataframe_B[\"numero_caf\"]) & (mon_dataframe_A[\"date_jour\"]==mon_dataframe_B[\"date_jour\"])), how=\"full_outer\")\n\n# Affichage du nombre de rows -> 1730575\nprint(mon_dataframe_C.count())\n\n# Affichage de notre table\ndisplay(mon_dataframe_C)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea54ff3a-ec4e-4da0-ac0e-d4c432c3db0f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Exemple de jointure **OUTER**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04f1a79b-3767-423a-9de2-4c7b99499191"}}},{"cell_type":"code","source":["# Nous récuperons nos dataframe\nmon_dataframe_A = spark.table(\"formation.arrivees\")\nmon_dataframe_B = spark.table(\"formation.arrivees\")\n\n# Affichage du nombre de rows -> 409317\nprint(mon_dataframe_A.count())\n\n# Nous appliquons la jointure \nmon_dataframe_C = mon_dataframe_A.join(mon_dataframe_B ,on=((mon_dataframe_A[\"numero_caf\"]==mon_dataframe_B[\"numero_caf\"]) & (mon_dataframe_A[\"date_jour\"]==mon_dataframe_B[\"date_jour\"])), how=\"outer\")\n\n# Affichage du nombre de rows -> 1730575\nprint(mon_dataframe_C.count())\n\n# Affichage de notre table\ndisplay(mon_dataframe_C)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c8503c2-6ac5-4cab-9954-d69bf53a4ed6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 8.4 Jointure gauche ou droite \n\nLes jointures `LEFT` et `RIGHT` permettent de lister tous les résultats de la table de gauche ou de droite avec notre table de référence.\n\n> Ces jointures s'appliquent même s'il n'y a pas d'équivalence entre les champs de nos tables\n> Auquel cas les rows seront à **NULL**\n\nLe format est la suivante :\n* LEFT (ou appelé communément en SQL `LEFT OUTER JOIN`)\n  - `table1.join(table2, on=table1[\"nombre1\"]==table2[\"nombre2\"],how=\"left\").display()`\n* RIGHT (ou appelé communément en SQL `RIGHT OUTER JOIN`)\n  - `table1.join(table2, on=table1[\"nombre1\"]==table2[\"nombre2\"],how=\"right\").display()`\n  \n> Vous pouvez à la place de \"left\" ou \"right\" écrire \"left_outer\" ou \"right_outer\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4005fce7-d164-4207-9602-fbcb1f36d79c"}}},{"cell_type":"markdown","source":["Jointure **LEFT**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83a125fe-c1e2-4bb9-ad3b-17721f901e7c"}}},{"cell_type":"code","source":["# Nous récuperons nos dataframe\nmon_dataframe_A = spark.table(\"formation.arrivees\")\nmon_dataframe_B = spark.table(\"formation.stockees\")\n\n# Affichage du nombre de rows -> 409317\nprint(mon_dataframe_A.count())\n\n# Nous appliquons la jointure \nmon_dataframe_C = mon_dataframe_A.join(mon_dataframe_B ,on=((mon_dataframe_A[\"numero_caf\"]==mon_dataframe_B[\"numero_caf\"]) & (mon_dataframe_A[\"date_jour\"]==mon_dataframe_B[\"date_jour\"])), how=\"left\")\n\n# Affichage du nombre de rows -> 2094958\nprint(mon_dataframe_C.count())\n\n# Affichage de notre table\ndisplay(mon_dataframe_C)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5e505eb-613b-4766-82b9-0b327175c1c3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Jointure **RIGHT**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9fb28ab-b50a-409d-84da-841046dbcc83"}}},{"cell_type":"code","source":["# Nous récuperons nos dataframe\nmon_dataframe_A = spark.table(\"formation.arrivees\")\nmon_dataframe_B = spark.table(\"formation.stockees\")\n\n# Affichage du nombre de rows -> 409317\nprint(mon_dataframe_A.count())\n\n# Nous appliquons la jointure \nmon_dataframe_C = mon_dataframe_A.join(mon_dataframe_B ,on=((mon_dataframe_A[\"numero_caf\"]==mon_dataframe_B[\"numero_caf\"]) & (mon_dataframe_A[\"date_jour\"]==mon_dataframe_B[\"date_jour\"])), how=\"right\")\n\n# Affichage du nombre de rows -> 2095623\nprint(mon_dataframe_C.count())\n\n# Affichage de notre table\ndisplay(mon_dataframe_C)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76f3c9b6-b7aa-40a7-83e0-dedbd6bf3d4d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02_Pyspark_Basique","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4010891277313446,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":3414440142528180}},"nbformat":4,"nbformat_minor":0}
