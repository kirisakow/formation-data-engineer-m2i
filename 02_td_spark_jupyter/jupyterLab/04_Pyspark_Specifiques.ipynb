{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8a70c148-9cd3-4a52-88e9-26812382b2fe","showTitle":false,"title":""}},"source":["# Pyspark Fonctions Spécifiques"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7ea8f6f3-3e64-4c3b-8cac-482bf489b166","showTitle":false,"title":""}},"source":["### 1 La fonction collect()\n","\n","La fonction `collect()` est une fonction d'action qui permet de récupérer toutes les données préalablement dispatchées dans les noeuds de travail afin de les récupérer et de pouvoir les manipuler.\n","\n","L'opération `collect()` nous renvoie un tableau de \"rows\" (de type row)\n","\n","L'avantage est qu'une fois mise à disposition nous pouvons grâce à des \"**loop**\" itérer de manière à appliquer un traitement."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"30735491-ea2f-4bfa-8354-4f504be1caa6","showTitle":false,"title":""}},"outputs":[],"source":["# Exemple de collect() simple\n","mon_dataframe = spark.table(\"formation.arrivees\").limit(1).collect()\n","\n","# Affichage de mon dataframe\n","print(mon_dataframe)\n","\n","# Affichage de son type\n","print(type(mon_dataframe))\n","\n","# Affichage d'un row\n","# Dans mon print nous savons que nous avons qu'1 seul row\n","# nous utiliserons donc le premier [0]\n","# puis nous lui indiquons que nous voulons la valeur 0 (numero_caf) puis les suivantes avec [0:]\n","# cette formulation nous sera utile afin de selectionner les rows et les valeurs contenues dans ceux-ci\n","print(mon_dataframe[0][0:])"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1e666dff-0aad-4065-ac82-670eac6e4a5b","showTitle":false,"title":""}},"source":["#### 1.1 Iteration sur notre liste\n","\n","Nous allons voir ci-dessous plusieurs exemples d'iterations sur notre liste (une fois le **collect()** réalisé)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1cb70a7d-2a38-49ff-951b-a55195b5899e","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import col\n","\n","# Exemple de collect() simple\n","mon_dataframe = spark.table(\"formation.arrivees\").select(\"numero_caf\",\"date_jour\",\"service\").limit(5).collect()\n","\n","# Boucle simple\n","for row in mon_dataframe:\n","    print(str(row[\"numero_caf\"]),\",\",str(row[\"date_jour\"]),\",\",str(row[\"service\"]))\n","    \n","print(\"\")   \n","print(\"-----------------------------------------------------------------------\")\n","print(\"\")\n","\n","\n","# Boucle avec limite\n","for row in mon_dataframe[0:2]:\n","    print(str(row[\"numero_caf\"]),\",\",str(row[\"date_jour\"]),\",\",str(row[\"service\"]))\n","    \n","print(\"\")   \n","print(\"-----------------------------------------------------------------------\")\n","print(\"\")\n","\n","\n","# Boucle pour ne récuperer qu'une colonne\n","for col in mon_dataframe:\n","    ma = col[\"service\"]\n","        "]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0cf7e96c-b249-4149-baa9-805345bc8166","showTitle":false,"title":""}},"source":["## 2 La fonction window()\n","\n","Le fonction `window()` va nous permettre de manière simple de réaliser des opérations \"statistiques\" à partir d'un groupe, d'une collection, d'un dataframe.\n","\n","On va retrouver de manière générale trois grande fonctions lié à `window()` :\n","\n","- Les  fonctions Analytiques\n","- Les fonctions permettant d'établir un classement\n","- Les fonctions d'aggrégations\n","\n","\n","![pyspark window](<https://miro.medium.com/max/1200/1*lUxrBqSZ7lci8UGvOlyRUg.png>)\n","\n","\n","La fonction `window()` est également utilisé afin d'opérer un transformation des données."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ca00c861-dacb-443e-950d-5946e666f610","showTitle":false,"title":""}},"source":["La syntaxe concernant `window()` est la suivante :\n","\n","- `Window.partitionBY(\"nom_de_ma_colonne\").orderBy(\"nom_de_ma_colonne\")`"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"51a4896f-ba5c-43bf-8c07-5dd67b2be981","showTitle":false,"title":""}},"outputs":[],"source":["# importing pyspark\n","from pyspark.sql.window import Window\n","from pyspark.sql import SparkSession\n","\n","# Nous allons créer un dataframe propre pour la compréhension \n","spark = SparkSession.builder.getOrCreate()\n","\n","# Creation de nos données\n","donnee = ((\"Killian\", \"Dev\", 3000),(\"Benoit\", \"Dev\", 4600),(\"Romain\", \"Dev\", 4100),(\"Lindsay\", \"RH\", 3000),(\"Simon\", \"RH\", 3000),(\"Émilie\", \"Direction\", 3300),(\"Adrien\", \"Direction\",3900))\n","\n","# Creation de nos colonnes\n","colonnes = [\"Nom\", \"Service\", \"Salaire\"]\n","\n","# Récupération de notre dataframe\n","mon_dataframe = spark.createDataFrame(data = donnee, schema = colonnes)\n","\n","# Création de notre window\n","ma_window = Window.partitionBy(\"Service\").orderBy(\"Salaire\")\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f8b0f03a-f443-4ad9-b531-745f6b06f1ee","showTitle":false,"title":""}},"source":["### 2.1 Les fonctions de classements ou \"ranking fonctions\""]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"11937408-1426-4f69-8c18-e6bd5317daac","showTitle":false,"title":""}},"source":["La fonction `row_number()`  va numéroter les rows, nous affichant une séquence en fonction de la window que l'on aura créée. \n","\n","Dans l'exemple ci-dessous :\n","\n","- nous ajoutons à la suite de notre dataframe une colonne appelée \"numero_row\" qui permettra l'affichage de notre séquence\n","- nous appliquons `row_number()` qui prendra notre window pour faire la séquence\n","\n","Constater l'affichage de notre dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"34d879af-a452-4239-aeed-ad293ab5b231","showTitle":false,"title":""}},"outputs":[],"source":["# importing pyspark\n","from pyspark.sql.functions import row_number\n","\n","# Application de notre row_nuber()\n","mon_dataframe = mon_dataframe.withColumn(\"numero_row\",row_number().over(ma_window))\n","\n","# Lorsque nous affichons notre dataframe, l'on se rend bien compte qu'il à numeroter les rows en créant une sequence\n","display(mon_dataframe)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"52de8f4c-f805-48d1-a7ef-b13923441875","showTitle":false,"title":""}},"source":["La fonction `rank()` nous permet d'établir un classement basé sur notre window\n","\n","Ce classement se traduira sous forme de chiffre (allant de 1 à ...).\n","\n","Dans notre exemple ci-dessous, le salaire le plus élevé à le rank le plus haut.\n","\n","\n","> la fonction `rank()` est similaire à la fonction `RANK` en SQL  \n","> Par défaut les rows présentant une égalité ne seront pas affichés  \n","> Pour palier à cela nous utiliserons la fonction `dense_rank()`\n","\n","\n","On trouve également la fonction `percent_rank()` similaire à `rank()` mais exprimé sous forme de pourcentage.   \n","Appliqué à notre exemple le salaire le plus elevé aura la valeur **1.0** et le salaire le moins élevé aura la valeur **0.0**\n","\n","La fonction `ntile()` qui prend une valeur en paramètre peut nous permettre d'intéragir directement avec les limites du classement\n"," - si nous lui donnons \"2\" en paramètre le ranking exprimé ne dépassera jamais \"2\" (les rows auront la valeur soit \"1\" soit \"2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"13439962-84a7-416c-936a-6f4429cbfcb4","showTitle":false,"title":""}},"outputs":[],"source":["# importing pyspark\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import rank\n","\n","# Appliquons à notre dataframe la fonction rank\n","mon_dataframe = mon_dataframe.withColumn(\"rank\",rank().over(ma_window))\n","\n","# On peut constater qu'il à établit un classement en fonction de notre window()\n","# Autrement dit, il a établit un classement en fonction du salaire \n","display(mon_dataframe)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"af74bef6-0057-4d11-be67-17eb6f4e8846","showTitle":false,"title":""}},"source":["### 2.2 Les fonctions analytiques\n","\n","Les fonctions analytiques permettent d'effectuer des opérations sur un nombre définis de rows.  \n","Le résultat retourné comprant autant de rows , que le nombre de rows en \"input\"."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4114631a-1c09-4f62-bdae-6b69f1e3988b","showTitle":false,"title":""}},"source":["La fonction `cume_dist()` nous permet de manière simple de déterminer l'emplacement relatif d'une valeur dans un ensemble de valeurs."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0491d682-1b4f-4ec4-9ce9-b31d6c7b467d","showTitle":false,"title":""}},"outputs":[],"source":["# importing pyspark\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import cume_dist\n","\n","# Appliquons à notre dataframe la fonction rank\n","mon_dataframe = mon_dataframe.withColumn(\"cumulative_valeur\",cume_dist().over(ma_window))\n","\n","\n","display(mon_dataframe)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"56bec683-2d25-4d6a-a8b9-a86963cc0dfa","showTitle":false,"title":""}},"source":["La fonction `lag()` nous permet de manière simple de garder la valeur du précédent row.\n","\n","La fonction `lag()` prend deux paramètre :\n","\n","  - la colonne à regarder \n","  - le \"look-Back\" c'est à dire le nombre d'itération précédente sur laquelle se baser.\n","\n","> Pour utiliser `lag()` il faut que la colonne à laquelle est appliqué la fonction soit cohérente.  \n","> Dans notre exemple ci-dessous, la fonction `lag()` va nous renvoyer des `null` car les valeures contenues dans ma colonnes salaire ne sont pas cohérente avec son utilisation"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8d7901c6-7e89-46ec-bece-be7e11815a9a","showTitle":false,"title":""}},"outputs":[],"source":["# importing pyspark\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import lag\n","\n","# Appliquons à notre dataframe la fonction rank\n","mon_dataframe = mon_dataframe.withColumn(\"valeur_précédente\",lag(\"Salaire\",1).over(ma_window))\n","\n","\n","display(mon_dataframe)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a44ed9b4-2f5b-405d-907f-fd049017fdbd","showTitle":false,"title":""}},"source":["La fonction `lead()` nous permet d'itérer sur la colonne spécifié afin de retrouver le row suivant notre row actuel\n","\n","> Attention cela dépend de la colonne spécifié et de l'application que l'on veut en faire \n","\n","**EXEMPLE** :\n","\n","Imaginons que l'on est une table composé de 2 colonnes comme suit :\n","\n","|FILM|DUREE|\n","|:--:|:--:|\n","|film1|duree1|\n","|film2|duree2|\n","\n","`lead()` avec comme paramètre la colonne \"duree\" va en premier parcourir la colonne \"duree\" de manière ascendante , et retourner la valeur supérieur en durée au row actuel\n","\n","|FILM|DUREE|LEAD|\n","|:--:|:--:|:--:|\n","|film1|duree1|duree2|\n","|film2|duree2|null|\n","\n","> Tout comme `lag()` il faut que la colonne à laquelle est appliqué la fonction soit cohérente."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"de62b036-3f0e-483e-aff5-98989e511cd0","showTitle":false,"title":""}},"outputs":[],"source":["# importing pyspark\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import lead\n","\n","# Appliquons à notre dataframe la fonction rank\n","mon_dataframe = mon_dataframe.withColumn(\"lead\",lead(\"Salaire\",1).over(ma_window))\n","\n"," \n","display(mon_dataframe)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0761441a-5876-4efd-8304-1a2416c44772","showTitle":false,"title":""}},"source":["### 2.3 Les fonctions d'aggrégations\n","\n","Les fonctions d'aggrégations sont similaires à celles déja vu en Pyspark.\n","Avec la fonction `window()` \n","\n","> Les fonctions d'aggrégations reprennent la `window()` que l'on à établit.  \n","> Par conséquent, les résultat ci-dessous sont valable pour le partitionnement par \"Service\" que l'on a fait.  \n",">\n","> Exemple :  \n","> Dans la colonne \"sum\" vous constaterez qu'il additionne uniquement au fur et a mesure les **salaires** concernant **uniquement** le **service**"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0c3a5bdc-644e-45c6-836b-99258bc8f6f4","showTitle":false,"title":""}},"outputs":[],"source":["# importing pyspark\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import col,avg,sum,min\n","\n","# Appliquons à notre dataframe la fonction rank\n","#mon_dataframe = mon_dataframe.withColumn(\"lead\",lead(\"Salaire\",1).over(ma_window))\n","\n","mon_dataframe = mon_dataframe.withColumn(\"avg\",avg(col(\"Salaire\")).over(ma_window)) \\\n","                .withColumn(\"sum\",sum(col(\"Salaire\")).over(ma_window)) \\\n","                .withColumn(\"min\",min(col(\"Salaire\")).over(ma_window)) \n","\n","display(mon_dataframe)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"04_Pyspark_Specifiques","notebookOrigID":3414440142528184,"widgets":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"bd6f263fc755eebb9705a1037789dd7cb01391d5841f7a8e7a78f9bd6260bb09"}}},"nbformat":4,"nbformat_minor":0}
