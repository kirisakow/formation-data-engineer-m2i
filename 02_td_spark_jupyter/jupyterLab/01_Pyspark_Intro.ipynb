{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9ccb32c6-5018-4cb8-bcb5-9c68e898b3f9","showTitle":false,"title":""}},"source":["# Pyspark Introduction\n","\n","<br> \n","\n","![spark](<https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1400%2F1*tP-dw4Oj_42BYbkdtYbjMA.png&f=1&nofb=1>)\n","![python](<https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F3200%2F1*-3yJf8MBLwJ3rf047Fpopg.png&f=1&nofb=1>)\n","<br>\n","\n","> Apache Spark est écrit en Scala.  \n","> PySpark a été créé afin de pouvoir faire collaborer Apache Spark et Python"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"78188d23-84d4-4db3-8f43-ab8ebabb4556","showTitle":false,"title":""}},"source":["## Rappel sur les différentes technologies\n","\n","<br>\n","\n","|NOM||\n","|:--:|:--:|\n","|<a href=\"$./Technologie/Apache Spark\">Apache Spark</a>|<a href=\"$./Technologie/Pandas\">Pandas</a>|\n","|<a href=\"$./Technologie/Databricks\">Databrick</a>|<a href=\"$./Technologie/Hadoop\">Hadoop</a>|"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"99e77174-07f3-4068-b439-f7f5e8d4a737","showTitle":false,"title":""}},"source":["### Rappel sur ce qu'est Map Reduce :\n","\n","**Map Reduce** est en réalité issue de GoogleMapReduce outil prévus afin d'analyser de grandes quantités d'information (page Web) pour le moteur de recherche Google.\n","\n","**Point fort :**\n","- Facile d'écriture (son adoption est trés répandue grâce à cela)\n","- Il est tolérant au pannes (le calcul étant distribué, et les données stockées à plusieurs endroits)\n","- Capacité de traitement parallèles forte\n","\n","**Map Reduce _Hello World_ :**\n","\n","<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n","  <img src=\"https://intellipaat.com/blog/wp-content/uploads/2016/11/The-Architecture-of-MapReduce.jpg\" alt='GrabNGoInfo Logo' style=\"width: 1000px\" >\n","</div>\n","\n","**Trois étapes importantes :**\n","\n","- Mapping : Cela représente le processus de lecture des informations à partir d'une source ( par exemple _HDFS_ : Systeme de fichier distribué Hadoop )\n","- Brassage : Cela représente de manère simplifier la phase de traitement des données\n","- Reduce : Cela représente la phase final, la sortie du réducteur peut-être stockée directement."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cb2a8dd4-d2cf-4640-8049-ecf5d2eb2c65","showTitle":false,"title":""}},"source":["### Hadoop vs Spark :\n","\n","<br>\n","\n","> La première différence à noter entre ces deux frameworks relatif au Big Data est que l'un est spécialisé dans le système de stockage distribué (_Apache Hadoop avec HDFS_) et que l'autre est plus spécialisé dans le traitement même des données (_Apache Spark_).  \n","> Malgrès cela les deux peuvent s'utiliser de manière indépendante.\n","\n","---\n","\n","**La différence de traitement de la donnée**\n","\n","<br>\n","\n","<img src=\"https://img1.lemondeinformatique.fr/fichiers/telechargement/spark-hadoop.png\">\n","\n","<br>\n","\n","> Hadoop travail en mode \"lots\" avec le MapReduce  \n","> Spark fait du temps réel in-memory\n","\n","---\n","\n","> Tous les deux dispose de reprise après incident."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6962e1f2-14f8-4a79-a546-d66e3d01b1b0","showTitle":false,"title":""}},"source":["### Point fort d'Apache Spark :\n","\n","Le point d'Apache Spark réside dans sa vitesse d'exécution. Réalisant l'intégralité des opérations dans la mémoire (la lecture, l'analyse et les opérations) sa vitesse s'en trouve accrue.\n","\n","<br>\n","\n","> Pour l'anecdote on considère que la vitesse entre Hadoop et Spark pourrait être mutliplié par 10."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c7470ae6-3c57-4f51-90ff-478430ebc5b3","showTitle":false,"title":""}},"source":["### Synthèse\n","\n","Malgrès les différences intrasèques à ces deux framework. Apache Spark utilise quand même le modèle MapReduce mais avec une couche d'abstraction supplémentaire, simplifiant ainsi le traitement pour l'utilisateur.\n","\n","Le choix entre _Apache Hadoop_ et _Apache Spark_ est une question à part entière :\n","- La question de la rapidité (Tout le monde n'a pas besoin de la rapidité de Spark)\n","- La question de la persistence des données (Contrairement à Hadoop, Spark ne dispose d'un système de fichier propres par conséquent il aura toujours besoin de cela afin de persister les données post-traitement)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"342d142f-dea1-4efd-a81a-5d79f074392c","showTitle":false,"title":""}},"source":["# PySpark :"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"afd11c45-55e1-409e-8a89-393e6ae96c66","showTitle":false,"title":""}},"source":["## Spark\n","\n","Spark est une librairie Scala permettant d'effectuer du `calcul distribué`.\n","\n","Quand vous exécutez du code sur votre machine, seule votre machine est capable de l'exécuter.\n","\n","Avec Spark nous pouvons répartir les calculs sur un cluster et donc sur plusieurs machines.\n","\n","C'est le coeur de Databricks.\n","\n","Databricks simplifie BEAUCOUP son utilisation en proposant un certain nombre d'outils qui en font une abstraction.\n","\n","Spark est développée en Scala mais vous pouvez aussi utiliser le python pour développer vos scripts."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1e2591cc-37b8-473e-b5e8-cdcba2cf09e1","showTitle":false,"title":""}},"source":["## Spark SQL\n","\n","Spark propose un ensemble d'outils pour développer des scripts en utilisant du SQL.\n","\n","> C'est de cette façon que Databricks propose la création de cellule SQL! Vous avez donc l'ensemble des clauses et concepts vue pendant la formation SQL qui sont directement utilisables ICI.\n","\n","> Pas besoin d'importer Spark... Spark étant le concept de base de databricks, il est par défaut dans tous les notebooks."]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"01_Pyspark_Intro","notebookOrigID":2549891417614052,"widgets":{}},"kernelspec":{"display_name":"Python 3.10.8 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.8"},"vscode":{"interpreter":{"hash":"bd6f263fc755eebb9705a1037789dd7cb01391d5841f7a8e7a78f9bd6260bb09"}}},"nbformat":4,"nbformat_minor":0}
