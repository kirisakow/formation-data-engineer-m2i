{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: mlxtend in /usr/local/lib/python3.6/dist-packages (0.18.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend) (41.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.24.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.1.5)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (3.3.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->mlxtend) (8.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->mlxtend) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: H2O==3.26.0.1 in /usr/local/lib/python3.6/dist-packages (3.26.0.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from H2O==3.26.0.1) (0.18.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from H2O==3.26.0.1) (2.25.1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from H2O==3.26.0.1) (0.8.9)\n",
      "Requirement already satisfied: colorama>=0.3.8 in /usr/local/lib/python3.6/dist-packages (from H2O==3.26.0.1) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->H2O==3.26.0.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->H2O==3.26.0.1) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->H2O==3.26.0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->H2O==3.26.0.1) (2.6)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: mcfly in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from mcfly) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from mcfly) (0.24.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mcfly) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.11 in /usr/local/lib/python3.6/dist-packages (from mcfly) (1.5.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from mcfly) (2.10.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.15.0->mcfly) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.15.0->mcfly) (2.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (1.12.1)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (1.1.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (0.3.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (0.36.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (2.5.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (1.32.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (3.7.4.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (3.16.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (1.12)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->mcfly) (0.12.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (41.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (1.30.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (4.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (4.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (2.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.0.0->mcfly) (3.4.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install mlxtend\n",
    "!{sys.executable} -m pip install H2O==3.26.0.1\n",
    "!{sys.executable} -m pip install mcfly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data Reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EN3hwOnojFXg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import sys\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from matplotlib import cm as cmap\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos de nuestro problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s_Z9r7rnj1FV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1562837846563</th>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>...</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562837909199</th>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>162.1</td>\n",
       "      <td>...</td>\n",
       "      <td>172.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>172.1</td>\n",
       "      <td>172.1</td>\n",
       "      <td>172.1</td>\n",
       "      <td>172.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562837972444</th>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>...</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.5</td>\n",
       "      <td>172.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562838035703</th>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>...</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562838103028</th>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>161.2</td>\n",
       "      <td>...</td>\n",
       "      <td>171.6</td>\n",
       "      <td>171.6</td>\n",
       "      <td>171.6</td>\n",
       "      <td>171.6</td>\n",
       "      <td>171.6</td>\n",
       "      <td>171.6</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1      2      3      4      5      6      7      8  \\\n",
       "1562837846563  162.3  162.3  162.3  162.3  162.3  162.3  162.3  162.3  162.3   \n",
       "1562837909199  162.1  162.1  162.1  162.1  162.1  162.1  162.1  162.1  162.1   \n",
       "1562837972444  158.4  158.4  158.4  158.4  158.4  158.4  158.4  158.4  158.4   \n",
       "1562838035703  158.6  158.6  158.6  158.6  158.6  158.6  158.6  158.6  158.6   \n",
       "1562838103028  161.2  161.2  161.2  161.2  161.2  161.2  161.2  161.2  161.2   \n",
       "\n",
       "                   9  ...    170    171    172    173    174    175    176  \\\n",
       "1562837846563  162.3  ...  171.3  171.3  171.3  171.3  171.3  171.3  171.3   \n",
       "1562837909199  162.1  ...  172.0  172.0  172.0  172.0  172.0  172.0  172.1   \n",
       "1562837972444  158.4  ...  172.4  172.4  172.4  172.4  172.4  172.4  172.4   \n",
       "1562838035703  158.6  ...  171.8  171.8  171.8  171.8  171.8  171.8  171.8   \n",
       "1562838103028  161.2  ...  171.6  171.6  171.6  171.6  171.6  171.6  171.7   \n",
       "\n",
       "                 177    178    179  \n",
       "1562837846563  171.3  171.3  171.3  \n",
       "1562837909199  172.1  172.1  172.1  \n",
       "1562837972444  172.4  172.5  172.5  \n",
       "1562838035703  171.8  171.8  171.8  \n",
       "1562838103028  171.7  171.7  171.7  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 180)\n"
     ]
    }
   ],
   "source": [
    "index_file = 1\n",
    "files = ['/data/cell_TP.csv','/data/cell_TB.csv','/data/cell_PW.csv','/data/cell_RC.csv']\n",
    "ds = pd.read_csv(files[index_file], index_col=0)\n",
    "display(ds.head())\n",
    "print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mq5Ys76UkrBP",
    "outputId": "091bec61-cef7-48cf-f912-1ebd04f3552e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label = 'cluster_id'\n",
    "labels = pd.read_csv('/data/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>maximum</th>\n",
       "      <th>minimum</th>\n",
       "      <th>mean</th>\n",
       "      <th>peak_to_peak</th>\n",
       "      <th>trimmed_mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>standard_deviation</th>\n",
       "      <th>mean_abs_deviation</th>\n",
       "      <th>median_abs_deviation</th>\n",
       "      <th>...</th>\n",
       "      <th>frequency_center</th>\n",
       "      <th>rms_frequency</th>\n",
       "      <th>sd_frequency</th>\n",
       "      <th>largest_freq_amp</th>\n",
       "      <th>largest_freq_idx</th>\n",
       "      <th>largest_sideband_amp</th>\n",
       "      <th>sideband_index</th>\n",
       "      <th>sideband_level_factor</th>\n",
       "      <th>figure_of_merit</th>\n",
       "      <th>cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1562837846563</td>\n",
       "      <td>305.45</td>\n",
       "      <td>161.35</td>\n",
       "      <td>213.275596</td>\n",
       "      <td>466.80</td>\n",
       "      <td>217.402577</td>\n",
       "      <td>1067.841483</td>\n",
       "      <td>32.677844</td>\n",
       "      <td>29.293215</td>\n",
       "      <td>28.140415</td>\n",
       "      <td>...</td>\n",
       "      <td>8.008879</td>\n",
       "      <td>21.744025</td>\n",
       "      <td>20.490150</td>\n",
       "      <td>4.318028</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>2.366411</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.011341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1562837909199</td>\n",
       "      <td>321.72</td>\n",
       "      <td>163.71</td>\n",
       "      <td>216.213158</td>\n",
       "      <td>485.43</td>\n",
       "      <td>220.065313</td>\n",
       "      <td>1087.538834</td>\n",
       "      <td>32.977854</td>\n",
       "      <td>29.221867</td>\n",
       "      <td>28.091053</td>\n",
       "      <td>...</td>\n",
       "      <td>10.824146</td>\n",
       "      <td>25.863114</td>\n",
       "      <td>20.420879</td>\n",
       "      <td>4.030683</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>2.726474</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.012466</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1562837972444</td>\n",
       "      <td>326.63</td>\n",
       "      <td>168.69</td>\n",
       "      <td>218.770158</td>\n",
       "      <td>495.32</td>\n",
       "      <td>222.742917</td>\n",
       "      <td>1041.748219</td>\n",
       "      <td>32.276125</td>\n",
       "      <td>28.595067</td>\n",
       "      <td>27.022263</td>\n",
       "      <td>...</td>\n",
       "      <td>11.047986</td>\n",
       "      <td>26.233696</td>\n",
       "      <td>20.658063</td>\n",
       "      <td>3.861453</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>2.506141</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.011916</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1562838035703</td>\n",
       "      <td>316.56</td>\n",
       "      <td>168.14</td>\n",
       "      <td>219.490211</td>\n",
       "      <td>484.70</td>\n",
       "      <td>223.064167</td>\n",
       "      <td>1033.828423</td>\n",
       "      <td>32.153202</td>\n",
       "      <td>28.757844</td>\n",
       "      <td>27.464316</td>\n",
       "      <td>...</td>\n",
       "      <td>10.482651</td>\n",
       "      <td>25.267198</td>\n",
       "      <td>20.726171</td>\n",
       "      <td>3.993014</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>2.433635</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1562838103028</td>\n",
       "      <td>332.00</td>\n",
       "      <td>170.01</td>\n",
       "      <td>219.675497</td>\n",
       "      <td>502.01</td>\n",
       "      <td>222.354124</td>\n",
       "      <td>1005.580535</td>\n",
       "      <td>31.710890</td>\n",
       "      <td>27.902710</td>\n",
       "      <td>27.299895</td>\n",
       "      <td>...</td>\n",
       "      <td>10.763227</td>\n",
       "      <td>25.680438</td>\n",
       "      <td>20.848452</td>\n",
       "      <td>3.748432</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>2.683621</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>0.011965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1571112943712</td>\n",
       "      <td>248.44</td>\n",
       "      <td>160.03</td>\n",
       "      <td>210.865403</td>\n",
       "      <td>408.47</td>\n",
       "      <td>216.855421</td>\n",
       "      <td>726.255337</td>\n",
       "      <td>26.949125</td>\n",
       "      <td>24.684338</td>\n",
       "      <td>22.315024</td>\n",
       "      <td>...</td>\n",
       "      <td>4.168542</td>\n",
       "      <td>15.033020</td>\n",
       "      <td>22.152112</td>\n",
       "      <td>3.207209</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>1.309421</td>\n",
       "      <td>12.121212</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1571113010123</td>\n",
       "      <td>237.51</td>\n",
       "      <td>160.12</td>\n",
       "      <td>207.963555</td>\n",
       "      <td>397.63</td>\n",
       "      <td>213.284673</td>\n",
       "      <td>632.337518</td>\n",
       "      <td>25.146322</td>\n",
       "      <td>22.740359</td>\n",
       "      <td>21.463270</td>\n",
       "      <td>...</td>\n",
       "      <td>4.199836</td>\n",
       "      <td>15.390785</td>\n",
       "      <td>21.841669</td>\n",
       "      <td>3.402772</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>1.224823</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.009062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1571113076527</td>\n",
       "      <td>249.03</td>\n",
       "      <td>160.80</td>\n",
       "      <td>208.938152</td>\n",
       "      <td>409.83</td>\n",
       "      <td>213.845607</td>\n",
       "      <td>673.352787</td>\n",
       "      <td>25.949042</td>\n",
       "      <td>23.396685</td>\n",
       "      <td>22.114692</td>\n",
       "      <td>...</td>\n",
       "      <td>4.497012</td>\n",
       "      <td>15.665329</td>\n",
       "      <td>21.943483</td>\n",
       "      <td>3.378990</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>1.301198</td>\n",
       "      <td>12.121212</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1571113147315</td>\n",
       "      <td>244.79</td>\n",
       "      <td>161.20</td>\n",
       "      <td>209.198143</td>\n",
       "      <td>405.99</td>\n",
       "      <td>213.976604</td>\n",
       "      <td>633.721875</td>\n",
       "      <td>25.173833</td>\n",
       "      <td>22.578966</td>\n",
       "      <td>21.653095</td>\n",
       "      <td>...</td>\n",
       "      <td>4.933193</td>\n",
       "      <td>16.995465</td>\n",
       "      <td>21.861492</td>\n",
       "      <td>3.433354</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>1.247520</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.009241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1571113213720</td>\n",
       "      <td>248.69</td>\n",
       "      <td>157.76</td>\n",
       "      <td>209.436981</td>\n",
       "      <td>406.45</td>\n",
       "      <td>215.777642</td>\n",
       "      <td>754.751859</td>\n",
       "      <td>27.472748</td>\n",
       "      <td>24.854391</td>\n",
       "      <td>22.889057</td>\n",
       "      <td>...</td>\n",
       "      <td>4.615904</td>\n",
       "      <td>15.894384</td>\n",
       "      <td>22.106684</td>\n",
       "      <td>3.131782</td>\n",
       "      <td>4.040404</td>\n",
       "      <td>1.196984</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>0.009154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  maximum  minimum        mean  peak_to_peak  trimmed_mean  \\\n",
       "0    1562837846563   305.45   161.35  213.275596        466.80    217.402577   \n",
       "1    1562837909199   321.72   163.71  216.213158        485.43    220.065313   \n",
       "2    1562837972444   326.63   168.69  218.770158        495.32    222.742917   \n",
       "3    1562838035703   316.56   168.14  219.490211        484.70    223.064167   \n",
       "4    1562838103028   332.00   170.01  219.675497        502.01    222.354124   \n",
       "..             ...      ...      ...         ...           ...           ...   \n",
       "270  1571112943712   248.44   160.03  210.865403        408.47    216.855421   \n",
       "271  1571113010123   237.51   160.12  207.963555        397.63    213.284673   \n",
       "272  1571113076527   249.03   160.80  208.938152        409.83    213.845607   \n",
       "273  1571113147315   244.79   161.20  209.198143        405.99    213.976604   \n",
       "274  1571113213720   248.69   157.76  209.436981        406.45    215.777642   \n",
       "\n",
       "        variance  standard_deviation  mean_abs_deviation  \\\n",
       "0    1067.841483           32.677844           29.293215   \n",
       "1    1087.538834           32.977854           29.221867   \n",
       "2    1041.748219           32.276125           28.595067   \n",
       "3    1033.828423           32.153202           28.757844   \n",
       "4    1005.580535           31.710890           27.902710   \n",
       "..           ...                 ...                 ...   \n",
       "270   726.255337           26.949125           24.684338   \n",
       "271   632.337518           25.146322           22.740359   \n",
       "272   673.352787           25.949042           23.396685   \n",
       "273   633.721875           25.173833           22.578966   \n",
       "274   754.751859           27.472748           24.854391   \n",
       "\n",
       "     median_abs_deviation  ...  frequency_center  rms_frequency  sd_frequency  \\\n",
       "0               28.140415  ...          8.008879      21.744025     20.490150   \n",
       "1               28.091053  ...         10.824146      25.863114     20.420879   \n",
       "2               27.022263  ...         11.047986      26.233696     20.658063   \n",
       "3               27.464316  ...         10.482651      25.267198     20.726171   \n",
       "4               27.299895  ...         10.763227      25.680438     20.848452   \n",
       "..                    ...  ...               ...            ...           ...   \n",
       "270             22.315024  ...          4.168542      15.033020     22.152112   \n",
       "271             21.463270  ...          4.199836      15.390785     21.841669   \n",
       "272             22.114692  ...          4.497012      15.665329     21.943483   \n",
       "273             21.653095  ...          4.933193      16.995465     21.861492   \n",
       "274             22.889057  ...          4.615904      15.894384     22.106684   \n",
       "\n",
       "     largest_freq_amp  largest_freq_idx  largest_sideband_amp  sideband_index  \\\n",
       "0            4.318028          4.040404              2.366411        9.090909   \n",
       "1            4.030683          4.040404              2.726474        9.090909   \n",
       "2            3.861453          4.040404              2.506141        9.090909   \n",
       "3            3.993014          4.040404              2.433635        9.090909   \n",
       "4            3.748432          4.040404              2.683621        9.090909   \n",
       "..                ...               ...                   ...             ...   \n",
       "270          3.207209          4.040404              1.309421       12.121212   \n",
       "271          3.402772          4.040404              1.224823        9.090909   \n",
       "272          3.378990          4.040404              1.301198       12.121212   \n",
       "273          3.433354          4.040404              1.247520        9.090909   \n",
       "274          3.131782          4.040404              1.196984        9.090909   \n",
       "\n",
       "     sideband_level_factor  figure_of_merit  cluster_id  \n",
       "0                 0.010968         0.011341           0  \n",
       "1                 0.012466         0.011817           0  \n",
       "2                 0.011333         0.011916           0  \n",
       "3                 0.010971         0.011623           0  \n",
       "4                 0.012091         0.011965           0  \n",
       "..                     ...              ...         ...  \n",
       "270               0.006160         0.009181           1  \n",
       "271               0.005847         0.009062           1  \n",
       "272               0.006180         0.009296           1  \n",
       "273               0.005921         0.009241           1  \n",
       "274               0.005667         0.009154           1  \n",
       "\n",
       "[275 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWajMxrprEGA",
    "outputId": "86eed7b5-b73e-49ea-c4bd-d926bc49f7df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [0 2 3 4 1]\n",
      "frq. dict_values([58, 20, 78, 2, 117])\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter(labels[label])\n",
    "print('labels', labels.cluster_id.unique())\n",
    "print('frq.', counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yFmRWwielefJ"
   },
   "outputs": [],
   "source": [
    "ds[label] = labels.cluster_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYjxvb1MCJk9"
   },
   "source": [
    "Si se quiere hacer clasificación binaria descomentar la siguiente línea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-NQFxvIh8FEK"
   },
   "outputs": [],
   "source": [
    "ds.loc[ds[ds[label] != 1].index, label] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos los datos en train y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LRP6zu4vqACM"
   },
   "outputs": [],
   "source": [
    "train_ds, test_ds = train_test_split(ds, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1566294266394</th>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>...</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.3</td>\n",
       "      <td>87.3</td>\n",
       "      <td>87.3</td>\n",
       "      <td>87.3</td>\n",
       "      <td>87.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563515374206</th>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>165.6</td>\n",
       "      <td>...</td>\n",
       "      <td>160.6</td>\n",
       "      <td>160.6</td>\n",
       "      <td>160.6</td>\n",
       "      <td>160.6</td>\n",
       "      <td>160.6</td>\n",
       "      <td>160.6</td>\n",
       "      <td>160.6</td>\n",
       "      <td>160.6</td>\n",
       "      <td>160.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567743025494</th>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>148.5</td>\n",
       "      <td>...</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569240773903</th>\n",
       "      <td>147.3</td>\n",
       "      <td>147.3</td>\n",
       "      <td>147.3</td>\n",
       "      <td>147.3</td>\n",
       "      <td>147.3</td>\n",
       "      <td>147.3</td>\n",
       "      <td>147.3</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>...</td>\n",
       "      <td>170.0</td>\n",
       "      <td>169.9</td>\n",
       "      <td>169.9</td>\n",
       "      <td>169.9</td>\n",
       "      <td>169.9</td>\n",
       "      <td>169.9</td>\n",
       "      <td>169.9</td>\n",
       "      <td>169.9</td>\n",
       "      <td>169.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571113213720</th>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>150.4</td>\n",
       "      <td>...</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564023401591</th>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>...</td>\n",
       "      <td>173.1</td>\n",
       "      <td>173.1</td>\n",
       "      <td>173.1</td>\n",
       "      <td>173.1</td>\n",
       "      <td>173.1</td>\n",
       "      <td>173.1</td>\n",
       "      <td>173.1</td>\n",
       "      <td>173.1</td>\n",
       "      <td>173.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564645817522</th>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>...</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568002278276</th>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>148.9</td>\n",
       "      <td>...</td>\n",
       "      <td>168.4</td>\n",
       "      <td>168.4</td>\n",
       "      <td>168.4</td>\n",
       "      <td>168.4</td>\n",
       "      <td>168.4</td>\n",
       "      <td>168.4</td>\n",
       "      <td>168.4</td>\n",
       "      <td>168.5</td>\n",
       "      <td>168.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571113076527</th>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>149.3</td>\n",
       "      <td>...</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562838297406</th>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>...</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567743236899</th>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>...</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564023338203</th>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>160.2</td>\n",
       "      <td>...</td>\n",
       "      <td>173.9</td>\n",
       "      <td>173.9</td>\n",
       "      <td>173.9</td>\n",
       "      <td>173.9</td>\n",
       "      <td>173.9</td>\n",
       "      <td>173.9</td>\n",
       "      <td>173.9</td>\n",
       "      <td>173.9</td>\n",
       "      <td>173.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567742892690</th>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>...</td>\n",
       "      <td>175.2</td>\n",
       "      <td>175.2</td>\n",
       "      <td>175.2</td>\n",
       "      <td>175.2</td>\n",
       "      <td>175.2</td>\n",
       "      <td>175.2</td>\n",
       "      <td>175.2</td>\n",
       "      <td>175.2</td>\n",
       "      <td>175.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569240504175</th>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>...</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571112877295</th>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>149.8</td>\n",
       "      <td>...</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563852826691</th>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>158.9</td>\n",
       "      <td>...</td>\n",
       "      <td>169.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564643972288</th>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>159.7</td>\n",
       "      <td>...</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570439292852</th>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>154.1</td>\n",
       "      <td>...</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562904950590</th>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>...</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570507708083</th>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>...</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571029477837</th>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>152.3</td>\n",
       "      <td>...</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569297125516</th>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>...</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563852763621</th>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>160.9</td>\n",
       "      <td>...</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>169.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568002145469</th>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>149.2</td>\n",
       "      <td>...</td>\n",
       "      <td>173.8</td>\n",
       "      <td>173.8</td>\n",
       "      <td>173.8</td>\n",
       "      <td>173.8</td>\n",
       "      <td>173.7</td>\n",
       "      <td>173.7</td>\n",
       "      <td>173.7</td>\n",
       "      <td>173.7</td>\n",
       "      <td>173.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563988000170</th>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>161.6</td>\n",
       "      <td>...</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566559428210</th>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>154.4</td>\n",
       "      <td>...</td>\n",
       "      <td>173.6</td>\n",
       "      <td>173.6</td>\n",
       "      <td>173.6</td>\n",
       "      <td>173.6</td>\n",
       "      <td>173.6</td>\n",
       "      <td>173.6</td>\n",
       "      <td>173.6</td>\n",
       "      <td>173.6</td>\n",
       "      <td>173.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566998321804</th>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>148.7</td>\n",
       "      <td>...</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>169.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563515444859</th>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566388730248</th>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>156.6</td>\n",
       "      <td>...</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566964635819</th>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>151.3</td>\n",
       "      <td>...</td>\n",
       "      <td>179.3</td>\n",
       "      <td>179.3</td>\n",
       "      <td>179.3</td>\n",
       "      <td>179.3</td>\n",
       "      <td>179.2</td>\n",
       "      <td>179.2</td>\n",
       "      <td>179.2</td>\n",
       "      <td>179.2</td>\n",
       "      <td>179.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563516957602</th>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>159.5</td>\n",
       "      <td>...</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566538450724</th>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>152.8</td>\n",
       "      <td>...</td>\n",
       "      <td>171.2</td>\n",
       "      <td>171.2</td>\n",
       "      <td>171.2</td>\n",
       "      <td>171.2</td>\n",
       "      <td>171.2</td>\n",
       "      <td>171.2</td>\n",
       "      <td>171.2</td>\n",
       "      <td>171.2</td>\n",
       "      <td>171.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569240707507</th>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>150.2</td>\n",
       "      <td>...</td>\n",
       "      <td>170.4</td>\n",
       "      <td>170.4</td>\n",
       "      <td>170.4</td>\n",
       "      <td>170.4</td>\n",
       "      <td>170.4</td>\n",
       "      <td>170.4</td>\n",
       "      <td>170.4</td>\n",
       "      <td>170.4</td>\n",
       "      <td>170.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566998544587</th>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>...</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>171.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563901282330</th>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>...</td>\n",
       "      <td>172.9</td>\n",
       "      <td>172.9</td>\n",
       "      <td>172.9</td>\n",
       "      <td>172.9</td>\n",
       "      <td>172.9</td>\n",
       "      <td>172.9</td>\n",
       "      <td>172.9</td>\n",
       "      <td>172.9</td>\n",
       "      <td>172.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569298493008</th>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>147.2</td>\n",
       "      <td>...</td>\n",
       "      <td>179.4</td>\n",
       "      <td>179.4</td>\n",
       "      <td>179.4</td>\n",
       "      <td>179.4</td>\n",
       "      <td>179.4</td>\n",
       "      <td>179.4</td>\n",
       "      <td>179.4</td>\n",
       "      <td>179.4</td>\n",
       "      <td>179.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566391521124</th>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>160.3</td>\n",
       "      <td>...</td>\n",
       "      <td>170.9</td>\n",
       "      <td>170.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570623140418</th>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>151.1</td>\n",
       "      <td>...</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570438956529</th>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>154.3</td>\n",
       "      <td>...</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563421794988</th>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>162.3</td>\n",
       "      <td>...</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>174.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566464467266</th>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>167.9</td>\n",
       "      <td>...</td>\n",
       "      <td>161.9</td>\n",
       "      <td>161.9</td>\n",
       "      <td>161.9</td>\n",
       "      <td>161.9</td>\n",
       "      <td>161.9</td>\n",
       "      <td>161.9</td>\n",
       "      <td>161.9</td>\n",
       "      <td>161.9</td>\n",
       "      <td>161.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567743373974</th>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>148.8</td>\n",
       "      <td>...</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566979402848</th>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>154.7</td>\n",
       "      <td>...</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>170.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563163743835</th>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>...</td>\n",
       "      <td>165.3</td>\n",
       "      <td>165.3</td>\n",
       "      <td>165.3</td>\n",
       "      <td>165.3</td>\n",
       "      <td>165.3</td>\n",
       "      <td>165.3</td>\n",
       "      <td>165.3</td>\n",
       "      <td>165.2</td>\n",
       "      <td>165.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566997847819</th>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>152.5</td>\n",
       "      <td>...</td>\n",
       "      <td>168.5</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>168.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566463036306</th>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>163.1</td>\n",
       "      <td>...</td>\n",
       "      <td>166.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569240433567</th>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>146.2</td>\n",
       "      <td>...</td>\n",
       "      <td>177.2</td>\n",
       "      <td>177.2</td>\n",
       "      <td>177.2</td>\n",
       "      <td>177.2</td>\n",
       "      <td>177.2</td>\n",
       "      <td>177.2</td>\n",
       "      <td>177.2</td>\n",
       "      <td>177.2</td>\n",
       "      <td>177.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562904631274</th>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>157.3</td>\n",
       "      <td>...</td>\n",
       "      <td>176.3</td>\n",
       "      <td>176.3</td>\n",
       "      <td>176.3</td>\n",
       "      <td>176.3</td>\n",
       "      <td>176.3</td>\n",
       "      <td>176.3</td>\n",
       "      <td>176.3</td>\n",
       "      <td>176.3</td>\n",
       "      <td>176.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566294587838</th>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>...</td>\n",
       "      <td>124.1</td>\n",
       "      <td>124.1</td>\n",
       "      <td>124.1</td>\n",
       "      <td>124.1</td>\n",
       "      <td>124.1</td>\n",
       "      <td>124.1</td>\n",
       "      <td>124.1</td>\n",
       "      <td>124.1</td>\n",
       "      <td>124.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563850448961</th>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>162.6</td>\n",
       "      <td>...</td>\n",
       "      <td>179.8</td>\n",
       "      <td>179.8</td>\n",
       "      <td>179.8</td>\n",
       "      <td>179.8</td>\n",
       "      <td>179.8</td>\n",
       "      <td>179.8</td>\n",
       "      <td>179.8</td>\n",
       "      <td>179.8</td>\n",
       "      <td>179.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563901484037</th>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>158.8</td>\n",
       "      <td>...</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566538249625</th>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>146.7</td>\n",
       "      <td>...</td>\n",
       "      <td>177.9</td>\n",
       "      <td>177.9</td>\n",
       "      <td>177.9</td>\n",
       "      <td>177.9</td>\n",
       "      <td>177.9</td>\n",
       "      <td>177.9</td>\n",
       "      <td>177.9</td>\n",
       "      <td>177.9</td>\n",
       "      <td>177.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563852383524</th>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.3</td>\n",
       "      <td>159.4</td>\n",
       "      <td>159.4</td>\n",
       "      <td>...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568002211869</th>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>147.1</td>\n",
       "      <td>...</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562837972444</th>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>158.4</td>\n",
       "      <td>...</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.5</td>\n",
       "      <td>172.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1      2      3      4      5      6      7      8  \\\n",
       "1566294266394   78.4   78.4   78.4   78.4   78.4   78.4   78.4   78.4   78.4   \n",
       "1563515374206  165.6  165.6  165.6  165.6  165.6  165.6  165.6  165.6  165.6   \n",
       "1567743025494  148.5  148.5  148.5  148.5  148.5  148.5  148.5  148.5  148.5   \n",
       "1569240773903  147.3  147.3  147.3  147.3  147.3  147.3  147.3  147.2  147.2   \n",
       "1571113213720  150.4  150.4  150.4  150.4  150.4  150.4  150.4  150.4  150.4   \n",
       "1564023401591  161.0  161.0  161.0  161.0  161.0  161.0  161.0  161.0  161.0   \n",
       "1564645817522  159.0  159.0  159.0  159.0  159.0  159.0  159.0  159.0  159.0   \n",
       "1568002278276  148.9  148.9  148.9  148.9  148.9  148.9  148.9  148.9  148.9   \n",
       "1571113076527  149.3  149.3  149.3  149.3  149.3  149.3  149.3  149.3  149.3   \n",
       "1562838297406  159.3  159.3  159.3  159.3  159.3  159.4  159.4  159.4  159.4   \n",
       "1567743236899  150.0  150.0  150.0  150.0  150.0  150.0  150.0  150.0  150.0   \n",
       "1564023338203  160.2  160.2  160.2  160.2  160.2  160.2  160.2  160.2  160.2   \n",
       "1567742892690  148.3  148.3  148.3  148.3  148.3  148.3  148.3  148.3  148.3   \n",
       "1569240504175  148.8  148.8  148.8  148.8  148.8  148.8  148.8  148.8  148.8   \n",
       "1571112877295  149.8  149.8  149.8  149.8  149.8  149.8  149.8  149.8  149.8   \n",
       "1563852826691  158.9  158.9  158.9  158.9  158.9  158.9  158.9  158.9  158.9   \n",
       "1564643972288  159.7  159.7  159.7  159.7  159.7  159.7  159.7  159.7  159.7   \n",
       "1570439292852  154.1  154.1  154.1  154.1  154.1  154.1  154.1  154.1  154.1   \n",
       "1562904950590  159.3  159.3  159.3  159.3  159.3  159.3  159.3  159.3  159.3   \n",
       "1570507708083  147.5  147.5  147.5  147.5  147.5  147.5  147.5  147.5  147.5   \n",
       "1571029477837  152.3  152.3  152.3  152.3  152.3  152.3  152.3  152.3  152.3   \n",
       "1569297125516  152.8  152.8  152.8  152.8  152.8  152.8  152.8  152.8  152.8   \n",
       "1563852763621  160.9  160.9  160.9  160.9  160.9  160.9  160.9  160.9  160.9   \n",
       "1568002145469  149.2  149.2  149.2  149.2  149.2  149.2  149.2  149.2  149.2   \n",
       "1563988000170  161.6  161.6  161.6  161.6  161.6  161.6  161.6  161.6  161.6   \n",
       "1566559428210  154.4  154.4  154.4  154.4  154.4  154.4  154.4  154.4  154.4   \n",
       "1566998321804  148.7  148.7  148.7  148.7  148.7  148.7  148.7  148.7  148.7   \n",
       "1563515444859  160.0  160.0  160.0  160.0  160.0  160.0  160.0  160.0  160.0   \n",
       "1566388730248  156.6  156.6  156.6  156.6  156.6  156.6  156.6  156.6  156.6   \n",
       "1566964635819  151.3  151.3  151.3  151.3  151.3  151.3  151.3  151.3  151.3   \n",
       "1563516957602  159.5  159.5  159.5  159.5  159.5  159.5  159.5  159.5  159.5   \n",
       "1566538450724  152.8  152.8  152.8  152.8  152.8  152.8  152.8  152.8  152.8   \n",
       "1569240707507  150.2  150.2  150.2  150.2  150.2  150.2  150.2  150.2  150.2   \n",
       "1566998544587  154.3  154.3  154.3  154.3  154.3  154.3  154.3  154.3  154.3   \n",
       "1563901282330  159.8  159.8  159.8  159.8  159.8  159.8  159.8  159.8  159.8   \n",
       "1569298493008  147.2  147.2  147.2  147.2  147.2  147.2  147.2  147.2  147.2   \n",
       "1566391521124  160.3  160.3  160.3  160.3  160.3  160.3  160.3  160.3  160.3   \n",
       "1570623140418  151.1  151.1  151.1  151.1  151.1  151.1  151.1  151.1  151.1   \n",
       "1570438956529  154.3  154.3  154.3  154.3  154.3  154.3  154.3  154.3  154.3   \n",
       "1563421794988  162.3  162.3  162.3  162.3  162.3  162.3  162.3  162.3  162.3   \n",
       "1566464467266  167.9  167.9  167.9  167.9  167.9  167.9  167.9  167.9  167.9   \n",
       "1567743373974  148.8  148.8  148.8  148.8  148.8  148.8  148.8  148.8  148.8   \n",
       "1566979402848  154.7  154.7  154.7  154.7  154.7  154.7  154.7  154.7  154.7   \n",
       "1563163743835  159.4  159.4  159.4  159.4  159.4  159.4  159.4  159.4  159.4   \n",
       "1566997847819  152.5  152.5  152.5  152.5  152.5  152.5  152.5  152.5  152.5   \n",
       "1566463036306  163.1  163.1  163.1  163.1  163.1  163.1  163.1  163.1  163.1   \n",
       "1569240433567  146.2  146.2  146.2  146.2  146.2  146.2  146.2  146.2  146.2   \n",
       "1562904631274  157.3  157.3  157.3  157.3  157.3  157.3  157.3  157.3  157.3   \n",
       "1566294587838   93.4   93.4   93.4   93.4   93.4   93.4   93.4   93.4   93.4   \n",
       "1563850448961  162.6  162.6  162.6  162.6  162.6  162.6  162.6  162.6  162.6   \n",
       "1563901484037  158.8  158.8  158.8  158.8  158.8  158.8  158.8  158.8  158.8   \n",
       "1566538249625  146.7  146.7  146.7  146.7  146.7  146.7  146.7  146.7  146.7   \n",
       "1563852383524  159.3  159.3  159.3  159.3  159.3  159.3  159.3  159.3  159.4   \n",
       "1568002211869  147.1  147.1  147.1  147.1  147.1  147.1  147.1  147.1  147.1   \n",
       "1562837972444  158.4  158.4  158.4  158.4  158.4  158.4  158.4  158.4  158.4   \n",
       "\n",
       "                   9  ...    171    172    173    174    175    176    177  \\\n",
       "1566294266394   78.4  ...   87.1   87.1   87.1   87.1   87.3   87.3   87.3   \n",
       "1563515374206  165.6  ...  160.6  160.6  160.6  160.6  160.6  160.6  160.6   \n",
       "1567743025494  148.5  ...  171.1  171.1  171.1  171.1  171.1  171.1  171.1   \n",
       "1569240773903  147.2  ...  170.0  169.9  169.9  169.9  169.9  169.9  169.9   \n",
       "1571113213720  150.4  ...  170.2  170.2  170.2  170.2  170.2  170.2  170.2   \n",
       "1564023401591  161.0  ...  173.1  173.1  173.1  173.1  173.1  173.1  173.1   \n",
       "1564645817522  159.0  ...  169.3  169.3  169.3  169.3  169.3  169.3  169.3   \n",
       "1568002278276  148.9  ...  168.4  168.4  168.4  168.4  168.4  168.4  168.4   \n",
       "1571113076527  149.3  ...  170.0  170.0  170.0  170.0  170.0  170.0  170.0   \n",
       "1562838297406  159.4  ...  171.7  171.7  171.7  171.7  171.7  171.7  171.7   \n",
       "1567743236899  150.0  ...  170.0  170.0  170.0  170.0  170.0  170.0  170.0   \n",
       "1564023338203  160.2  ...  173.9  173.9  173.9  173.9  173.9  173.9  173.9   \n",
       "1567742892690  148.3  ...  175.2  175.2  175.2  175.2  175.2  175.2  175.2   \n",
       "1569240504175  148.8  ...  174.4  174.4  174.4  174.4  174.4  174.4  174.4   \n",
       "1571112877295  149.8  ...  169.6  169.6  169.6  169.7  169.7  169.7  169.7   \n",
       "1563852826691  158.9  ...  169.1  169.1  169.1  169.1  169.1  169.1  169.1   \n",
       "1564643972288  159.7  ...  171.1  171.1  171.1  171.1  171.1  171.1  171.1   \n",
       "1570439292852  154.1  ...  170.2  170.2  170.2  170.2  170.2  170.2  170.2   \n",
       "1562904950590  159.3  ...  172.4  172.4  172.4  172.4  172.4  172.4  172.4   \n",
       "1570507708083  147.5  ...  178.0  178.0  178.0  178.0  178.0  178.0  178.0   \n",
       "1571029477837  152.3  ...  174.4  174.4  174.4  174.4  174.4  174.4  174.4   \n",
       "1569297125516  152.8  ...  168.8  168.8  168.8  168.8  168.8  168.8  168.8   \n",
       "1563852763621  160.9  ...  169.3  169.3  169.3  169.3  169.3  169.3  169.3   \n",
       "1568002145469  149.2  ...  173.8  173.8  173.8  173.8  173.7  173.7  173.7   \n",
       "1563988000170  161.6  ...  171.0  171.0  171.0  171.0  171.0  171.0  171.0   \n",
       "1566559428210  154.4  ...  173.6  173.6  173.6  173.6  173.6  173.6  173.6   \n",
       "1566998321804  148.7  ...  169.5  169.6  169.6  169.6  169.6  169.6  169.6   \n",
       "1563515444859  160.0  ...  161.0  161.0  161.0  161.0  161.0  161.0  161.0   \n",
       "1566388730248  156.6  ...  171.0  171.0  171.0  171.0  171.0  171.0  171.0   \n",
       "1566964635819  151.3  ...  179.3  179.3  179.3  179.3  179.2  179.2  179.2   \n",
       "1563516957602  159.5  ...  167.9  167.9  167.9  167.9  167.9  167.9  167.9   \n",
       "1566538450724  152.8  ...  171.2  171.2  171.2  171.2  171.2  171.2  171.2   \n",
       "1569240707507  150.2  ...  170.4  170.4  170.4  170.4  170.4  170.4  170.4   \n",
       "1566998544587  154.3  ...  171.3  171.3  171.3  171.3  171.3  171.3  171.3   \n",
       "1563901282330  159.8  ...  172.9  172.9  172.9  172.9  172.9  172.9  172.9   \n",
       "1569298493008  147.2  ...  179.4  179.4  179.4  179.4  179.4  179.4  179.4   \n",
       "1566391521124  160.3  ...  170.9  170.8  170.8  170.8  170.8  170.8  170.8   \n",
       "1570623140418  151.1  ...  169.7  169.7  169.7  169.7  169.7  169.7  169.7   \n",
       "1570438956529  154.3  ...  171.8  171.8  171.8  171.8  171.8  171.8  171.8   \n",
       "1563421794988  162.3  ...  174.4  174.4  174.4  174.4  174.4  174.4  174.4   \n",
       "1566464467266  167.9  ...  161.9  161.9  161.9  161.9  161.9  161.9  161.9   \n",
       "1567743373974  148.8  ...  170.2  170.2  170.2  170.2  170.2  170.2  170.2   \n",
       "1566979402848  154.7  ...  170.2  170.2  170.2  170.2  170.2  170.2  170.2   \n",
       "1563163743835  159.4  ...  165.3  165.3  165.3  165.3  165.3  165.3  165.3   \n",
       "1566997847819  152.5  ...  168.5  168.8  168.8  168.8  168.8  168.8  168.8   \n",
       "1566463036306  163.1  ...  166.5  166.5  166.5  166.5  166.5  166.5  166.5   \n",
       "1569240433567  146.2  ...  177.2  177.2  177.2  177.2  177.2  177.2  177.2   \n",
       "1562904631274  157.3  ...  176.3  176.3  176.3  176.3  176.3  176.3  176.3   \n",
       "1566294587838   93.4  ...  124.1  124.1  124.1  124.1  124.1  124.1  124.1   \n",
       "1563850448961  162.6  ...  179.8  179.8  179.8  179.8  179.8  179.8  179.8   \n",
       "1563901484037  158.8  ...  171.0  171.0  171.0  171.0  171.0  171.0  171.0   \n",
       "1566538249625  146.7  ...  177.9  177.9  177.9  177.9  177.9  177.9  177.9   \n",
       "1563852383524  159.4  ...  162.0  162.0  162.0  162.0  162.0  162.0  162.0   \n",
       "1568002211869  147.1  ...  169.7  169.7  169.7  169.7  169.7  169.7  169.7   \n",
       "1562837972444  158.4  ...  172.4  172.4  172.4  172.4  172.4  172.4  172.4   \n",
       "\n",
       "                 178    179  cluster_id  \n",
       "1566294266394   87.3   87.3           0  \n",
       "1563515374206  160.6  160.6           0  \n",
       "1567743025494  171.1  171.1           1  \n",
       "1569240773903  169.9  169.9           1  \n",
       "1571113213720  170.2  170.2           1  \n",
       "1564023401591  173.1  173.1           0  \n",
       "1564645817522  169.3  169.3           0  \n",
       "1568002278276  168.5  168.5           1  \n",
       "1571113076527  170.0  170.0           1  \n",
       "1562838297406  171.7  171.7           0  \n",
       "1567743236899  170.0  170.0           1  \n",
       "1564023338203  173.9  173.9           0  \n",
       "1567742892690  175.2  175.2           0  \n",
       "1569240504175  174.4  174.4           1  \n",
       "1571112877295  169.7  169.7           1  \n",
       "1563852826691  169.1  169.1           0  \n",
       "1564643972288  171.1  171.1           0  \n",
       "1570439292852  170.2  170.2           1  \n",
       "1562904950590  172.4  172.4           0  \n",
       "1570507708083  178.0  178.0           1  \n",
       "1571029477837  174.4  174.4           0  \n",
       "1569297125516  168.8  168.8           0  \n",
       "1563852763621  169.3  169.3           0  \n",
       "1568002145469  173.7  173.7           0  \n",
       "1563988000170  171.0  171.0           0  \n",
       "1566559428210  173.6  173.6           1  \n",
       "1566998321804  169.6  169.6           1  \n",
       "1563515444859  161.0  161.0           0  \n",
       "1566388730248  171.0  171.0           0  \n",
       "1566964635819  179.2  179.2           0  \n",
       "1563516957602  167.9  167.9           0  \n",
       "1566538450724  171.2  171.2           1  \n",
       "1569240707507  170.4  170.4           1  \n",
       "1566998544587  171.3  171.3           1  \n",
       "1563901282330  172.9  172.9           0  \n",
       "1569298493008  179.4  179.4           1  \n",
       "1566391521124  170.8  170.8           0  \n",
       "1570623140418  169.7  169.7           1  \n",
       "1570438956529  171.8  171.8           1  \n",
       "1563421794988  174.4  174.4           0  \n",
       "1566464467266  161.9  161.9           0  \n",
       "1567743373974  170.2  170.2           1  \n",
       "1566979402848  170.2  170.2           1  \n",
       "1563163743835  165.2  165.2           0  \n",
       "1566997847819  168.8  168.8           0  \n",
       "1566463036306  166.5  166.5           0  \n",
       "1569240433567  177.2  177.1           1  \n",
       "1562904631274  176.3  176.3           0  \n",
       "1566294587838  124.1  124.1           0  \n",
       "1563850448961  179.8  179.8           0  \n",
       "1563901484037  171.0  171.0           0  \n",
       "1566538249625  177.9  177.9           1  \n",
       "1563852383524  162.0  162.0           0  \n",
       "1568002211869  169.7  169.7           1  \n",
       "1562837972444  172.5  172.5           0  \n",
       "\n",
       "[55 rows x 181 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1563901420279</th>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>160.4</td>\n",
       "      <td>...</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>171.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562838035703</th>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>158.6</td>\n",
       "      <td>...</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>171.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570076181965</th>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>146.9</td>\n",
       "      <td>...</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563422973094</th>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>...</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571049730353</th>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>147.6</td>\n",
       "      <td>...</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570439160052</th>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>148.3</td>\n",
       "      <td>...</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>169.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570623074009</th>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>155.1</td>\n",
       "      <td>...</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>169.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563250342275</th>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>161.7</td>\n",
       "      <td>...</td>\n",
       "      <td>167.2</td>\n",
       "      <td>167.2</td>\n",
       "      <td>167.2</td>\n",
       "      <td>167.2</td>\n",
       "      <td>167.2</td>\n",
       "      <td>167.2</td>\n",
       "      <td>167.2</td>\n",
       "      <td>167.2</td>\n",
       "      <td>167.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562905013659</th>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>160.5</td>\n",
       "      <td>...</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.4</td>\n",
       "      <td>172.5</td>\n",
       "      <td>172.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563901219278</th>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>157.7</td>\n",
       "      <td>...</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>171.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1      2      3      4      5      6      7      8  \\\n",
       "1563901420279  160.4  160.4  160.4  160.4  160.4  160.4  160.4  160.4  160.4   \n",
       "1562838035703  158.6  158.6  158.6  158.6  158.6  158.6  158.6  158.6  158.6   \n",
       "1570076181965  146.9  146.9  146.9  146.9  146.9  146.9  146.9  146.9  146.9   \n",
       "1563422973094  159.1  159.1  159.1  159.1  159.1  159.1  159.1  159.1  159.1   \n",
       "1571049730353  147.6  147.6  147.6  147.6  147.6  147.6  147.6  147.6  147.6   \n",
       "...              ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1570439160052  148.3  148.3  148.3  148.3  148.3  148.3  148.3  148.3  148.3   \n",
       "1570623074009  155.1  155.1  155.1  155.1  155.1  155.1  155.1  155.1  155.1   \n",
       "1563250342275  161.7  161.7  161.7  161.7  161.7  161.7  161.7  161.7  161.7   \n",
       "1562905013659  160.5  160.5  160.5  160.5  160.5  160.5  160.5  160.5  160.5   \n",
       "1563901219278  157.7  157.7  157.7  157.7  157.7  157.7  157.7  157.7  157.7   \n",
       "\n",
       "                   9  ...    171    172    173    174    175    176    177  \\\n",
       "1563901420279  160.4  ...  171.1  171.1  171.1  171.1  171.1  171.1  171.1   \n",
       "1562838035703  158.6  ...  171.8  171.8  171.8  171.8  171.8  171.8  171.8   \n",
       "1570076181965  146.9  ...  170.0  170.0  170.0  170.0  170.0  170.0  170.0   \n",
       "1563422973094  159.1  ...  169.5  169.5  169.5  169.5  169.5  169.5  169.5   \n",
       "1571049730353  147.6  ...  170.1  170.1  170.1  170.1  170.1  170.1  170.1   \n",
       "...              ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1570439160052  148.3  ...  169.7  169.7  169.7  169.7  169.7  169.7  169.7   \n",
       "1570623074009  155.1  ...  169.5  169.5  169.5  169.5  169.5  169.5  169.5   \n",
       "1563250342275  161.7  ...  167.2  167.2  167.2  167.2  167.2  167.2  167.2   \n",
       "1562905013659  160.5  ...  172.4  172.4  172.4  172.4  172.4  172.4  172.4   \n",
       "1563901219278  157.7  ...  171.7  171.7  171.7  171.7  171.7  171.7  171.7   \n",
       "\n",
       "                 178    179  cluster_id  \n",
       "1563901420279  171.1  171.1           0  \n",
       "1562838035703  171.8  171.8           0  \n",
       "1570076181965  170.0  170.0           1  \n",
       "1563422973094  169.5  169.5           0  \n",
       "1571049730353  170.1  170.1           1  \n",
       "...              ...    ...         ...  \n",
       "1570439160052  169.7  169.7           1  \n",
       "1570623074009  169.5  169.5           1  \n",
       "1563250342275  167.2  167.2           0  \n",
       "1562905013659  172.5  172.5           0  \n",
       "1563901219278  171.7  171.7           0  \n",
       "\n",
       "[220 rows x 181 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_pNnZzwbCGf"
   },
   "source": [
    "# 1. AutoGluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoGluon enables easy-to-use and easy-to-extend AutoML with a focus on automated stack ensembling, deep learning, and real-world applications spanning text, image, and tabular data. Intended for both ML beginners and experts, AutoGluon enables you to:\n",
    "\n",
    "- Quickly prototype deep learning and classical ML solutions for your raw data with a few lines of code.\n",
    "- Automatically utilize state-of-the-art techniques (where appropriate) without expert knowledge.\n",
    "- Leverage automatic hyperparameter tuning, model selection/ensembling, architecture search, and data processing.\n",
    "- Easily improve/tune your bespoke models and data pipelines, or customize AutoGluon for your use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://auto.gluon.ai/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Specific Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor as task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define and train predictor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3__IWiDdCO91"
   },
   "source": [
    "El parámetro:\n",
    "- save_space = Borra los modelos entrenados al final del .fit\n",
    "- presets = Activa ensamblado y otras cosas. Consume recursos pero trata de obtimizar la métrica de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgG8fX3xlKEH",
    "outputId": "6ba01549-9c36-4897-b97a-b6f275187d0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"agModels-predictClass/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    220\n",
      "Train Data Columns: 180\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6599.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 180 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 180 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t180 features in original data used to generate 180 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 176, Val Rows: 44\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9773\t = Validation accuracy score\n",
      "\t0.01s\t = Training runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9773\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9318\t = Validation accuracy score\n",
      "\t0.86s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9318\t = Validation accuracy score\n",
      "\t1.73s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9318\t = Validation accuracy score\n",
      "\t0.85s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9318\t = Validation accuracy score\n",
      "\t0.67s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9773\t = Validation accuracy score\n",
      "\t2.13s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9318\t = Validation accuracy score\n",
      "\t0.57s\t = Training runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9318\t = Validation accuracy score\n",
      "\t0.68s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9318\t = Validation accuracy score\n",
      "\t6.8s\t = Training runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9318\t = Validation accuracy score\n",
      "\t0.52s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet ...\n",
      "\t0.9545\t = Validation accuracy score\n",
      "\t7.0s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9091\t = Validation accuracy score\n",
      "\t0.66s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9773\t = Validation accuracy score\n",
      "\t0.36s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 25.13s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictClass/\")\n"
     ]
    }
   ],
   "source": [
    "save_path = 'agModels-predictClass'  # specifies folder to store trained models\n",
    "predictor = task(label=label, path=save_path).fit(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "rHXy_86VllGJ",
    "outputId": "4aa6c34d-d278-436f-8455-36c8a0ca353a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>2.133220</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>2.133220</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.104870</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.104870</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.130010</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.130010</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.130585</td>\n",
       "      <td>0.363523</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.358281</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeuralNetMXNet</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.120138</td>\n",
       "      <td>6.996468</td>\n",
       "      <td>0.120138</td>\n",
       "      <td>6.996468</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>1.731732</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>1.731732</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.863779</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.863779</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.523260</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.523260</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.070858</td>\n",
       "      <td>6.801754</td>\n",
       "      <td>0.070858</td>\n",
       "      <td>6.801754</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreesGini</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.104239</td>\n",
       "      <td>0.573350</td>\n",
       "      <td>0.104239</td>\n",
       "      <td>0.573350</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ExtraTreesEntr</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.106039</td>\n",
       "      <td>0.682417</td>\n",
       "      <td>0.106039</td>\n",
       "      <td>0.682417</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.106150</td>\n",
       "      <td>0.853484</td>\n",
       "      <td>0.106150</td>\n",
       "      <td>0.853484</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.106325</td>\n",
       "      <td>0.669195</td>\n",
       "      <td>0.106325</td>\n",
       "      <td>0.669195</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.659604</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.659604</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  score_val  pred_time_val  fit_time  \\\n",
       "0              CatBoost   0.977273       0.004613  2.133220   \n",
       "1        KNeighborsDist   0.977273       0.104870  0.004588   \n",
       "2        KNeighborsUnif   0.977273       0.130010  0.005242   \n",
       "3   WeightedEnsemble_L2   0.977273       0.130585  0.363523   \n",
       "4        NeuralNetMXNet   0.954545       0.120138  6.996468   \n",
       "5              LightGBM   0.931818       0.003791  1.731732   \n",
       "6            LightGBMXT   0.931818       0.004629  0.863779   \n",
       "7               XGBoost   0.931818       0.005733  0.523260   \n",
       "8       NeuralNetFastAI   0.931818       0.070858  6.801754   \n",
       "9        ExtraTreesGini   0.931818       0.104239  0.573350   \n",
       "10       ExtraTreesEntr   0.931818       0.106039  0.682417   \n",
       "11     RandomForestGini   0.931818       0.106150  0.853484   \n",
       "12     RandomForestEntr   0.931818       0.106325  0.669195   \n",
       "13        LightGBMLarge   0.909091       0.003670  0.659604   \n",
       "\n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                 0.004613           2.133220            1       True   \n",
       "1                 0.104870           0.004588            1       True   \n",
       "2                 0.130010           0.005242            1       True   \n",
       "3                 0.000576           0.358281            2       True   \n",
       "4                 0.120138           6.996468            1       True   \n",
       "5                 0.003791           1.731732            1       True   \n",
       "6                 0.004629           0.863779            1       True   \n",
       "7                 0.005733           0.523260            1       True   \n",
       "8                 0.070858           6.801754            1       True   \n",
       "9                 0.104239           0.573350            1       True   \n",
       "10                0.106039           0.682417            1       True   \n",
       "11                0.106150           0.853484            1       True   \n",
       "12                0.106325           0.669195            1       True   \n",
       "13                0.003670           0.659604            1       True   \n",
       "\n",
       "    fit_order  \n",
       "0           7  \n",
       "1           2  \n",
       "2           1  \n",
       "3          14  \n",
       "4          12  \n",
       "5           4  \n",
       "6           3  \n",
       "7          11  \n",
       "8          10  \n",
       "9           8  \n",
       "10          9  \n",
       "11          5  \n",
       "12          6  \n",
       "13         13  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hxt2nKGUClD1"
   },
   "source": [
    "Si se quiere usar el modelo que ganó.\n",
    "Si no pasar un String con el nombre del modelo que se quiera a la función .load_model('modelo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "abPnEtKSn_d9"
   },
   "outputs": [],
   "source": [
    "model = predictor._trainer.load_model(predictor.get_model_best())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'KNeighborsUnif',\n",
       " 'model_type': 'KNNModel',\n",
       " 'problem_type': 'binary',\n",
       " 'eval_metric': 'accuracy',\n",
       " 'stopping_metric': 'accuracy',\n",
       " 'fit_time': 0.0052416324615478516,\n",
       " 'num_classes': 2,\n",
       " 'quantile_levels': None,\n",
       " 'predict_time': 0.13000988960266113,\n",
       " 'val_score': 0.9772727272727273,\n",
       " 'hyperparameters': {'weights': 'uniform', 'n_jobs': -1},\n",
       " 'hyperparameters_fit': {},\n",
       " 'hyperparameters_nondefault': ['weights'],\n",
       " 'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "  'max_time_limit_ratio': 1.0,\n",
       "  'max_time_limit': None,\n",
       "  'min_time_limit': 0,\n",
       "  'ignored_type_group_special': ['text_ngram',\n",
       "   'text_special',\n",
       "   'datetime_as_int'],\n",
       "  'ignored_type_group_raw': ['category', 'object'],\n",
       "  'get_features_kwargs': None,\n",
       "  'get_features_kwargs_extra': None},\n",
       " 'num_features': 180,\n",
       " 'features': ['0',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '4',\n",
       "  '5',\n",
       "  '6',\n",
       "  '7',\n",
       "  '8',\n",
       "  '9',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  '14',\n",
       "  '15',\n",
       "  '16',\n",
       "  '17',\n",
       "  '18',\n",
       "  '19',\n",
       "  '20',\n",
       "  '21',\n",
       "  '22',\n",
       "  '23',\n",
       "  '24',\n",
       "  '25',\n",
       "  '26',\n",
       "  '27',\n",
       "  '28',\n",
       "  '29',\n",
       "  '30',\n",
       "  '31',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '35',\n",
       "  '36',\n",
       "  '37',\n",
       "  '38',\n",
       "  '39',\n",
       "  '40',\n",
       "  '41',\n",
       "  '42',\n",
       "  '43',\n",
       "  '44',\n",
       "  '45',\n",
       "  '46',\n",
       "  '47',\n",
       "  '48',\n",
       "  '49',\n",
       "  '50',\n",
       "  '51',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '59',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '74',\n",
       "  '75',\n",
       "  '76',\n",
       "  '77',\n",
       "  '78',\n",
       "  '79',\n",
       "  '80',\n",
       "  '81',\n",
       "  '82',\n",
       "  '83',\n",
       "  '84',\n",
       "  '85',\n",
       "  '86',\n",
       "  '87',\n",
       "  '88',\n",
       "  '89',\n",
       "  '90',\n",
       "  '91',\n",
       "  '92',\n",
       "  '93',\n",
       "  '94',\n",
       "  '95',\n",
       "  '96',\n",
       "  '97',\n",
       "  '98',\n",
       "  '99',\n",
       "  '100',\n",
       "  '101',\n",
       "  '102',\n",
       "  '103',\n",
       "  '104',\n",
       "  '105',\n",
       "  '106',\n",
       "  '107',\n",
       "  '108',\n",
       "  '109',\n",
       "  '110',\n",
       "  '111',\n",
       "  '112',\n",
       "  '113',\n",
       "  '114',\n",
       "  '115',\n",
       "  '116',\n",
       "  '117',\n",
       "  '118',\n",
       "  '119',\n",
       "  '120',\n",
       "  '121',\n",
       "  '122',\n",
       "  '123',\n",
       "  '124',\n",
       "  '125',\n",
       "  '126',\n",
       "  '127',\n",
       "  '128',\n",
       "  '129',\n",
       "  '130',\n",
       "  '131',\n",
       "  '132',\n",
       "  '133',\n",
       "  '134',\n",
       "  '135',\n",
       "  '136',\n",
       "  '137',\n",
       "  '138',\n",
       "  '139',\n",
       "  '140',\n",
       "  '141',\n",
       "  '142',\n",
       "  '143',\n",
       "  '144',\n",
       "  '145',\n",
       "  '146',\n",
       "  '147',\n",
       "  '148',\n",
       "  '149',\n",
       "  '150',\n",
       "  '151',\n",
       "  '152',\n",
       "  '153',\n",
       "  '154',\n",
       "  '155',\n",
       "  '156',\n",
       "  '157',\n",
       "  '158',\n",
       "  '159',\n",
       "  '160',\n",
       "  '161',\n",
       "  '162',\n",
       "  '163',\n",
       "  '164',\n",
       "  '165',\n",
       "  '166',\n",
       "  '167',\n",
       "  '168',\n",
       "  '169',\n",
       "  '170',\n",
       "  '171',\n",
       "  '172',\n",
       "  '173',\n",
       "  '174',\n",
       "  '175',\n",
       "  '176',\n",
       "  '177',\n",
       "  '178',\n",
       "  '179'],\n",
       " 'feature_metadata': <autogluon.core.features.feature_metadata.FeatureMetadata at 0x7efe902c6be0>,\n",
       " 'memory_size': 131845}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0Q08f4cunjdj"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_target=test_ds[label], \n",
    "                      y_predicted=predictor.predict(test_ds), \n",
    "                      binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ukdFAS0ooTuO",
    "outputId": "55f19d8f-580d-4712-b2dd-aa6eb06e8265"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plot_confusion_matrix(cm, cmap = cmap.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "NFaY23UVocwH",
    "outputId": "acf16882-a11c-4f00-a0dc-680ae561b6c4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_target=test_ds[label], \n",
    "                      y_predicted=predictor.predict(test_ds),\n",
    "                      binary=True, \n",
    "                      positive_label=1)\n",
    "fig, ax = plot_confusion_matrix(cm, cmap = cmap.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p97YYo5Yfaix"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVIqbF0rb0BD"
   },
   "source": [
    "# 2. H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H2O’s AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles – one based on all previously trained models, another one on the best model of each family – will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard.\n",
    "\n",
    "H2O offers a number of model explainability methods that apply to AutoML objects (groups of models), as well as individual models (e.g. leader model). Explanations can be generated automatically with a single function call, providing a simple interface to exploring and explaining the AutoML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Specific Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6xkHwrnvbJYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://h2o:54321 . connected.\n",
      "Warning: Your H2O cluster version is too old (2 years, 1 month and 10 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>1 min 49 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.26.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>2 years, 1 month and 10 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>root</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>7.105 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>2</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>2</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://h2o:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.9 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         1 min 49 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.26.0.1\n",
       "H2O cluster version age:    2 years, 1 month and 10 days !!!\n",
       "H2O cluster name:           root\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    7.105 Gb\n",
       "H2O cluster total cores:    2\n",
       "H2O cluster allowed cores:  2\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://h2o:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.9 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init(\"http://h2o:54321\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNB5EY7PdO1T"
   },
   "source": [
    "La siguiente linea importa desde ruta web/local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4rOrH_2rdKvp"
   },
   "outputs": [],
   "source": [
    "#h2o.import_file('/data/cell_TP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWdrKI0VdTMz"
   },
   "source": [
    "Para importar desde pandas, y así poder juntar la X y la label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7DEwLfYbJPo",
    "outputId": "feedcc9c-3ea2-4ae3-95d7-aca23a830046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "h2oframe = h2o.H2OFrame(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaepGtUee2AY"
   },
   "source": [
    "Si es clasificación (y la columna no es categórica) hay que transformar la columna objetivo a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AmhIKjp6e_Rd"
   },
   "outputs": [],
   "source": [
    "h2oframe[label] = h2oframe[label].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uRXqXKYfAze"
   },
   "source": [
    "Hay que pasarle el nombre de las variables X y el nombre de la columna objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MFZN63Xkdipl"
   },
   "outputs": [],
   "source": [
    "columns = h2oframe.columns\n",
    "columns.remove(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define and train predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwARnK59faEf",
    "outputId": "d6b15f1d-3e71-4985-8338-ea982e61f2ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "predictor = H2OAutoML(max_models=5)\n",
    "predictor.train(x=columns, y=label, training_frame=h2oframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "Nc1Cfo6VfZ4x",
    "outputId": "ab81c63f-ffe8-49f3-f125-214728c4969a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                     </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">      mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GBM_3_AutoML_20210826_124110                 </td><td style=\"text-align: right;\">0.993076</td><td style=\"text-align: right;\"> 0.100432</td><td style=\"text-align: right;\">             0.0286433</td><td style=\"text-align: right;\">0.159351</td><td style=\"text-align: right;\">0.0253926</td></tr>\n",
       "<tr><td>DeepLearning_1_AutoML_20210826_124110        </td><td style=\"text-align: right;\">0.993022</td><td style=\"text-align: right;\"> 0.121203</td><td style=\"text-align: right;\">             0.0403549</td><td style=\"text-align: right;\">0.189359</td><td style=\"text-align: right;\">0.035857 </td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20210826_124110_model_2    </td><td style=\"text-align: right;\">0.99267 </td><td style=\"text-align: right;\"> 0.322895</td><td style=\"text-align: right;\">             0.0403549</td><td style=\"text-align: right;\">0.199775</td><td style=\"text-align: right;\">0.0399102</td></tr>\n",
       "<tr><td>GBM_2_AutoML_20210826_124110                 </td><td style=\"text-align: right;\">0.991074</td><td style=\"text-align: right;\"> 0.111808</td><td style=\"text-align: right;\">             0.0286433</td><td style=\"text-align: right;\">0.168564</td><td style=\"text-align: right;\">0.028414 </td></tr>\n",
       "<tr><td>GBM_4_AutoML_20210826_124110                 </td><td style=\"text-align: right;\">0.990912</td><td style=\"text-align: right;\"> 0.112061</td><td style=\"text-align: right;\">             0.0286433</td><td style=\"text-align: right;\">0.165708</td><td style=\"text-align: right;\">0.0274592</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20210826_124110_model_1    </td><td style=\"text-align: right;\">0.990858</td><td style=\"text-align: right;\"> 0.108034</td><td style=\"text-align: right;\">             0.0234231</td><td style=\"text-align: right;\">0.159627</td><td style=\"text-align: right;\">0.0254809</td></tr>\n",
       "<tr><td>XGBoost_grid_1_AutoML_20210826_124110_model_2</td><td style=\"text-align: right;\">0.989262</td><td style=\"text-align: right;\"> 0.176781</td><td style=\"text-align: right;\">             0.0297522</td><td style=\"text-align: right;\">0.194815</td><td style=\"text-align: right;\">0.0379527</td></tr>\n",
       "<tr><td>DRF_1_AutoML_20210826_124531                 </td><td style=\"text-align: right;\">0.98745 </td><td style=\"text-align: right;\"> 0.322436</td><td style=\"text-align: right;\">             0.0254787</td><td style=\"text-align: right;\">0.15882 </td><td style=\"text-align: right;\">0.0252237</td></tr>\n",
       "<tr><td>GBM_1_AutoML_20210826_124110                 </td><td style=\"text-align: right;\">0.987044</td><td style=\"text-align: right;\"> 0.119763</td><td style=\"text-align: right;\">             0.0254787</td><td style=\"text-align: right;\">0.162692</td><td style=\"text-align: right;\">0.0264688</td></tr>\n",
       "<tr><td>XGBoost_3_AutoML_20210826_124110             </td><td style=\"text-align: right;\">0.986152</td><td style=\"text-align: right;\"> 0.132035</td><td style=\"text-align: right;\">             0.0276966</td><td style=\"text-align: right;\">0.182702</td><td style=\"text-align: right;\">0.03338  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method H2OFrame.table of >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = predictor.leaderboard\n",
    "lb.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ITGJErz5xs1X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">         p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999748</td><td style=\"text-align: right;\">0.000252139</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999748</td><td style=\"text-align: right;\">0.000251968</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999702</td><td style=\"text-align: right;\">0.000297681</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999748</td><td style=\"text-align: right;\">0.00025217 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999748</td><td style=\"text-align: right;\">0.000252259</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999697</td><td style=\"text-align: right;\">0.000303106</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999748</td><td style=\"text-align: right;\">0.000252441</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999748</td><td style=\"text-align: right;\">0.000252449</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999748</td><td style=\"text-align: right;\">0.00025231 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.999748</td><td style=\"text-align: right;\">0.000252407</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(h2oframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31gDGBxUCvvt"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ochk5vsOa8G3"
   },
   "source": [
    "# 3. Mcfly AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of mcfly is to ease using deep learning technology for time series classification. The advantages of deep learning algorithms is that it can handle raw data directly with no need to compute signal features, it does not require a expert domain knowledge about the data, and it has been shown to be competitive with conventional machine learning techniques. As an example, you can apply mcfly on, accelerometer data for activity classification, as shown in the mcfly tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_qP4CbPasze"
   },
   "source": [
    "MCfly no es tan \"automático\", es decir, solo entrena redes neuronales y no se muy bien si hace mucho HPO o son arquitecturas fijas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mcfly.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Specific Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mcfly import find_architecture.find_best_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KbDm9cR8CAw1"
   },
   "outputs": [],
   "source": [
    "import mcfly\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Preprocess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XfSsS_weEBJN"
   },
   "outputs": [],
   "source": [
    "ds_tmp = pd.read_csv(files[index_file], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvfbUHo-GygB"
   },
   "source": [
    "Aquí poner el shape pertinente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "htg_XqfBET9-"
   },
   "outputs": [],
   "source": [
    "size = (275, 180, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXLZmkLAGrOr"
   },
   "source": [
    "Transformamos los datos tal que \n",
    "(num_samples, num_timesteps, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-YWT3XV8Drhm"
   },
   "outputs": [],
   "source": [
    "X = np.zeros(size)\n",
    "for i in range(len(files)):\n",
    "    ds_tmp = pd.read_csv(files[i], index_col=0)\n",
    "    X[:,:,i] = ds_tmp.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI3RFP6oF1Yy"
   },
   "source": [
    "Mcfly utiliza onehotencoding en la columna objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "4MBA8oDoF9pp"
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "Y = pd.read_csv('/data/labels.csv')[label].values\n",
    "Y = enc.fit_transform(Y.reshape((-1,1))).toarray()\n",
    "Y = enc.fit_transform(pd.read_csv('/data/labels.csv')[label].values.reshape((1,-1))).toarray().reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos train y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "WcbWwDVHG5lR"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train)\n",
    "X_val = np.nan_to_num(X_val)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "y_val = np.nan_to_num(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdNqMg_uJOJw"
   },
   "source": [
    "Normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "2x4tRCw4JMbx"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKwhNBOdEiY6"
   },
   "source": [
    "input_size = (num_samples, num_timesteps, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIe_p41fHul6",
    "outputId": "8dc67475-d52f-4fe3-cd9f-5b4e0256c752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (220, 180, 4)\n",
      "X_val.shape (55, 180, 4)\n",
      "y_train.shape (220, 1)\n",
      "y_val.shape (55, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape', X_train.shape)\n",
    "print('X_val.shape', X_val.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_val.shape', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "WKucuHnfH9xe"
   },
   "outputs": [],
   "source": [
    "class_weight = {0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define and train predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzpbPyl-C0xe",
    "outputId": "289023b3-26b3-49eb-83f2-cb9da2d5bd98",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models will be trained on subset of the data (subset size: 100).\n",
      "Training model 0 InceptionTime\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - ETA: 11s - loss: 0.0000e+00 - accuracy: 1.000 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.000 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 5s 435ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 272ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 270ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 274ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 293ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 265ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 00006: early stopping\n",
      "Training model 1 ResNet\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - ETA: 12s - loss: 0.0000e+00 - accuracy: 1.000 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.000 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 5s 402ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 289ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 288ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 286ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 286ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 1s 289ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 00006: early stopping\n",
      "Training model 2 DeepConvLSTM\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - ETA: 22s - loss: 0.9563 - accuracy: 1.000 - ETA: 1s - loss: 0.9538 - accuracy: 1.000 - ETA: 0s - loss: 0.9512 - accuracy: 1.00 - ETA: 0s - loss: 0.9487 - accuracy: 1.00 - ETA: 0s - loss: 0.9462 - accuracy: 1.00 - 9s 753ms/step - loss: 0.9445 - accuracy: 1.0000 - val_loss: 0.9060 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.9060 - accuracy: 1.00 - ETA: 1s - loss: 0.9035 - accuracy: 1.00 - ETA: 0s - loss: 0.9011 - accuracy: 1.00 - ETA: 0s - loss: 0.8986 - accuracy: 1.00 - ETA: 0s - loss: 0.8962 - accuracy: 1.00 - 2s 475ms/step - loss: 0.8946 - accuracy: 1.0000 - val_loss: 0.8578 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.8578 - accuracy: 1.00 - ETA: 1s - loss: 0.8555 - accuracy: 1.00 - ETA: 0s - loss: 0.8531 - accuracy: 1.00 - ETA: 0s - loss: 0.8508 - accuracy: 1.00 - ETA: 0s - loss: 0.8485 - accuracy: 1.00 - 2s 429ms/step - loss: 0.8470 - accuracy: 1.0000 - val_loss: 0.8120 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.8120 - accuracy: 1.00 - ETA: 1s - loss: 0.8097 - accuracy: 1.00 - ETA: 0s - loss: 0.8075 - accuracy: 1.00 - ETA: 0s - loss: 0.8053 - accuracy: 1.00 - ETA: 0s - loss: 0.8031 - accuracy: 1.00 - 2s 421ms/step - loss: 0.8017 - accuracy: 1.0000 - val_loss: 0.7685 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.7685 - accuracy: 1.00 - ETA: 1s - loss: 0.7664 - accuracy: 1.00 - ETA: 0s - loss: 0.7643 - accuracy: 1.00 - ETA: 0s - loss: 0.7622 - accuracy: 1.00 - ETA: 0s - loss: 0.7601 - accuracy: 1.00 - 2s 425ms/step - loss: 0.7587 - accuracy: 1.0000 - val_loss: 0.7273 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.7273 - accuracy: 1.00 - ETA: 1s - loss: 0.7253 - accuracy: 1.00 - ETA: 0s - loss: 0.7234 - accuracy: 1.00 - ETA: 0s - loss: 0.7214 - accuracy: 1.00 - ETA: 0s - loss: 0.7194 - accuracy: 1.00 - 2s 443ms/step - loss: 0.7181 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.6885 - accuracy: 1.00 - ETA: 1s - loss: 0.6866 - accuracy: 1.00 - ETA: 0s - loss: 0.6847 - accuracy: 1.00 - ETA: 0s - loss: 0.6829 - accuracy: 1.00 - ETA: 0s - loss: 0.6810 - accuracy: 1.00 - 2s 432ms/step - loss: 0.6798 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.6519 - accuracy: 1.00 - ETA: 1s - loss: 0.6501 - accuracy: 1.00 - ETA: 0s - loss: 0.6483 - accuracy: 1.00 - ETA: 0s - loss: 0.6466 - accuracy: 1.00 - ETA: 0s - loss: 0.6449 - accuracy: 1.00 - 2s 430ms/step - loss: 0.6437 - accuracy: 1.0000 - val_loss: 0.6174 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.6174 - accuracy: 1.00 - ETA: 1s - loss: 0.6157 - accuracy: 1.00 - ETA: 0s - loss: 0.6141 - accuracy: 1.00 - ETA: 0s - loss: 0.6124 - accuracy: 1.00 - ETA: 0s - loss: 0.6108 - accuracy: 1.00 - 2s 442ms/step - loss: 0.6097 - accuracy: 1.0000 - val_loss: 0.5849 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 1s - loss: 0.5849 - accuracy: 1.00 - ETA: 1s - loss: 0.5834 - accuracy: 1.00 - ETA: 0s - loss: 0.5818 - accuracy: 1.00 - ETA: 0s - loss: 0.5803 - accuracy: 1.00 - ETA: 0s - loss: 0.5787 - accuracy: 1.00 - 2s 435ms/step - loss: 0.5777 - accuracy: 1.0000 - val_loss: 0.5544 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.5544 - accuracy: 1.00 - ETA: 1s - loss: 0.5530 - accuracy: 1.00 - ETA: 0s - loss: 0.5515 - accuracy: 1.00 - ETA: 0s - loss: 0.5500 - accuracy: 1.00 - ETA: 0s - loss: 0.5486 - accuracy: 1.00 - 2s 442ms/step - loss: 0.5476 - accuracy: 1.0000 - val_loss: 0.5258 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.5258 - accuracy: 1.00 - ETA: 1s - loss: 0.5244 - accuracy: 1.00 - ETA: 0s - loss: 0.5230 - accuracy: 1.00 - ETA: 0s - loss: 0.5216 - accuracy: 1.00 - ETA: 0s - loss: 0.5203 - accuracy: 1.00 - 2s 413ms/step - loss: 0.5194 - accuracy: 1.0000 - val_loss: 0.4988 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.4988 - accuracy: 1.00 - ETA: 1s - loss: 0.4975 - accuracy: 1.00 - ETA: 0s - loss: 0.4962 - accuracy: 1.00 - ETA: 0s - loss: 0.4949 - accuracy: 1.00 - ETA: 0s - loss: 0.4937 - accuracy: 1.00 - 2s 435ms/step - loss: 0.4928 - accuracy: 1.0000 - val_loss: 0.4735 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.4735 - accuracy: 1.00 - ETA: 1s - loss: 0.4723 - accuracy: 1.00 - ETA: 0s - loss: 0.4711 - accuracy: 1.00 - ETA: 0s - loss: 0.4699 - accuracy: 1.00 - ETA: 0s - loss: 0.4687 - accuracy: 1.00 - 2s 517ms/step - loss: 0.4679 - accuracy: 1.0000 - val_loss: 0.4497 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.4497 - accuracy: 1.00 - ETA: 1s - loss: 0.4486 - accuracy: 1.00 - ETA: 0s - loss: 0.4474 - accuracy: 1.00 - ETA: 0s - loss: 0.4463 - accuracy: 1.00 - ETA: 0s - loss: 0.4452 - accuracy: 1.00 - 2s 449ms/step - loss: 0.4444 - accuracy: 1.0000 - val_loss: 0.4274 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.4274 - accuracy: 1.00 - ETA: 1s - loss: 0.4263 - accuracy: 1.00 - ETA: 0s - loss: 0.4252 - accuracy: 1.00 - ETA: 0s - loss: 0.4242 - accuracy: 1.00 - ETA: 0s - loss: 0.4231 - accuracy: 1.00 - 2s 438ms/step - loss: 0.4224 - accuracy: 1.0000 - val_loss: 0.4064 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.4064 - accuracy: 1.00 - ETA: 1s - loss: 0.4054 - accuracy: 1.00 - ETA: 0s - loss: 0.4044 - accuracy: 1.00 - ETA: 0s - loss: 0.4034 - accuracy: 1.00 - ETA: 0s - loss: 0.4024 - accuracy: 1.00 - 2s 414ms/step - loss: 0.4017 - accuracy: 1.0000 - val_loss: 0.3867 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.3867 - accuracy: 1.00 - ETA: 1s - loss: 0.3857 - accuracy: 1.00 - ETA: 0s - loss: 0.3848 - accuracy: 1.00 - ETA: 0s - loss: 0.3838 - accuracy: 1.00 - ETA: 0s - loss: 0.3829 - accuracy: 1.00 - 2s 405ms/step - loss: 0.3823 - accuracy: 1.0000 - val_loss: 0.3682 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.3682 - accuracy: 1.00 - ETA: 1s - loss: 0.3673 - accuracy: 1.00 - ETA: 0s - loss: 0.3664 - accuracy: 1.00 - ETA: 0s - loss: 0.3655 - accuracy: 1.00 - ETA: 0s - loss: 0.3646 - accuracy: 1.00 - 2s 420ms/step - loss: 0.3640 - accuracy: 1.0000 - val_loss: 0.3508 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.3508 - accuracy: 1.00 - ETA: 1s - loss: 0.3499 - accuracy: 1.00 - ETA: 0s - loss: 0.3491 - accuracy: 1.00 - ETA: 0s - loss: 0.3483 - accuracy: 1.00 - ETA: 0s - loss: 0.3475 - accuracy: 1.00 - 2s 422ms/step - loss: 0.3469 - accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.3345 - accuracy: 1.00 - ETA: 1s - loss: 0.3337 - accuracy: 1.00 - ETA: 0s - loss: 0.3329 - accuracy: 1.00 - ETA: 0s - loss: 0.3321 - accuracy: 1.00 - ETA: 0s - loss: 0.3313 - accuracy: 1.00 - 2s 420ms/step - loss: 0.3308 - accuracy: 1.0000 - val_loss: 0.3191 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.3191 - accuracy: 1.00 - ETA: 1s - loss: 0.3184 - accuracy: 1.00 - ETA: 0s - loss: 0.3177 - accuracy: 1.00 - ETA: 0s - loss: 0.3169 - accuracy: 1.00 - ETA: 0s - loss: 0.3162 - accuracy: 1.00 - 2s 428ms/step - loss: 0.3157 - accuracy: 1.0000 - val_loss: 0.3047 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.3047 - accuracy: 1.00 - ETA: 1s - loss: 0.3040 - accuracy: 1.00 - ETA: 0s - loss: 0.3034 - accuracy: 1.00 - ETA: 0s - loss: 0.3027 - accuracy: 1.00 - ETA: 0s - loss: 0.3020 - accuracy: 1.00 - 2s 418ms/step - loss: 0.3015 - accuracy: 1.0000 - val_loss: 0.2912 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2912 - accuracy: 1.00 - ETA: 1s - loss: 0.2906 - accuracy: 1.00 - ETA: 0s - loss: 0.2899 - accuracy: 1.00 - ETA: 0s - loss: 0.2893 - accuracy: 1.00 - ETA: 0s - loss: 0.2886 - accuracy: 1.00 - 2s 421ms/step - loss: 0.2882 - accuracy: 1.0000 - val_loss: 0.2785 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2785 - accuracy: 1.00 - ETA: 1s - loss: 0.2779 - accuracy: 1.00 - ETA: 0s - loss: 0.2773 - accuracy: 1.00 - ETA: 0s - loss: 0.2767 - accuracy: 1.00 - ETA: 0s - loss: 0.2761 - accuracy: 1.00 - 2s 427ms/step - loss: 0.2757 - accuracy: 1.0000 - val_loss: 0.2666 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2666 - accuracy: 1.00 - ETA: 0s - loss: 0.2661 - accuracy: 1.00 - ETA: 0s - loss: 0.2655 - accuracy: 1.00 - ETA: 0s - loss: 0.2649 - accuracy: 1.00 - ETA: 0s - loss: 0.2643 - accuracy: 1.00 - 2s 423ms/step - loss: 0.2640 - accuracy: 1.0000 - val_loss: 0.2554 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2554 - accuracy: 1.00 - ETA: 1s - loss: 0.2549 - accuracy: 1.00 - ETA: 0s - loss: 0.2544 - accuracy: 1.00 - ETA: 0s - loss: 0.2538 - accuracy: 1.00 - ETA: 0s - loss: 0.2533 - accuracy: 1.00 - 2s 420ms/step - loss: 0.2529 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2449 - accuracy: 1.00 - ETA: 1s - loss: 0.2444 - accuracy: 1.00 - ETA: 0s - loss: 0.2439 - accuracy: 1.00 - ETA: 0s - loss: 0.2434 - accuracy: 1.00 - ETA: 0s - loss: 0.2429 - accuracy: 1.00 - 2s 421ms/step - loss: 0.2426 - accuracy: 1.0000 - val_loss: 0.2351 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2351 - accuracy: 1.00 - ETA: 1s - loss: 0.2346 - accuracy: 1.00 - ETA: 0s - loss: 0.2341 - accuracy: 1.00 - ETA: 0s - loss: 0.2337 - accuracy: 1.00 - ETA: 0s - loss: 0.2332 - accuracy: 1.00 - 2s 421ms/step - loss: 0.2329 - accuracy: 1.0000 - val_loss: 0.2258 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2258 - accuracy: 1.00 - ETA: 1s - loss: 0.2254 - accuracy: 1.00 - ETA: 0s - loss: 0.2249 - accuracy: 1.00 - ETA: 0s - loss: 0.2245 - accuracy: 1.00 - ETA: 0s - loss: 0.2240 - accuracy: 1.00 - 2s 446ms/step - loss: 0.2237 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2171 - accuracy: 1.00 - ETA: 1s - loss: 0.2167 - accuracy: 1.00 - ETA: 0s - loss: 0.2163 - accuracy: 1.00 - ETA: 0s - loss: 0.2159 - accuracy: 1.00 - ETA: 0s - loss: 0.2154 - accuracy: 1.00 - 2s 453ms/step - loss: 0.2152 - accuracy: 1.0000 - val_loss: 0.2089 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.2089 - accuracy: 1.00 - ETA: 1s - loss: 0.2085 - accuracy: 1.00 - ETA: 0s - loss: 0.2081 - accuracy: 1.00 - ETA: 0s - loss: 0.2078 - accuracy: 1.00 - ETA: 0s - loss: 0.2074 - accuracy: 1.00 - 2s 499ms/step - loss: 0.2071 - accuracy: 1.0000 - val_loss: 0.2013 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 1s - loss: 0.2013 - accuracy: 1.00 - ETA: 1s - loss: 0.2009 - accuracy: 1.00 - ETA: 0s - loss: 0.2005 - accuracy: 1.00 - ETA: 0s - loss: 0.2002 - accuracy: 1.00 - ETA: 0s - loss: 0.1998 - accuracy: 1.00 - 2s 437ms/step - loss: 0.1995 - accuracy: 1.0000 - val_loss: 0.1940 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1940 - accuracy: 1.00 - ETA: 1s - loss: 0.1937 - accuracy: 1.00 - ETA: 0s - loss: 0.1934 - accuracy: 1.00 - ETA: 0s - loss: 0.1930 - accuracy: 1.00 - ETA: 0s - loss: 0.1927 - accuracy: 1.00 - 2s 419ms/step - loss: 0.1924 - accuracy: 1.0000 - val_loss: 0.1873 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1873 - accuracy: 1.00 - ETA: 1s - loss: 0.1869 - accuracy: 1.00 - ETA: 0s - loss: 0.1866 - accuracy: 1.00 - ETA: 0s - loss: 0.1863 - accuracy: 1.00 - ETA: 0s - loss: 0.1860 - accuracy: 1.00 - 2s 440ms/step - loss: 0.1858 - accuracy: 1.0000 - val_loss: 0.1809 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1809 - accuracy: 1.00 - ETA: 1s - loss: 0.1806 - accuracy: 1.00 - ETA: 0s - loss: 0.1803 - accuracy: 1.00 - ETA: 0s - loss: 0.1800 - accuracy: 1.00 - ETA: 0s - loss: 0.1797 - accuracy: 1.00 - 2s 428ms/step - loss: 0.1795 - accuracy: 1.0000 - val_loss: 0.1749 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1749 - accuracy: 1.00 - ETA: 1s - loss: 0.1746 - accuracy: 1.00 - ETA: 0s - loss: 0.1743 - accuracy: 1.00 - ETA: 0s - loss: 0.1741 - accuracy: 1.00 - ETA: 0s - loss: 0.1738 - accuracy: 1.00 - 2s 407ms/step - loss: 0.1736 - accuracy: 1.0000 - val_loss: 0.1693 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1693 - accuracy: 1.00 - ETA: 1s - loss: 0.1690 - accuracy: 1.00 - ETA: 0s - loss: 0.1687 - accuracy: 1.00 - ETA: 0s - loss: 0.1685 - accuracy: 1.00 - ETA: 0s - loss: 0.1682 - accuracy: 1.00 - 2s 432ms/step - loss: 0.1680 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1640 - accuracy: 1.00 - ETA: 1s - loss: 0.1637 - accuracy: 1.00 - ETA: 0s - loss: 0.1635 - accuracy: 1.00 - ETA: 0s - loss: 0.1632 - accuracy: 1.00 - ETA: 0s - loss: 0.1630 - accuracy: 1.00 - 2s 417ms/step - loss: 0.1628 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1590 - accuracy: 1.00 - ETA: 1s - loss: 0.1588 - accuracy: 1.00 - ETA: 0s - loss: 0.1585 - accuracy: 1.00 - ETA: 0s - loss: 0.1583 - accuracy: 1.00 - ETA: 0s - loss: 0.1580 - accuracy: 1.00 - 2s 447ms/step - loss: 0.1579 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1543 - accuracy: 1.00 - ETA: 1s - loss: 0.1541 - accuracy: 1.00 - ETA: 0s - loss: 0.1539 - accuracy: 1.00 - ETA: 0s - loss: 0.1536 - accuracy: 1.00 - ETA: 0s - loss: 0.1534 - accuracy: 1.00 - 2s 433ms/step - loss: 0.1533 - accuracy: 1.0000 - val_loss: 0.1499 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1499 - accuracy: 1.00 - ETA: 1s - loss: 0.1497 - accuracy: 1.00 - ETA: 0s - loss: 0.1495 - accuracy: 1.00 - ETA: 0s - loss: 0.1492 - accuracy: 1.00 - ETA: 0s - loss: 0.1490 - accuracy: 1.00 - 2s 423ms/step - loss: 0.1489 - accuracy: 1.0000 - val_loss: 0.1457 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1457 - accuracy: 1.00 - ETA: 1s - loss: 0.1455 - accuracy: 1.00 - ETA: 0s - loss: 0.1453 - accuracy: 1.00 - ETA: 0s - loss: 0.1451 - accuracy: 1.00 - ETA: 0s - loss: 0.1449 - accuracy: 1.00 - 2s 428ms/step - loss: 0.1448 - accuracy: 1.0000 - val_loss: 0.1418 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1418 - accuracy: 1.00 - ETA: 1s - loss: 0.1416 - accuracy: 1.00 - ETA: 0s - loss: 0.1414 - accuracy: 1.00 - ETA: 0s - loss: 0.1412 - accuracy: 1.00 - ETA: 0s - loss: 0.1410 - accuracy: 1.00 - 2s 437ms/step - loss: 0.1409 - accuracy: 1.0000 - val_loss: 0.1381 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1381 - accuracy: 1.00 - ETA: 1s - loss: 0.1379 - accuracy: 1.00 - ETA: 0s - loss: 0.1377 - accuracy: 1.00 - ETA: 0s - loss: 0.1376 - accuracy: 1.00 - ETA: 0s - loss: 0.1374 - accuracy: 1.00 - 2s 426ms/step - loss: 0.1373 - accuracy: 1.0000 - val_loss: 0.1346 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1346 - accuracy: 1.00 - ETA: 1s - loss: 0.1344 - accuracy: 1.00 - ETA: 0s - loss: 0.1343 - accuracy: 1.00 - ETA: 0s - loss: 0.1341 - accuracy: 1.00 - ETA: 0s - loss: 0.1339 - accuracy: 1.00 - 2s 430ms/step - loss: 0.1338 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1313 - accuracy: 1.00 - ETA: 1s - loss: 0.1311 - accuracy: 1.00 - ETA: 0s - loss: 0.1310 - accuracy: 1.00 - ETA: 0s - loss: 0.1308 - accuracy: 1.00 - ETA: 0s - loss: 0.1307 - accuracy: 1.00 - 2s 425ms/step - loss: 0.1305 - accuracy: 1.0000 - val_loss: 0.1282 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1282 - accuracy: 1.00 - ETA: 1s - loss: 0.1280 - accuracy: 1.00 - ETA: 0s - loss: 0.1279 - accuracy: 1.00 - ETA: 0s - loss: 0.1277 - accuracy: 1.00 - ETA: 0s - loss: 0.1276 - accuracy: 1.00 - 2s 439ms/step - loss: 0.1275 - accuracy: 1.0000 - val_loss: 0.1252 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1252 - accuracy: 1.00 - ETA: 1s - loss: 0.1251 - accuracy: 1.00 - ETA: 0s - loss: 0.1249 - accuracy: 1.00 - ETA: 0s - loss: 0.1248 - accuracy: 1.00 - ETA: 0s - loss: 0.1246 - accuracy: 1.00 - 2s 444ms/step - loss: 0.1246 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1224 - accuracy: 1.00 - ETA: 1s - loss: 0.1223 - accuracy: 1.00 - ETA: 0s - loss: 0.1221 - accuracy: 1.00 - ETA: 0s - loss: 0.1220 - accuracy: 1.00 - ETA: 0s - loss: 0.1219 - accuracy: 1.00 - 2s 440ms/step - loss: 0.1218 - accuracy: 1.0000 - val_loss: 0.1198 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1198 - accuracy: 1.00 - ETA: 1s - loss: 0.1196 - accuracy: 1.00 - ETA: 0s - loss: 0.1195 - accuracy: 1.00 - ETA: 0s - loss: 0.1194 - accuracy: 1.00 - ETA: 0s - loss: 0.1193 - accuracy: 1.00 - 2s 471ms/step - loss: 0.1192 - accuracy: 1.0000 - val_loss: 0.1173 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1173 - accuracy: 1.00 - ETA: 1s - loss: 0.1171 - accuracy: 1.00 - ETA: 0s - loss: 0.1170 - accuracy: 1.00 - ETA: 0s - loss: 0.1169 - accuracy: 1.00 - ETA: 0s - loss: 0.1168 - accuracy: 1.00 - 2s 502ms/step - loss: 0.1167 - accuracy: 1.0000 - val_loss: 0.1149 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1149 - accuracy: 1.00 - ETA: 1s - loss: 0.1148 - accuracy: 1.00 - ETA: 0s - loss: 0.1146 - accuracy: 1.00 - ETA: 0s - loss: 0.1145 - accuracy: 1.00 - ETA: 0s - loss: 0.1144 - accuracy: 1.00 - 2s 536ms/step - loss: 0.1143 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1126 - accuracy: 1.00 - ETA: 1s - loss: 0.1125 - accuracy: 1.00 - ETA: 0s - loss: 0.1124 - accuracy: 1.00 - ETA: 0s - loss: 0.1123 - accuracy: 1.00 - ETA: 0s - loss: 0.1122 - accuracy: 1.00 - 3s 516ms/step - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1105 - accuracy: 1.00 - ETA: 1s - loss: 0.1103 - accuracy: 1.00 - ETA: 0s - loss: 0.1102 - accuracy: 1.00 - ETA: 0s - loss: 0.1101 - accuracy: 1.00 - ETA: 0s - loss: 0.1100 - accuracy: 1.00 - 2s 399ms/step - loss: 0.1100 - accuracy: 1.0000 - val_loss: 0.1084 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 1s - loss: 0.1084 - accuracy: 1.00 - ETA: 0s - loss: 0.1083 - accuracy: 1.00 - ETA: 0s - loss: 0.1082 - accuracy: 1.00 - ETA: 0s - loss: 0.1081 - accuracy: 1.00 - ETA: 0s - loss: 0.1080 - accuracy: 1.00 - 2s 411ms/step - loss: 0.1079 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1064 - accuracy: 1.00 - ETA: 1s - loss: 0.1063 - accuracy: 1.00 - ETA: 0s - loss: 0.1063 - accuracy: 1.00 - ETA: 0s - loss: 0.1062 - accuracy: 1.00 - ETA: 0s - loss: 0.1061 - accuracy: 1.00 - 2s 420ms/step - loss: 0.1060 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1046 - accuracy: 1.00 - ETA: 1s - loss: 0.1045 - accuracy: 1.00 - ETA: 0s - loss: 0.1044 - accuracy: 1.00 - ETA: 0s - loss: 0.1043 - accuracy: 1.00 - ETA: 0s - loss: 0.1042 - accuracy: 1.00 - 2s 457ms/step - loss: 0.1042 - accuracy: 1.0000 - val_loss: 0.1028 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1028 - accuracy: 1.00 - ETA: 1s - loss: 0.1027 - accuracy: 1.00 - ETA: 0s - loss: 0.1026 - accuracy: 1.00 - ETA: 0s - loss: 0.1025 - accuracy: 1.00 - ETA: 0s - loss: 0.1025 - accuracy: 1.00 - 2s 459ms/step - loss: 0.1024 - accuracy: 1.0000 - val_loss: 0.1011 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.1011 - accuracy: 1.00 - ETA: 1s - loss: 0.1010 - accuracy: 1.00 - ETA: 0s - loss: 0.1009 - accuracy: 1.00 - ETA: 0s - loss: 0.1008 - accuracy: 1.00 - ETA: 0s - loss: 0.1008 - accuracy: 1.00 - 2s 415ms/step - loss: 0.1007 - accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0995 - accuracy: 1.00 - ETA: 1s - loss: 0.0994 - accuracy: 1.00 - ETA: 0s - loss: 0.0993 - accuracy: 1.00 - ETA: 0s - loss: 0.0992 - accuracy: 1.00 - ETA: 0s - loss: 0.0991 - accuracy: 1.00 - 2s 413ms/step - loss: 0.0991 - accuracy: 1.0000 - val_loss: 0.0979 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0979 - accuracy: 1.00 - ETA: 1s - loss: 0.0978 - accuracy: 1.00 - ETA: 0s - loss: 0.0977 - accuracy: 1.00 - ETA: 0s - loss: 0.0977 - accuracy: 1.00 - ETA: 0s - loss: 0.0976 - accuracy: 1.00 - 2s 431ms/step - loss: 0.0975 - accuracy: 1.0000 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0964 - accuracy: 1.00 - ETA: 1s - loss: 0.0963 - accuracy: 1.00 - ETA: 0s - loss: 0.0963 - accuracy: 1.00 - ETA: 0s - loss: 0.0962 - accuracy: 1.00 - ETA: 0s - loss: 0.0961 - accuracy: 1.00 - 2s 411ms/step - loss: 0.0961 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0950 - accuracy: 1.00 - ETA: 1s - loss: 0.0949 - accuracy: 1.00 - ETA: 0s - loss: 0.0948 - accuracy: 1.00 - ETA: 0s - loss: 0.0947 - accuracy: 1.00 - ETA: 0s - loss: 0.0947 - accuracy: 1.00 - 2s 434ms/step - loss: 0.0946 - accuracy: 1.0000 - val_loss: 0.0936 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0936 - accuracy: 1.00 - ETA: 1s - loss: 0.0935 - accuracy: 1.00 - ETA: 0s - loss: 0.0934 - accuracy: 1.00 - ETA: 0s - loss: 0.0934 - accuracy: 1.00 - ETA: 0s - loss: 0.0933 - accuracy: 1.00 - 2s 434ms/step - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.0922 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0922 - accuracy: 1.00 - ETA: 1s - loss: 0.0922 - accuracy: 1.00 - ETA: 0s - loss: 0.0921 - accuracy: 1.00 - ETA: 0s - loss: 0.0920 - accuracy: 1.00 - ETA: 0s - loss: 0.0920 - accuracy: 1.00 - 2s 419ms/step - loss: 0.0919 - accuracy: 1.0000 - val_loss: 0.0909 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0909 - accuracy: 1.00 - ETA: 1s - loss: 0.0909 - accuracy: 1.00 - ETA: 0s - loss: 0.0908 - accuracy: 1.00 - ETA: 0s - loss: 0.0908 - accuracy: 1.00 - ETA: 0s - loss: 0.0907 - accuracy: 1.00 - 2s 438ms/step - loss: 0.0907 - accuracy: 1.0000 - val_loss: 0.0897 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0897 - accuracy: 1.00 - ETA: 1s - loss: 0.0896 - accuracy: 1.00 - ETA: 0s - loss: 0.0896 - accuracy: 1.00 - ETA: 0s - loss: 0.0895 - accuracy: 1.00 - ETA: 0s - loss: 0.0895 - accuracy: 1.00 - 2s 418ms/step - loss: 0.0894 - accuracy: 1.0000 - val_loss: 0.0885 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0885 - accuracy: 1.00 - ETA: 1s - loss: 0.0884 - accuracy: 1.00 - ETA: 0s - loss: 0.0884 - accuracy: 1.00 - ETA: 0s - loss: 0.0883 - accuracy: 1.00 - ETA: 0s - loss: 0.0883 - accuracy: 1.00 - 2s 487ms/step - loss: 0.0882 - accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0873 - accuracy: 1.00 - ETA: 1s - loss: 0.0873 - accuracy: 1.00 - ETA: 0s - loss: 0.0872 - accuracy: 1.00 - ETA: 0s - loss: 0.0872 - accuracy: 1.00 - ETA: 0s - loss: 0.0871 - accuracy: 1.00 - 2s 413ms/step - loss: 0.0871 - accuracy: 1.0000 - val_loss: 0.0862 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0862 - accuracy: 1.00 - ETA: 1s - loss: 0.0861 - accuracy: 1.00 - ETA: 0s - loss: 0.0861 - accuracy: 1.00 - ETA: 0s - loss: 0.0860 - accuracy: 1.00 - ETA: 0s - loss: 0.0860 - accuracy: 1.00 - 2s 496ms/step - loss: 0.0859 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0851 - accuracy: 1.00 - ETA: 1s - loss: 0.0851 - accuracy: 1.00 - ETA: 0s - loss: 0.0850 - accuracy: 1.00 - ETA: 0s - loss: 0.0850 - accuracy: 1.00 - ETA: 0s - loss: 0.0849 - accuracy: 1.00 - 2s 430ms/step - loss: 0.0849 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0840 - accuracy: 1.00 - ETA: 1s - loss: 0.0840 - accuracy: 1.00 - ETA: 0s - loss: 0.0839 - accuracy: 1.00 - ETA: 0s - loss: 0.0839 - accuracy: 1.00 - ETA: 0s - loss: 0.0838 - accuracy: 1.00 - 2s 446ms/step - loss: 0.0838 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0830 - accuracy: 1.00 - ETA: 1s - loss: 0.0830 - accuracy: 1.00 - ETA: 0s - loss: 0.0829 - accuracy: 1.00 - ETA: 0s - loss: 0.0829 - accuracy: 1.00 - ETA: 0s - loss: 0.0828 - accuracy: 1.00 - 2s 421ms/step - loss: 0.0828 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0820 - accuracy: 1.00 - ETA: 1s - loss: 0.0820 - accuracy: 1.00 - ETA: 0s - loss: 0.0819 - accuracy: 1.00 - ETA: 0s - loss: 0.0819 - accuracy: 1.00 - ETA: 0s - loss: 0.0818 - accuracy: 1.00 - 2s 419ms/step - loss: 0.0818 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0810 - accuracy: 1.00 - ETA: 1s - loss: 0.0810 - accuracy: 1.00 - ETA: 0s - loss: 0.0809 - accuracy: 1.00 - ETA: 0s - loss: 0.0809 - accuracy: 1.00 - ETA: 0s - loss: 0.0808 - accuracy: 1.00 - 2s 428ms/step - loss: 0.0808 - accuracy: 1.0000 - val_loss: 0.0801 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0801 - accuracy: 1.00 - ETA: 1s - loss: 0.0800 - accuracy: 1.00 - ETA: 0s - loss: 0.0800 - accuracy: 1.00 - ETA: 0s - loss: 0.0799 - accuracy: 1.00 - ETA: 0s - loss: 0.0799 - accuracy: 1.00 - 2s 434ms/step - loss: 0.0799 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0791 - accuracy: 1.00 - ETA: 1s - loss: 0.0791 - accuracy: 1.00 - ETA: 0s - loss: 0.0791 - accuracy: 1.00 - ETA: 0s - loss: 0.0790 - accuracy: 1.00 - ETA: 0s - loss: 0.0790 - accuracy: 1.00 - 2s 430ms/step - loss: 0.0789 - accuracy: 1.0000 - val_loss: 0.0782 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 1s - loss: 0.0782 - accuracy: 1.00 - ETA: 1s - loss: 0.0782 - accuracy: 1.00 - ETA: 0s - loss: 0.0781 - accuracy: 1.00 - ETA: 0s - loss: 0.0781 - accuracy: 1.00 - ETA: 0s - loss: 0.0781 - accuracy: 1.00 - 2s 433ms/step - loss: 0.0780 - accuracy: 1.0000 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0773 - accuracy: 1.00 - ETA: 1s - loss: 0.0773 - accuracy: 1.00 - ETA: 0s - loss: 0.0773 - accuracy: 1.00 - ETA: 0s - loss: 0.0772 - accuracy: 1.00 - ETA: 0s - loss: 0.0772 - accuracy: 1.00 - 2s 415ms/step - loss: 0.0771 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0765 - accuracy: 1.00 - ETA: 1s - loss: 0.0764 - accuracy: 1.00 - ETA: 0s - loss: 0.0764 - accuracy: 1.00 - ETA: 0s - loss: 0.0763 - accuracy: 1.00 - ETA: 0s - loss: 0.0763 - accuracy: 1.00 - 2s 412ms/step - loss: 0.0763 - accuracy: 1.0000 - val_loss: 0.0756 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0756 - accuracy: 1.00 - ETA: 1s - loss: 0.0756 - accuracy: 1.00 - ETA: 0s - loss: 0.0755 - accuracy: 1.00 - ETA: 0s - loss: 0.0755 - accuracy: 1.00 - ETA: 0s - loss: 0.0755 - accuracy: 1.00 - 2s 419ms/step - loss: 0.0754 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0748 - accuracy: 1.00 - ETA: 1s - loss: 0.0747 - accuracy: 1.00 - ETA: 0s - loss: 0.0747 - accuracy: 1.00 - ETA: 0s - loss: 0.0747 - accuracy: 1.00 - ETA: 0s - loss: 0.0746 - accuracy: 1.00 - 2s 434ms/step - loss: 0.0746 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0740 - accuracy: 1.00 - ETA: 1s - loss: 0.0739 - accuracy: 1.00 - ETA: 0s - loss: 0.0739 - accuracy: 1.00 - ETA: 0s - loss: 0.0738 - accuracy: 1.00 - ETA: 0s - loss: 0.0738 - accuracy: 1.00 - 2s 430ms/step - loss: 0.0738 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0732 - accuracy: 1.00 - ETA: 1s - loss: 0.0731 - accuracy: 1.00 - ETA: 0s - loss: 0.0731 - accuracy: 1.00 - ETA: 0s - loss: 0.0730 - accuracy: 1.00 - ETA: 0s - loss: 0.0730 - accuracy: 1.00 - 2s 465ms/step - loss: 0.0730 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0724 - accuracy: 1.00 - ETA: 1s - loss: 0.0723 - accuracy: 1.00 - ETA: 0s - loss: 0.0723 - accuracy: 1.00 - ETA: 0s - loss: 0.0722 - accuracy: 1.00 - ETA: 0s - loss: 0.0722 - accuracy: 1.00 - 2s 412ms/step - loss: 0.0722 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0716 - accuracy: 1.00 - ETA: 1s - loss: 0.0716 - accuracy: 1.00 - ETA: 0s - loss: 0.0715 - accuracy: 1.00 - ETA: 0s - loss: 0.0715 - accuracy: 1.00 - ETA: 0s - loss: 0.0714 - accuracy: 1.00 - 2s 415ms/step - loss: 0.0714 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0708 - accuracy: 1.00 - ETA: 1s - loss: 0.0708 - accuracy: 1.00 - ETA: 0s - loss: 0.0708 - accuracy: 1.00 - ETA: 0s - loss: 0.0707 - accuracy: 1.00 - ETA: 0s - loss: 0.0707 - accuracy: 1.00 - 2s 420ms/step - loss: 0.0707 - accuracy: 1.0000 - val_loss: 0.0701 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0701 - accuracy: 1.00 - ETA: 1s - loss: 0.0700 - accuracy: 1.00 - ETA: 0s - loss: 0.0700 - accuracy: 1.00 - ETA: 0s - loss: 0.0700 - accuracy: 1.00 - ETA: 0s - loss: 0.0699 - accuracy: 1.00 - 2s 420ms/step - loss: 0.0699 - accuracy: 1.0000 - val_loss: 0.0693 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0693 - accuracy: 1.00 - ETA: 1s - loss: 0.0693 - accuracy: 1.00 - ETA: 0s - loss: 0.0693 - accuracy: 1.00 - ETA: 0s - loss: 0.0692 - accuracy: 1.00 - ETA: 0s - loss: 0.0692 - accuracy: 1.00 - 2s 496ms/step - loss: 0.0692 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0686 - accuracy: 1.00 - ETA: 1s - loss: 0.0686 - accuracy: 1.00 - ETA: 0s - loss: 0.0685 - accuracy: 1.00 - ETA: 0s - loss: 0.0685 - accuracy: 1.00 - ETA: 0s - loss: 0.0685 - accuracy: 1.00 - 2s 412ms/step - loss: 0.0684 - accuracy: 1.0000 - val_loss: 0.0679 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0679 - accuracy: 1.00 - ETA: 1s - loss: 0.0679 - accuracy: 1.00 - ETA: 0s - loss: 0.0678 - accuracy: 1.00 - ETA: 0s - loss: 0.0678 - accuracy: 1.00 - ETA: 0s - loss: 0.0677 - accuracy: 1.00 - 2s 421ms/step - loss: 0.0677 - accuracy: 1.0000 - val_loss: 0.0672 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0672 - accuracy: 1.00 - ETA: 1s - loss: 0.0671 - accuracy: 1.00 - ETA: 0s - loss: 0.0671 - accuracy: 1.00 - ETA: 0s - loss: 0.0671 - accuracy: 1.00 - ETA: 0s - loss: 0.0670 - accuracy: 1.00 - 2s 458ms/step - loss: 0.0670 - accuracy: 1.0000 - val_loss: 0.0665 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0665 - accuracy: 1.00 - ETA: 1s - loss: 0.0665 - accuracy: 1.00 - ETA: 0s - loss: 0.0664 - accuracy: 1.00 - ETA: 0s - loss: 0.0664 - accuracy: 1.00 - ETA: 0s - loss: 0.0663 - accuracy: 1.00 - 2s 438ms/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.0658 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0658 - accuracy: 1.00 - ETA: 1s - loss: 0.0658 - accuracy: 1.00 - ETA: 0s - loss: 0.0657 - accuracy: 1.00 - ETA: 0s - loss: 0.0657 - accuracy: 1.00 - ETA: 0s - loss: 0.0657 - accuracy: 1.00 - 2s 437ms/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.0651 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0651 - accuracy: 1.00 - ETA: 1s - loss: 0.0651 - accuracy: 1.00 - ETA: 0s - loss: 0.0651 - accuracy: 1.00 - ETA: 0s - loss: 0.0650 - accuracy: 1.00 - ETA: 0s - loss: 0.0650 - accuracy: 1.00 - 3s 598ms/step - loss: 0.0650 - accuracy: 1.0000 - val_loss: 0.0644 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0644 - accuracy: 1.00 - ETA: 1s - loss: 0.0644 - accuracy: 1.00 - ETA: 0s - loss: 0.0644 - accuracy: 1.00 - ETA: 0s - loss: 0.0643 - accuracy: 1.00 - ETA: 0s - loss: 0.0643 - accuracy: 1.00 - 2s 427ms/step - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0638 - accuracy: 1.00 - ETA: 1s - loss: 0.0638 - accuracy: 1.00 - ETA: 0s - loss: 0.0637 - accuracy: 1.00 - ETA: 0s - loss: 0.0637 - accuracy: 1.00 - ETA: 0s - loss: 0.0637 - accuracy: 1.00 - 2s 411ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0631 - accuracy: 1.00 - ETA: 1s - loss: 0.0631 - accuracy: 1.00 - ETA: 0s - loss: 0.0631 - accuracy: 1.00 - ETA: 0s - loss: 0.0630 - accuracy: 1.00 - ETA: 0s - loss: 0.0630 - accuracy: 1.00 - 2s 414ms/step - loss: 0.0630 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0625 - accuracy: 1.00 - ETA: 1s - loss: 0.0625 - accuracy: 1.00 - ETA: 0s - loss: 0.0624 - accuracy: 1.00 - ETA: 0s - loss: 0.0624 - accuracy: 1.00 - ETA: 0s - loss: 0.0624 - accuracy: 1.00 - 2s 413ms/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 1.0000\n",
      "Training model 3 CNN\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - ETA: 12s - loss: 0.3395 - accuracy: 1.000 - ETA: 0s - loss: 0.3280 - accuracy: 1.000 - ETA: 0s - loss: 0.3169 - accuracy: 1.00 - ETA: 0s - loss: 0.3063 - accuracy: 1.00 - ETA: 0s - loss: 0.2964 - accuracy: 1.00 - 4s 312ms/step - loss: 0.2897 - accuracy: 1.0000 - val_loss: 0.1611 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 1.00 - ETA: 0s - loss: 0.1568 - accuracy: 1.00 - ETA: 0s - loss: 0.1531 - accuracy: 1.00 - ETA: 0s - loss: 0.1499 - accuracy: 1.00 - ETA: 0s - loss: 0.1471 - accuracy: 1.00 - 1s 188ms/step - loss: 0.1452 - accuracy: 1.0000 - val_loss: 0.1149 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 1.00 - ETA: 0s - loss: 0.1143 - accuracy: 1.00 - ETA: 0s - loss: 0.1136 - accuracy: 1.00 - ETA: 0s - loss: 0.1130 - accuracy: 1.00 - ETA: 0s - loss: 0.1123 - accuracy: 1.00 - 1s 166ms/step - loss: 0.1119 - accuracy: 1.0000 - val_loss: 0.1004 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 1.00 - ETA: 0s - loss: 0.0994 - accuracy: 1.00 - ETA: 0s - loss: 0.0983 - accuracy: 1.00 - ETA: 0s - loss: 0.0971 - accuracy: 1.00 - ETA: 0s - loss: 0.0959 - accuracy: 1.00 - 1s 231ms/step - loss: 0.0951 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 1.00 - ETA: 0s - loss: 0.0752 - accuracy: 1.00 - ETA: 0s - loss: 0.0740 - accuracy: 1.00 - ETA: 0s - loss: 0.0729 - accuracy: 1.00 - ETA: 0s - loss: 0.0719 - accuracy: 1.00 - 1s 187ms/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 1.00 - ETA: 0s - loss: 0.0560 - accuracy: 1.00 - ETA: 0s - loss: 0.0554 - accuracy: 1.00 - ETA: 0s - loss: 0.0548 - accuracy: 1.00 - ETA: 0s - loss: 0.0542 - accuracy: 1.00 - 1s 177ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 1.00 - ETA: 0s - loss: 0.0458 - accuracy: 1.00 - ETA: 0s - loss: 0.0454 - accuracy: 1.00 - ETA: 0s - loss: 0.0450 - accuracy: 1.00 - ETA: 0s - loss: 0.0447 - accuracy: 1.00 - 1s 168ms/step - loss: 0.0444 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 1.00 - ETA: 0s - loss: 0.0383 - accuracy: 1.00 - ETA: 0s - loss: 0.0379 - accuracy: 1.00 - ETA: 0s - loss: 0.0376 - accuracy: 1.00 - ETA: 0s - loss: 0.0372 - accuracy: 1.00 - 1s 171ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 1.00 - ETA: 0s - loss: 0.0315 - accuracy: 1.00 - ETA: 0s - loss: 0.0313 - accuracy: 1.00 - ETA: 0s - loss: 0.0310 - accuracy: 1.00 - ETA: 0s - loss: 0.0307 - accuracy: 1.00 - 1s 182ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 1.00 - ETA: 0s - loss: 0.0267 - accuracy: 1.00 - ETA: 0s - loss: 0.0265 - accuracy: 1.00 - ETA: 0s - loss: 0.0263 - accuracy: 1.00 - ETA: 0s - loss: 0.0261 - accuracy: 1.00 - 1s 213ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 1.00 - ETA: 0s - loss: 0.0232 - accuracy: 1.00 - ETA: 0s - loss: 0.0230 - accuracy: 1.00 - ETA: 0s - loss: 0.0229 - accuracy: 1.00 - ETA: 0s - loss: 0.0227 - accuracy: 1.00 - 1s 195ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 1.00 - ETA: 0s - loss: 0.0202 - accuracy: 1.00 - ETA: 0s - loss: 0.0200 - accuracy: 1.00 - ETA: 0s - loss: 0.0199 - accuracy: 1.00 - ETA: 0s - loss: 0.0198 - accuracy: 1.00 - 1s 230ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 1.00 - ETA: 0s - loss: 0.0177 - accuracy: 1.00 - ETA: 0s - loss: 0.0176 - accuracy: 1.00 - ETA: 0s - loss: 0.0175 - accuracy: 1.00 - ETA: 0s - loss: 0.0174 - accuracy: 1.00 - 1s 207ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 1.00 - ETA: 0s - loss: 0.0157 - accuracy: 1.00 - ETA: 0s - loss: 0.0156 - accuracy: 1.00 - ETA: 0s - loss: 0.0155 - accuracy: 1.00 - ETA: 0s - loss: 0.0154 - accuracy: 1.00 - 1s 167ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 1.00 - ETA: 0s - loss: 0.0139 - accuracy: 1.00 - ETA: 0s - loss: 0.0138 - accuracy: 1.00 - ETA: 0s - loss: 0.0137 - accuracy: 1.00 - ETA: 0s - loss: 0.0137 - accuracy: 1.00 - 1s 166ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 1.00 - ETA: 0s - loss: 0.0124 - accuracy: 1.00 - ETA: 0s - loss: 0.0123 - accuracy: 1.00 - ETA: 0s - loss: 0.0122 - accuracy: 1.00 - ETA: 0s - loss: 0.0122 - accuracy: 1.00 - 1s 166ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 1.00 - ETA: 0s - loss: 0.0110 - accuracy: 1.00 - ETA: 0s - loss: 0.0110 - accuracy: 1.00 - ETA: 0s - loss: 0.0109 - accuracy: 1.00 - ETA: 0s - loss: 0.0109 - accuracy: 1.00 - 1s 171ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 1.00 - ETA: 0s - loss: 0.0099 - accuracy: 1.00 - ETA: 0s - loss: 0.0098 - accuracy: 1.00 - ETA: 0s - loss: 0.0098 - accuracy: 1.00 - ETA: 0s - loss: 0.0097 - accuracy: 1.00 - 1s 158ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 1.00 - ETA: 0s - loss: 0.0088 - accuracy: 1.00 - ETA: 0s - loss: 0.0088 - accuracy: 1.00 - ETA: 0s - loss: 0.0087 - accuracy: 1.00 - ETA: 0s - loss: 0.0087 - accuracy: 1.00 - 1s 172ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 1.00 - ETA: 0s - loss: 0.0079 - accuracy: 1.00 - ETA: 0s - loss: 0.0079 - accuracy: 1.00 - ETA: 0s - loss: 0.0078 - accuracy: 1.00 - ETA: 0s - loss: 0.0078 - accuracy: 1.00 - 1s 158ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 1.00 - ETA: 0s - loss: 0.0071 - accuracy: 1.00 - ETA: 0s - loss: 0.0071 - accuracy: 1.00 - ETA: 0s - loss: 0.0070 - accuracy: 1.00 - ETA: 0s - loss: 0.0070 - accuracy: 1.00 - 1s 156ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 1.00 - ETA: 0s - loss: 0.0064 - accuracy: 1.00 - ETA: 0s - loss: 0.0064 - accuracy: 1.00 - ETA: 0s - loss: 0.0063 - accuracy: 1.00 - ETA: 0s - loss: 0.0063 - accuracy: 1.00 - 1s 156ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.00 - ETA: 0s - loss: 0.0058 - accuracy: 1.00 - ETA: 0s - loss: 0.0057 - accuracy: 1.00 - ETA: 0s - loss: 0.0057 - accuracy: 1.00 - ETA: 0s - loss: 0.0057 - accuracy: 1.00 - 1s 201ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 1.00 - ETA: 0s - loss: 0.0052 - accuracy: 1.00 - ETA: 0s - loss: 0.0052 - accuracy: 1.00 - ETA: 0s - loss: 0.0051 - accuracy: 1.00 - ETA: 0s - loss: 0.0051 - accuracy: 1.00 - 1s 254ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 1.00 - ETA: 0s - loss: 0.0047 - accuracy: 1.00 - ETA: 0s - loss: 0.0046 - accuracy: 1.00 - ETA: 0s - loss: 0.0046 - accuracy: 1.00 - ETA: 0s - loss: 0.0046 - accuracy: 1.00 - 1s 175ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - ETA: 0s - loss: 0.0041 - accuracy: 1.00 - 1s 180ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.00 - ETA: 0s - loss: 0.0038 - accuracy: 1.00 - ETA: 0s - loss: 0.0038 - accuracy: 1.00 - ETA: 0s - loss: 0.0037 - accuracy: 1.00 - ETA: 0s - loss: 0.0037 - accuracy: 1.00 - 1s 179ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.00 - ETA: 0s - loss: 0.0034 - accuracy: 1.00 - ETA: 0s - loss: 0.0034 - accuracy: 1.00 - ETA: 0s - loss: 0.0034 - accuracy: 1.00 - ETA: 0s - loss: 0.0034 - accuracy: 1.00 - 1s 179ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - ETA: 0s - loss: 0.0031 - accuracy: 1.00 - ETA: 0s - loss: 0.0030 - accuracy: 1.00 - ETA: 0s - loss: 0.0030 - accuracy: 1.00 - ETA: 0s - loss: 0.0030 - accuracy: 1.00 - 1s 176ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0028 - accuracy: 1.00 - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - 1s 210ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0024 - accuracy: 1.00 - 1s 195ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.00 - ETA: 0s - loss: 0.0022 - accuracy: 1.00 - ETA: 0s - loss: 0.0022 - accuracy: 1.00 - ETA: 0s - loss: 0.0022 - accuracy: 1.00 - ETA: 0s - loss: 0.0022 - accuracy: 1.00 - 1s 181ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - 1s 172ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0018 - accuracy: 1.00 - 1s 157ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - 1s 170ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - 1s 187ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - 1s 183ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - ETA: 0s - loss: 0.0012 - accuracy: 1.00 - 1s 183ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - ETA: 0s - loss: 0.0011 - accuracy: 1.00 - 1s 182ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.6651e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 9.6651e-04 - accuracy: 1.00 - ETA: 0s - loss: 9.6141e-04 - accuracy: 1.00 - ETA: 0s - loss: 9.5635e-04 - accuracy: 1.00 - ETA: 0s - loss: 9.5134e-04 - accuracy: 1.00 - ETA: 0s - loss: 9.4637e-04 - accuracy: 1.00 - 1s 179ms/step - loss: 9.4306e-04 - accuracy: 1.0000 - val_loss: 8.6852e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 8.6852e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.6391e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.5935e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.5483e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.5036e-04 - accuracy: 1.00 - 1s 182ms/step - loss: 8.4737e-04 - accuracy: 1.0000 - val_loss: 7.8016e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 7.8016e-04 - accuracy: 1.00 - ETA: 0s - loss: 7.7601e-04 - accuracy: 1.00 - ETA: 0s - loss: 7.7190e-04 - accuracy: 1.00 - ETA: 0s - loss: 7.6783e-04 - accuracy: 1.00 - ETA: 0s - loss: 7.6379e-04 - accuracy: 1.00 - 1s 182ms/step - loss: 7.6110e-04 - accuracy: 1.0000 - val_loss: 7.0051e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 7.0051e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.9677e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.9306e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.8939e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.8575e-04 - accuracy: 1.00 - 1s 160ms/step - loss: 6.8333e-04 - accuracy: 1.0000 - val_loss: 6.2872e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.2872e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.2535e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.2200e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.1870e-04 - accuracy: 1.00 - ETA: 0s - loss: 6.1542e-04 - accuracy: 1.00 - 1s 158ms/step - loss: 6.1323e-04 - accuracy: 1.0000 - val_loss: 5.6402e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.6402e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.6098e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.5797e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.5499e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.5203e-04 - accuracy: 1.00 - 1s 242ms/step - loss: 5.5006e-04 - accuracy: 1.0000 - val_loss: 5.0572e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.0572e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.0299e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.0028e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.9759e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.9493e-04 - accuracy: 1.00 - 1s 179ms/step - loss: 4.9316e-04 - accuracy: 1.0000 - val_loss: 4.5322e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 4.5322e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.5076e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4831e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4590e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.4350e-04 - accuracy: 1.00 - 1s 176ms/step - loss: 4.4190e-04 - accuracy: 1.0000 - val_loss: 4.0594e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.0594e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.0372e-04 - accuracy: 1.00 - ETA: 0s - loss: 4.0153e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.9935e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.9719e-04 - accuracy: 1.00 - 1s 182ms/step - loss: 3.9575e-04 - accuracy: 1.0000 - val_loss: 3.6339e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.6339e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.6139e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.5941e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.5745e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.5551e-04 - accuracy: 1.00 - 1s 179ms/step - loss: 3.5422e-04 - accuracy: 1.0000 - val_loss: 3.2509e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.2509e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2330e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.2152e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.1976e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.1801e-04 - accuracy: 1.00 - 1s 187ms/step - loss: 3.1685e-04 - accuracy: 1.0000 - val_loss: 2.9065e-04 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.9065e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.8904e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.8744e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.8585e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.8428e-04 - accuracy: 1.00 - 1s 162ms/step - loss: 2.8324e-04 - accuracy: 1.0000 - val_loss: 2.5969e-04 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.5969e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5824e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5680e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5538e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.5397e-04 - accuracy: 1.00 - 1s 179ms/step - loss: 2.5303e-04 - accuracy: 1.0000 - val_loss: 2.3187e-04 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.3187e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.3057e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.2928e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.2800e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.2673e-04 - accuracy: 1.00 - 1s 202ms/step - loss: 2.2589e-04 - accuracy: 1.0000 - val_loss: 2.0689e-04 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.0689e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.0572e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.0456e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.0341e-04 - accuracy: 1.00 - ETA: 0s - loss: 2.0227e-04 - accuracy: 1.00 - 1s 188ms/step - loss: 2.0151e-04 - accuracy: 1.0000 - val_loss: 1.8446e-04 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.8446e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8341e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8237e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8134e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8032e-04 - accuracy: 1.00 - 1s 178ms/step - loss: 1.7964e-04 - accuracy: 1.0000 - val_loss: 1.6435e-04 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.6435e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.6340e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.6247e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.6155e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.6063e-04 - accuracy: 1.00 - 1s 174ms/step - loss: 1.6002e-04 - accuracy: 1.0000 - val_loss: 1.4631e-04 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.4631e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.4547e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.4463e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.4381e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.4299e-04 - accuracy: 1.00 - 1s 159ms/step - loss: 1.4244e-04 - accuracy: 1.0000 - val_loss: 1.3016e-04 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.3016e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.2940e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.2865e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.2791e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.2718e-04 - accuracy: 1.00 - 1s 170ms/step - loss: 1.2669e-04 - accuracy: 1.0000 - val_loss: 1.1569e-04 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.1569e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.1502e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.1435e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.1369e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.1303e-04 - accuracy: 1.00 - 1s 180ms/step - loss: 1.1259e-04 - accuracy: 1.0000 - val_loss: 1.0276e-04 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.0276e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.0215e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.0155e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.0096e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.0037e-04 - accuracy: 1.00 - 1s 176ms/step - loss: 9.9982e-05 - accuracy: 1.0000 - val_loss: 9.1189e-05 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 9.1189e-05 - accuracy: 1.00 - ETA: 0s - loss: 9.0649e-05 - accuracy: 1.00 - ETA: 0s - loss: 9.0114e-05 - accuracy: 1.00 - ETA: 0s - loss: 8.9585e-05 - accuracy: 1.00 - ETA: 0s - loss: 8.9060e-05 - accuracy: 1.00 - 1s 165ms/step - loss: 8.8711e-05 - accuracy: 1.0000 - val_loss: 8.0857e-05 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 8.0857e-05 - accuracy: 1.00 - ETA: 0s - loss: 8.0375e-05 - accuracy: 1.00 - ETA: 0s - loss: 7.9897e-05 - accuracy: 1.00 - ETA: 0s - loss: 7.9425e-05 - accuracy: 1.00 - ETA: 0s - loss: 7.8957e-05 - accuracy: 1.00 - 1s 167ms/step - loss: 7.8645e-05 - accuracy: 1.0000 - val_loss: 7.1635e-05 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 7.1635e-05 - accuracy: 1.00 - ETA: 0s - loss: 7.1205e-05 - accuracy: 1.00 - ETA: 0s - loss: 7.0779e-05 - accuracy: 1.00 - ETA: 0s - loss: 7.0357e-05 - accuracy: 1.00 - ETA: 0s - loss: 6.9940e-05 - accuracy: 1.00 - 1s 164ms/step - loss: 6.9662e-05 - accuracy: 1.0000 - val_loss: 6.3411e-05 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.3411e-05 - accuracy: 1.00 - ETA: 0s - loss: 6.3027e-05 - accuracy: 1.00 - ETA: 0s - loss: 6.2648e-05 - accuracy: 1.00 - ETA: 0s - loss: 6.2272e-05 - accuracy: 1.00 - ETA: 0s - loss: 6.1900e-05 - accuracy: 1.00 - 1s 169ms/step - loss: 6.1652e-05 - accuracy: 1.0000 - val_loss: 5.6082e-05 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.6082e-05 - accuracy: 1.00 - ETA: 0s - loss: 5.5740e-05 - accuracy: 1.00 - ETA: 0s - loss: 5.5402e-05 - accuracy: 1.00 - ETA: 0s - loss: 5.5068e-05 - accuracy: 1.00 - ETA: 0s - loss: 5.4736e-05 - accuracy: 1.00 - 1s 233ms/step - loss: 5.4515e-05 - accuracy: 1.0000 - val_loss: 4.9557e-05 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.9557e-05 - accuracy: 1.00 - ETA: 0s - loss: 4.9253e-05 - accuracy: 1.00 - ETA: 0s - loss: 4.8952e-05 - accuracy: 1.00 - ETA: 0s - loss: 4.8654e-05 - accuracy: 1.00 - ETA: 0s - loss: 4.8359e-05 - accuracy: 1.00 - 1s 152ms/step - loss: 4.8163e-05 - accuracy: 1.0000 - val_loss: 4.3752e-05 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.3752e-05 - accuracy: 1.00 - ETA: 0s - loss: 4.3482e-05 - accuracy: 1.00 - ETA: 0s - loss: 4.3214e-05 - accuracy: 1.00 - ETA: 0s - loss: 4.2949e-05 - accuracy: 1.00 - ETA: 0s - loss: 4.2687e-05 - accuracy: 1.00 - 1s 160ms/step - loss: 4.2513e-05 - accuracy: 1.0000 - val_loss: 3.8593e-05 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 3.8593e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.8352e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.8115e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.7880e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.7647e-05 - accuracy: 1.00 - 1s 175ms/step - loss: 3.7492e-05 - accuracy: 1.0000 - val_loss: 3.4011e-05 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.4011e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.3798e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.3587e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.3378e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.3172e-05 - accuracy: 1.00 - 1s 165ms/step - loss: 3.3034e-05 - accuracy: 1.0000 - val_loss: 2.9946e-05 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.9946e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.9757e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.9570e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.9385e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.9202e-05 - accuracy: 1.00 - 1s 155ms/step - loss: 2.9080e-05 - accuracy: 1.0000 - val_loss: 2.6343e-05 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.6343e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.6175e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.6010e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.5846e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.5684e-05 - accuracy: 1.00 - 1s 171ms/step - loss: 2.5576e-05 - accuracy: 1.0000 - val_loss: 2.3152e-05 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.3152e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.3004e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.2857e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.2712e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.2569e-05 - accuracy: 1.00 - 1s 189ms/step - loss: 2.2473e-05 - accuracy: 1.0000 - val_loss: 2.0329e-05 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.0329e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.0198e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.0068e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.9940e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.9813e-05 - accuracy: 1.00 - 1s 171ms/step - loss: 1.9728e-05 - accuracy: 1.0000 - val_loss: 1.7833e-05 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.7833e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.7717e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.7603e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.7489e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.7377e-05 - accuracy: 1.00 - 1s 166ms/step - loss: 1.7303e-05 - accuracy: 1.0000 - val_loss: 1.5629e-05 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.5629e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.5527e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.5426e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.5326e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.5227e-05 - accuracy: 1.00 - 1s 151ms/step - loss: 1.5161e-05 - accuracy: 1.0000 - val_loss: 1.3685e-05 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.3685e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.3595e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.3506e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.3417e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.3330e-05 - accuracy: 1.00 - 1s 162ms/step - loss: 1.3272e-05 - accuracy: 1.0000 - val_loss: 1.1971e-05 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.1971e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.1892e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.1813e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.1735e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.1659e-05 - accuracy: 1.00 - 1s 183ms/step - loss: 1.1608e-05 - accuracy: 1.0000 - val_loss: 1.0462e-05 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.0462e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.0392e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.0323e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.0255e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.0187e-05 - accuracy: 1.00 - 1s 190ms/step - loss: 1.0142e-05 - accuracy: 1.0000 - val_loss: 9.1342e-06 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 9.1342e-06 - accuracy: 1.00 - ETA: 0s - loss: 9.0728e-06 - accuracy: 1.00 - ETA: 0s - loss: 9.0121e-06 - accuracy: 1.00 - ETA: 0s - loss: 8.9521e-06 - accuracy: 1.00 - ETA: 0s - loss: 8.8927e-06 - accuracy: 1.00 - 1s 158ms/step - loss: 8.8532e-06 - accuracy: 1.0000 - val_loss: 7.9674e-06 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 7.9674e-06 - accuracy: 1.00 - ETA: 0s - loss: 7.9135e-06 - accuracy: 1.00 - ETA: 0s - loss: 7.8602e-06 - accuracy: 1.00 - ETA: 0s - loss: 7.8075e-06 - accuracy: 1.00 - ETA: 0s - loss: 7.7554e-06 - accuracy: 1.00 - 1s 187ms/step - loss: 7.7207e-06 - accuracy: 1.0000 - val_loss: 6.9431e-06 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.9431e-06 - accuracy: 1.00 - ETA: 0s - loss: 6.8958e-06 - accuracy: 1.00 - ETA: 0s - loss: 6.8490e-06 - accuracy: 1.00 - ETA: 0s - loss: 6.8027e-06 - accuracy: 1.00 - ETA: 0s - loss: 6.7570e-06 - accuracy: 1.00 - 1s 177ms/step - loss: 6.7266e-06 - accuracy: 1.0000 - val_loss: 6.0446e-06 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.0446e-06 - accuracy: 1.00 - ETA: 0s - loss: 6.0031e-06 - accuracy: 1.00 - ETA: 0s - loss: 5.9621e-06 - accuracy: 1.00 - ETA: 0s - loss: 5.9216e-06 - accuracy: 1.00 - ETA: 0s - loss: 5.8815e-06 - accuracy: 1.00 - 1s 155ms/step - loss: 5.8548e-06 - accuracy: 1.0000 - val_loss: 5.2573e-06 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.2573e-06 - accuracy: 1.00 - ETA: 0s - loss: 5.2209e-06 - accuracy: 1.00 - ETA: 0s - loss: 5.1850e-06 - accuracy: 1.00 - ETA: 0s - loss: 5.1496e-06 - accuracy: 1.00 - ETA: 0s - loss: 5.1145e-06 - accuracy: 1.00 - 1s 172ms/step - loss: 5.0911e-06 - accuracy: 1.0000 - val_loss: 4.5681e-06 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.5681e-06 - accuracy: 1.00 - ETA: 0s - loss: 4.5363e-06 - accuracy: 1.00 - ETA: 0s - loss: 4.5049e-06 - accuracy: 1.00 - ETA: 0s - loss: 4.4739e-06 - accuracy: 1.00 - ETA: 0s - loss: 4.4432e-06 - accuracy: 1.00 - 1s 207ms/step - loss: 4.4227e-06 - accuracy: 1.0000 - val_loss: 3.9654e-06 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.9654e-06 - accuracy: 1.00 - ETA: 0s - loss: 3.9376e-06 - accuracy: 1.00 - ETA: 0s - loss: 3.9102e-06 - accuracy: 1.00 - ETA: 0s - loss: 3.8830e-06 - accuracy: 1.00 - ETA: 0s - loss: 3.8562e-06 - accuracy: 1.00 - 1s 162ms/step - loss: 3.8384e-06 - accuracy: 1.0000 - val_loss: 3.4388e-06 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.4388e-06 - accuracy: 1.00 - ETA: 0s - loss: 3.4146e-06 - accuracy: 1.00 - ETA: 0s - loss: 3.3906e-06 - accuracy: 1.00 - ETA: 0s - loss: 3.3670e-06 - accuracy: 1.00 - ETA: 0s - loss: 3.3436e-06 - accuracy: 1.00 - 1s 232ms/step - loss: 3.3280e-06 - accuracy: 1.0000 - val_loss: 2.9793e-06 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.9793e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.9582e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.9373e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.9166e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.8962e-06 - accuracy: 1.00 - 1s 171ms/step - loss: 2.8826e-06 - accuracy: 1.0000 - val_loss: 2.5786e-06 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.5786e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.5602e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.5420e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.5240e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.5063e-06 - accuracy: 1.00 - 1s 180ms/step - loss: 2.4944e-06 - accuracy: 1.0000 - val_loss: 2.2297e-06 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 2.2297e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.2136e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.1978e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.1821e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.1667e-06 - accuracy: 1.00 - 1s 185ms/step - loss: 2.1564e-06 - accuracy: 1.0000 - val_loss: 1.9260e-06 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.9260e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.9121e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.8983e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.8847e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.8713e-06 - accuracy: 1.00 - 1s 195ms/step - loss: 1.8623e-06 - accuracy: 1.0000 - val_loss: 1.6621e-06 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.6621e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.6500e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.6380e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.6262e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.6145e-06 - accuracy: 1.00 - 1s 154ms/step - loss: 1.6067e-06 - accuracy: 1.0000 - val_loss: 1.4329e-06 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.4329e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.4224e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.4120e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.4017e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.3916e-06 - accuracy: 1.00 - 1s 188ms/step - loss: 1.3849e-06 - accuracy: 1.0000 - val_loss: 1.2341e-06 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.2341e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.2250e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.2160e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.2071e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.1983e-06 - accuracy: 1.00 - 1s 176ms/step - loss: 1.1924e-06 - accuracy: 1.0000 - val_loss: 1.0618e-06 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.0618e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.0539e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.0461e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.0384e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.0308e-06 - accuracy: 1.00 - 1s 175ms/step - loss: 1.0257e-06 - accuracy: 1.0000 - val_loss: 9.1264e-07 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 9.1264e-07 - accuracy: 1.00 - ETA: 0s - loss: 9.0581e-07 - accuracy: 1.00 - ETA: 0s - loss: 8.9907e-07 - accuracy: 1.00 - ETA: 0s - loss: 8.9241e-07 - accuracy: 1.00 - ETA: 0s - loss: 8.8584e-07 - accuracy: 1.00 - 1s 172ms/step - loss: 8.8146e-07 - accuracy: 1.0000 - val_loss: 7.8366e-07 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 7.8366e-07 - accuracy: 1.00 - ETA: 0s - loss: 7.7775e-07 - accuracy: 1.00 - ETA: 0s - loss: 7.7193e-07 - accuracy: 1.00 - ETA: 0s - loss: 7.6617e-07 - accuracy: 1.00 - ETA: 0s - loss: 7.6049e-07 - accuracy: 1.00 - 1s 179ms/step - loss: 7.5671e-07 - accuracy: 1.0000 - val_loss: 6.7222e-07 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.7222e-07 - accuracy: 1.00 - ETA: 0s - loss: 6.6713e-07 - accuracy: 1.00 - ETA: 0s - loss: 6.6210e-07 - accuracy: 1.00 - ETA: 0s - loss: 6.5713e-07 - accuracy: 1.00 - ETA: 0s - loss: 6.5223e-07 - accuracy: 1.00 - 1s 170ms/step - loss: 6.4896e-07 - accuracy: 1.0000 - val_loss: 5.7606e-07 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.7606e-07 - accuracy: 1.00 - ETA: 0s - loss: 5.7166e-07 - accuracy: 1.00 - ETA: 0s - loss: 5.6732e-07 - accuracy: 1.00 - ETA: 0s - loss: 5.6304e-07 - accuracy: 1.00 - ETA: 0s - loss: 5.5881e-07 - accuracy: 1.00 - 1s 178ms/step - loss: 5.5600e-07 - accuracy: 1.0000 - val_loss: 4.9315e-07 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.9315e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.8936e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.8562e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.8193e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.7829e-07 - accuracy: 1.00 - 1s 167ms/step - loss: 4.7587e-07 - accuracy: 1.0000 - val_loss: 4.2174e-07 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.2174e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.1848e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.1527e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.1209e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.0896e-07 - accuracy: 1.00 - 1s 180ms/step - loss: 4.0687e-07 - accuracy: 1.0000 - val_loss: 3.6031e-07 - val_accuracy: 1.0000\n",
      "Training model 4 DeepConvLSTM\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - ETA: 25s - loss: 0.0508 - accuracy: 1.000 - ETA: 1s - loss: 0.0466 - accuracy: 1.000 - ETA: 0s - loss: 0.0430 - accuracy: 1.00 - ETA: 0s - loss: 0.0398 - accuracy: 1.00 - ETA: 0s - loss: 0.0371 - accuracy: 1.00 - 9s 793ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0077 - accuracy: 1.00 - ETA: 1s - loss: 0.0076 - accuracy: 1.00 - ETA: 0s - loss: 0.0075 - accuracy: 1.00 - ETA: 0s - loss: 0.0074 - accuracy: 1.00 - ETA: 0s - loss: 0.0074 - accuracy: 1.00 - 2s 502ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0071 - accuracy: 1.00 - ETA: 1s - loss: 0.0070 - accuracy: 1.00 - ETA: 0s - loss: 0.0069 - accuracy: 1.00 - ETA: 0s - loss: 0.0067 - accuracy: 1.00 - ETA: 0s - loss: 0.0066 - accuracy: 1.00 - 2s 508ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0040 - accuracy: 1.00 - ETA: 1s - loss: 0.0039 - accuracy: 1.00 - ETA: 0s - loss: 0.0038 - accuracy: 1.00 - ETA: 0s - loss: 0.0037 - accuracy: 1.00 - ETA: 0s - loss: 0.0036 - accuracy: 1.00 - 2s 484ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0026 - accuracy: 1.00 - ETA: 1s - loss: 0.0026 - accuracy: 1.00 - ETA: 1s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - 3s 532ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 0.0018 - accuracy: 1.00 - ETA: 1s - loss: 0.0018 - accuracy: 1.00 - ETA: 0s - loss: 0.0017 - accuracy: 1.00 - ETA: 0s - loss: 0.0017 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - 2s 476ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 8.7403e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 8.7403e-04 - accuracy: 1.00 - ETA: 1s - loss: 8.4633e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.2198e-04 - accuracy: 1.00 - ETA: 0s - loss: 8.0062e-04 - accuracy: 1.00 - ETA: 0s - loss: 7.8183e-04 - accuracy: 1.00 - 2s 476ms/step - loss: 7.6931e-04 - accuracy: 1.0000 - val_loss: 5.5729e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 5.5729e-04 - accuracy: 1.00 - ETA: 1s - loss: 5.5048e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.4345e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.3604e-04 - accuracy: 1.00 - ETA: 0s - loss: 5.2818e-04 - accuracy: 1.00 - 2s 486ms/step - loss: 5.2294e-04 - accuracy: 1.0000 - val_loss: 3.8660e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 3.8660e-04 - accuracy: 1.00 - ETA: 1s - loss: 3.7579e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.6510e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.5466e-04 - accuracy: 1.00 - ETA: 0s - loss: 3.4456e-04 - accuracy: 1.00 - 2s 477ms/step - loss: 3.3782e-04 - accuracy: 1.0000 - val_loss: 1.9745e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.9745e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.9118e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8541e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.8007e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.7513e-04 - accuracy: 1.00 - 2s 482ms/step - loss: 1.7184e-04 - accuracy: 1.0000 - val_loss: 1.0857e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.0857e-04 - accuracy: 1.00 - ETA: 1s - loss: 1.0592e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.0338e-04 - accuracy: 1.00 - ETA: 0s - loss: 1.0092e-04 - accuracy: 1.00 - ETA: 0s - loss: 9.8555e-05 - accuracy: 1.00 - 2s 487ms/step - loss: 9.6975e-05 - accuracy: 1.0000 - val_loss: 6.3409e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 6.3409e-05 - accuracy: 1.00 - ETA: 1s - loss: 6.1661e-05 - accuracy: 1.00 - ETA: 0s - loss: 5.9999e-05 - accuracy: 1.00 - ETA: 0s - loss: 5.8425e-05 - accuracy: 1.00 - ETA: 0s - loss: 5.6941e-05 - accuracy: 1.00 - 2s 518ms/step - loss: 5.5951e-05 - accuracy: 1.0000 - val_loss: 3.6271e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 3.6271e-05 - accuracy: 1.00 - ETA: 1s - loss: 3.5428e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.4629e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.3864e-05 - accuracy: 1.00 - ETA: 0s - loss: 3.3126e-05 - accuracy: 1.00 - 2s 497ms/step - loss: 3.2634e-05 - accuracy: 1.0000 - val_loss: 2.2052e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 2.2052e-05 - accuracy: 1.00 - ETA: 1s - loss: 2.1435e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.0829e-05 - accuracy: 1.00 - ETA: 0s - loss: 2.0241e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.9675e-05 - accuracy: 1.00 - 2s 501ms/step - loss: 1.9297e-05 - accuracy: 1.0000 - val_loss: 1.1566e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1566e-05 - accuracy: 1.00 - ETA: 1s - loss: 1.1259e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.0988e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.0751e-05 - accuracy: 1.00 - ETA: 0s - loss: 1.0540e-05 - accuracy: 1.00 - 3s 588ms/step - loss: 1.0400e-05 - accuracy: 1.0000 - val_loss: 7.9476e-06 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 7.9476e-06 - accuracy: 1.00 - ETA: 1s - loss: 7.8432e-06 - accuracy: 1.00 - ETA: 0s - loss: 7.7302e-06 - accuracy: 1.00 - ETA: 0s - loss: 7.6077e-06 - accuracy: 1.00 - ETA: 0s - loss: 7.4764e-06 - accuracy: 1.00 - 2s 484ms/step - loss: 7.3889e-06 - accuracy: 1.0000 - val_loss: 5.1274e-06 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - ETA: 2s - loss: 5.1274e-06 - accuracy: 1.00 - ETA: 1s - loss: 4.9710e-06 - accuracy: 1.00 - ETA: 0s - loss: 4.8243e-06 - accuracy: 1.00 - ETA: 0s - loss: 4.6888e-06 - accuracy: 1.00 - ETA: 0s - loss: 4.5645e-06 - accuracy: 1.00 - 2s 482ms/step - loss: 4.4817e-06 - accuracy: 1.0000 - val_loss: 2.9620e-06 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 2.9620e-06 - accuracy: 1.00 - ETA: 1s - loss: 2.9122e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.8642e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.8170e-06 - accuracy: 1.00 - ETA: 0s - loss: 2.7697e-06 - accuracy: 1.00 - 2s 476ms/step - loss: 2.7382e-06 - accuracy: 1.0000 - val_loss: 2.0032e-06 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 2.0032e-06 - accuracy: 1.00 - ETA: 1s - loss: 1.9556e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.9103e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.8677e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.8278e-06 - accuracy: 1.00 - 2s 490ms/step - loss: 1.8013e-06 - accuracy: 1.0000 - val_loss: 1.2861e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.2861e-06 - accuracy: 1.00 - ETA: 1s - loss: 1.2644e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.2429e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.2214e-06 - accuracy: 1.00 - ETA: 0s - loss: 1.1997e-06 - accuracy: 1.00 - 2s 481ms/step - loss: 1.1853e-06 - accuracy: 1.0000 - val_loss: 8.4410e-07 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 8.4410e-07 - accuracy: 1.00 - ETA: 1s - loss: 8.2180e-07 - accuracy: 1.00 - ETA: 0s - loss: 8.0054e-07 - accuracy: 1.00 - ETA: 0s - loss: 7.8056e-07 - accuracy: 1.00 - ETA: 0s - loss: 7.6203e-07 - accuracy: 1.00 - 2s 472ms/step - loss: 7.4967e-07 - accuracy: 1.0000 - val_loss: 5.1828e-07 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 5.1828e-07 - accuracy: 1.00 - ETA: 1s - loss: 5.1087e-07 - accuracy: 1.00 - ETA: 0s - loss: 5.0393e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.9720e-07 - accuracy: 1.00 - ETA: 0s - loss: 4.9051e-07 - accuracy: 1.00 - 2s 478ms/step - loss: 4.8605e-07 - accuracy: 1.0000 - val_loss: 3.8120e-07 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 3.8120e-07 - accuracy: 1.00 - ETA: 1s - loss: 3.7352e-07 - accuracy: 1.00 - ETA: 0s - loss: 3.6595e-07 - accuracy: 1.00 - ETA: 0s - loss: 3.5861e-07 - accuracy: 1.00 - ETA: 0s - loss: 3.5159e-07 - accuracy: 1.00 - 2s 526ms/step - loss: 3.4691e-07 - accuracy: 1.0000 - val_loss: 2.5299e-07 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 2.5299e-07 - accuracy: 1.00 - ETA: 1s - loss: 2.4932e-07 - accuracy: 1.00 - ETA: 0s - loss: 2.4594e-07 - accuracy: 1.00 - ETA: 0s - loss: 2.4278e-07 - accuracy: 1.00 - ETA: 0s - loss: 2.3981e-07 - accuracy: 1.00 - 2s 500ms/step - loss: 2.3782e-07 - accuracy: 1.0000 - val_loss: 1.9720e-07 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.9720e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.9508e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.9305e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.9110e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.8927e-07 - accuracy: 1.00 - 2s 475ms/step - loss: 1.8805e-07 - accuracy: 1.0000 - val_loss: 1.6405e-07 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.6405e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.6310e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.6219e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.6131e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.6043e-07 - accuracy: 1.00 - 2s 513ms/step - loss: 1.5984e-07 - accuracy: 1.0000 - val_loss: 1.4658e-07 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.4658e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.4578e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.4503e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.4432e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.4367e-07 - accuracy: 1.00 - 2s 491ms/step - loss: 1.4324e-07 - accuracy: 1.0000 - val_loss: 1.3513e-07 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.3513e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.3486e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.3460e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.3435e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.3411e-07 - accuracy: 1.00 - 2s 486ms/step - loss: 1.3394e-07 - accuracy: 1.0000 - val_loss: 1.3008e-07 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.3008e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.2981e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2956e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2931e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2906e-07 - accuracy: 1.00 - 2s 506ms/step - loss: 1.2890e-07 - accuracy: 1.0000 - val_loss: 1.2557e-07 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.2557e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.2542e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2527e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2514e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2501e-07 - accuracy: 1.00 - 2s 481ms/step - loss: 1.2492e-07 - accuracy: 1.0000 - val_loss: 1.2316e-07 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.2316e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.2306e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2297e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2288e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2280e-07 - accuracy: 1.00 - 3s 505ms/step - loss: 1.2274e-07 - accuracy: 1.0000 - val_loss: 1.2154e-07 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 1s - loss: 1.2154e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.2148e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2143e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2137e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2132e-07 - accuracy: 1.00 - 2s 515ms/step - loss: 1.2129e-07 - accuracy: 1.0000 - val_loss: 1.2054e-07 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.2054e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.2050e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2047e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2043e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2040e-07 - accuracy: 1.00 - 2s 501ms/step - loss: 1.2038e-07 - accuracy: 1.0000 - val_loss: 1.1996e-07 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1996e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1994e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1992e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1991e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1989e-07 - accuracy: 1.00 - 2s 490ms/step - loss: 1.1989e-07 - accuracy: 1.0000 - val_loss: 1.1967e-07 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1967e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1966e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1964e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1963e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1962e-07 - accuracy: 1.00 - 2s 519ms/step - loss: 1.1961e-07 - accuracy: 1.0000 - val_loss: 1.1947e-07 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1947e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1946e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1945e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1945e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1944e-07 - accuracy: 1.00 - 2s 497ms/step - loss: 1.1944e-07 - accuracy: 1.0000 - val_loss: 1.1937e-07 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1937e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1937e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1936e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1936e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1935e-07 - accuracy: 1.00 - 2s 523ms/step - loss: 1.1935e-07 - accuracy: 1.0000 - val_loss: 1.1931e-07 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1931e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1930e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1930e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1930e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1930e-07 - accuracy: 1.00 - 2s 488ms/step - loss: 1.1930e-07 - accuracy: 1.0000 - val_loss: 1.1927e-07 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1927e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1926e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1926e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1926e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1926e-07 - accuracy: 1.00 - 3s 599ms/step - loss: 1.1926e-07 - accuracy: 1.0000 - val_loss: 1.1925e-07 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1925e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1924e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1924e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1924e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1924e-07 - accuracy: 1.00 - 2s 485ms/step - loss: 1.1924e-07 - accuracy: 1.0000 - val_loss: 1.1923e-07 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1923e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1923e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1923e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1923e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1923e-07 - accuracy: 1.00 - 2s 496ms/step - loss: 1.1923e-07 - accuracy: 1.0000 - val_loss: 1.1922e-07 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1922e-07 - accuracy: 1.00 - 2s 485ms/step - loss: 1.1922e-07 - accuracy: 1.0000 - val_loss: 1.1922e-07 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1922e-07 - accuracy: 1.00 - 3s 524ms/step - loss: 1.1922e-07 - accuracy: 1.0000 - val_loss: 1.1921e-07 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1921e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1921e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1921e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1922e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1922e-07 - accuracy: 1.00 - 2s 483ms/step - loss: 1.1922e-07 - accuracy: 1.0000 - val_loss: 1.1927e-07 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1927e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1928e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1930e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1932e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1935e-07 - accuracy: 1.00 - 2s 483ms/step - loss: 1.1936e-07 - accuracy: 1.0000 - val_loss: 1.1973e-07 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1973e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1978e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1985e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1992e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1996e-07 - accuracy: 1.00 - 2s 490ms/step - loss: 1.2000e-07 - accuracy: 1.0000 - val_loss: 1.1994e-07 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.1994e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.1992e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.1996e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2001e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2008e-07 - accuracy: 1.00 - 2s 466ms/step - loss: 1.2012e-07 - accuracy: 1.0000 - val_loss: 1.2083e-07 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - ETA: 1s - loss: 1.2083e-07 - accuracy: 1.00 - ETA: 1s - loss: 1.2081e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2078e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2076e-07 - accuracy: 1.00 - ETA: 0s - loss: 1.2074e-07 - accuracy: 1.00 - 2s 485ms/step - loss: 1.2073e-07 - accuracy: 1.0000 - val_loss: 1.2075e-07 - val_accuracy: 1.0000\n",
      "Epoch 00048: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/_classification.py:179: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: model  0\n",
      "Model type:  InceptionTime\n",
      "Hyperparameters:  {'learning_rate': 0.00025536164770718195, 'regularization_rate': 0.0005590713834809168, 'network_depth': 4, 'filters_number': 50, 'max_kernel_size': 27}\n",
      "accuracy on validation set:  1.0\n",
      "Accuracy of kNN on validation set 1.0\n"
     ]
    }
   ],
   "source": [
    "best_model, best_params, best_model_type, knn_acc = \\\n",
    "mcfly.find_architecture.find_best_architecture(X_train,\n",
    "                                               y_train,\n",
    "                                               X_val,\n",
    "                                               y_val,\n",
    "                                               verbose=True,\n",
    "                                               number_of_models=5,\n",
    "                                               nr_epochs=100,\n",
    "                                               subset_size=100,\n",
    "                                               outputpath=None,\n",
    "                                               model_path=None,\n",
    "                                               metric='accuracy',\n",
    "                                               class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module mcfly.find_architecture in mcfly:\n",
      "\n",
      "NAME\n",
      "    mcfly.find_architecture\n",
      "\n",
      "DESCRIPTION\n",
      "    Summary:\n",
      "    This module provides the main functionality of mcfly: searching for an\n",
      "    optimal model architecture. The work flow is as follows:\n",
      "    Function generate_models from modelgen.py generates and compiles models.\n",
      "    Function train_models_on_samples trains those models.\n",
      "    Function find_best_architecture is wrapper function that combines\n",
      "    these steps.\n",
      "    Example function calls can be found in the tutorial notebook\n",
      "    (https://github.com/NLeSC/mcfly-tutorial)\n",
      "\n",
      "FUNCTIONS\n",
      "    find_best_architecture(X_train, y_train, X_val, y_val, verbose=True, number_of_models=5, nr_epochs=5, subset_size=100, outputpath=None, model_path=None, metric='accuracy', class_weight=None, **kwargs)\n",
      "        Tries out a number of models on a subsample of the data,\n",
      "        and outputs the best found architecture and hyperparameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X_train : Supported types:\n",
      "        \n",
      "            - numpy array\n",
      "            - `tf.data` dataset. Should return a tuple of `(inputs, targets)`\n",
      "              or `(inputs, targets, sample_weights)`\n",
      "            - generator or `keras.utils.Sequence`. Should return a tuple of\n",
      "              `(inputs, targets)` or `(inputs, targets, sample_weights)`\n",
      "        \n",
      "            The input dataset for training of shape\n",
      "            (num_samples, num_timesteps, num_channels)\n",
      "            More details can be found in the documentation for the Keras\n",
      "            function Model.fit() [1]\n",
      "        y_train : numpy array\n",
      "            The output classes for the train data, in binary format of shape\n",
      "            (num_samples, num_classes)\n",
      "            If the training data is a dataset, generator or\n",
      "            `keras.utils.Sequence`, y_train should not be specified.\n",
      "        X_val : Supported types:\n",
      "            - numpy array\n",
      "            - `tf.data` dataset. Should return a tuple of `(inputs, targets)`\n",
      "              or `(inputs, targets, sample_weights)`\n",
      "            - generator or `keras.utils.Sequence`. Should return a tuple of\n",
      "              `(inputs, targets)` or `(inputs, targets, sample_weights)`\n",
      "        \n",
      "            The input dataset for validation of shape\n",
      "            (num_samples_val, num_timesteps, num_channels)\n",
      "            More details can be found in the documentation for the Keras\n",
      "            function Model.fit() [1]\n",
      "        y_val : numpy array\n",
      "            The output classes for the validation data, in binary format of shape\n",
      "            (num_samples_val, num_classes)\n",
      "            If the validation data is a dataset, generator or\n",
      "            `keras.utils.Sequence`, y_val should not be specified.\n",
      "        verbose : bool, optional\n",
      "            flag for displaying verbose output\n",
      "        number_of_models : int, optiona\n",
      "            The number of models to generate and test\n",
      "        nr_epochs : int, optional\n",
      "            The number of epochs that each model is trained\n",
      "        subset_size : int, optional\n",
      "            The size of the subset of the data that is used for finding\n",
      "            the optimal architecture. Default is 100.\n",
      "        outputpath : str, optional\n",
      "            File location to store the model results\n",
      "        model_path: str, optional\n",
      "            Directory to save the models as HDF5 files\n",
      "        class_weight: dict, optional\n",
      "            Dictionary containing class weights (example: {0: 0.5, 1: 2.})\n",
      "        metric: str, optional\n",
      "            metric that is used to evaluate the model on the validation set.\n",
      "            See https://keras.io/metrics/ for possible metrics\n",
      "        **kwargs: key-value parameters\n",
      "            parameters for generating the models\n",
      "            (see docstring for modelgen.generate_models)\n",
      "        \n",
      "        Returns\n",
      "        ----------\n",
      "        best_model : Keras model\n",
      "            Best performing model, already trained on a small sample data set.\n",
      "        best_params : dict\n",
      "            Dictionary containing the hyperparameters for the best model\n",
      "        best_model_type : str\n",
      "            Type of the best model\n",
      "        knn_acc : float\n",
      "            accuaracy for kNN prediction on validation set\n",
      "        \n",
      "        \n",
      "        [1]: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
      "    \n",
      "    kNN_accuracy(X_train, y_train, X_val, y_val, k=1)\n",
      "        Performs k-Neigherst Neighbors and returns the accuracy score.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X_train : numpy array\n",
      "            Train set of shape (num_samples, num_timesteps, num_channels)\n",
      "        y_train : numpy array\n",
      "            Class labels for train set\n",
      "        X_val : numpy array\n",
      "            Validation set of shape (num_samples, num_timesteps, num_channels)\n",
      "        y_val : numpy array\n",
      "            Class labels for validation set\n",
      "        k : int\n",
      "            number of neighbors to use for classifying\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        accuracy: float\n",
      "            accuracy score on the validation set\n",
      "    \n",
      "    store_train_hist_as_json(params, model_type, history, outputfile, metric_name=None)\n",
      "        This function stores the model parameters, the loss and accuracy history\n",
      "        of one model in a JSON file. It appends the model information to the\n",
      "        existing models in the file.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            parameters for one model\n",
      "        model_type : Keras model object\n",
      "            Keras model object for one model\n",
      "        history : dict\n",
      "            training history from one model\n",
      "        outputfile : str\n",
      "            path where the json file needs to be stored\n",
      "        metric_name : str, optional\n",
      "            DEPRECATED: name of metric from history to store\n",
      "    \n",
      "    train_models_on_samples(X_train, y_train, X_val, y_val, models, nr_epochs=5, subset_size=100, verbose=True, outputfile=None, model_path=None, early_stopping_patience='auto', batch_size=20, metric=None, class_weight=None)\n",
      "        Given a list of compiled models, this function trains\n",
      "        them all on a subset of the train data. If the given size of the subset is\n",
      "        smaller then the size of the data, the complete data set is used.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X_train : Supported types:\n",
      "            - numpy array\n",
      "            - `tf.data` dataset. Should return a tuple of `(inputs, targets)`\n",
      "              or `(inputs, targets, sample_weights)`\n",
      "            - generator or `keras.utils.Sequence`. Should return a tuple of\n",
      "              `(inputs, targets)` or `(inputs, targets, sample_weights)`\n",
      "        \n",
      "            The input dataset for training of shape\n",
      "            (num_samples, num_timesteps, num_channels)\n",
      "            More details can be found in the documentation for the Keras\n",
      "            function Model.fit() [1]\n",
      "        y_train : numpy array\n",
      "            The output classes for the train data, in binary format of shape\n",
      "            (num_samples, num_classes)\n",
      "            If the training data is a dataset, generator or\n",
      "            `keras.utils.Sequence`, y_train should not be specified.\n",
      "        X_val : Supported types:\n",
      "            - numpy array\n",
      "            - `tf.data` dataset. Should return a tuple of `(inputs, targets)`\n",
      "              or `(inputs, targets, sample_weights)`\n",
      "            - generator or `keras.utils.Sequence`. Should return a tuple of\n",
      "              `(inputs, targets)` or `(inputs, targets, sample_weights)`\n",
      "              \n",
      "            The input dataset for validation of shape\n",
      "            (num_samples_val, num_timesteps, num_channels)\n",
      "            More details can be found in the documentation for the Keras\n",
      "            function Model.fit() [1]\n",
      "        y_val : numpy array\n",
      "            The output classes for the validation data, in binary format of shape\n",
      "            (num_samples_val, num_classes)\n",
      "            If the validation data is a dataset, generator or\n",
      "            `keras.utils.Sequence`, y_val should not be specified.\n",
      "        models : list of model, params, modeltypes\n",
      "            List of keras models to train\n",
      "        nr_epochs : int, optional\n",
      "            nr of epochs to use for training one model\n",
      "        subset_size :\n",
      "            The number of samples used from the complete train set. If set to 'None'\n",
      "            use the entire dataset. Default is 100, but should be adjusted depending\n",
      "            on the type ans size of the dataset.\n",
      "            Subset is not supported for tf.data.Dataset objects or generators\n",
      "        verbose : bool, optional\n",
      "            flag for displaying verbose output\n",
      "        outputfile: str, optional\n",
      "            Filename to store the model training results\n",
      "        model_path : str, optional\n",
      "            Directory to store the models as HDF5 files\n",
      "        early_stopping_patience: str, int\n",
      "            Unless 'None' early Stopping is used for the model training. Set to integer\n",
      "            to define how many epochs without improvement to wait for before stopping.\n",
      "            Default is 'auto' in which case the patience will be set to number of epochs/10\n",
      "            (and not bigger than 5).\n",
      "        batch_size : int\n",
      "            nr of samples per batch\n",
      "        metric : str\n",
      "            DEPRECATED: metric to store in the history object\n",
      "        class_weight: dict, optional\n",
      "            Dictionary containing class weights (example: {0: 0.5, 1: 2.})\n",
      "        \n",
      "        Returns\n",
      "        ----------\n",
      "        histories : list of Keras History objects\n",
      "            train histories for all models\n",
      "        val_metrics : list of floats\n",
      "            validation metrics of the models\n",
      "        val_losses : list of floats\n",
      "            validation losses of the models\n",
      "        \n",
      "        [1]: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
      "\n",
      "FILE\n",
      "    /usr/local/lib/python3.6/dist-packages/mcfly/find_architecture.py\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "help(mcfly.find_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MseSzdVcakuf"
   },
   "source": [
    "probabilidades predicción:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 180, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 180, 4)       16          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 180, 32)      128         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 180, 4)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 180, 50)      43200       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 180, 50)      20800       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 180, 50)      9600        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 180, 50)      200         max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 180, 200)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 180, 200)     800         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 180, 200)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 180, 32)      6400        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 180, 200)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 180, 50)      43200       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 180, 50)      20800       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 180, 50)      9600        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 180, 50)      10000       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 180, 200)     0           conv1d_6[0][0]                   \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 180, 200)     800         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 180, 200)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 180, 32)      6400        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 180, 200)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 180, 50)      43200       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 180, 50)      20800       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 180, 50)      9600        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 180, 50)      10000       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 180, 200)     0           conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "                                                                 conv1d_13[0][0]                  \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 180, 200)     800         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 180, 200)     800         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 180, 200)     800         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 180, 200)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 180, 200)     0           batch_normalization_4[0][0]      \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 180, 200)     0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 180, 32)      6400        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 180, 200)     0           activation_3[0][0]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 180, 50)      43200       conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 180, 50)      20800       conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 180, 50)      9600        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 180, 50)      10000       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 180, 200)     0           conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "                                                                 conv1d_19[0][0]                  \n",
      "                                                                 conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 180, 200)     800         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 180, 200)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 200)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            201         global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 348,945\n",
      "Trainable params: 346,937\n",
      "Non-trainable params: 2,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AutoML_Ander.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
